<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML" lang="EN" xml:lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="default-style"/>
<title>2: Programming Language Syntax</title>


<script type="text/javascript" src="libs/jquery-3.6.0.min.js"/>
<script type="text/javascript" src="hoveredTooltip-data.js"/>
<link rel="stylesheet" type="text/css" href="hoveredTooltip.css"/>
<script type="text/javascript" src="hoveredTooltip.js"/>

</head>
<body>
<section epub:type="chapter" role="doc-chapter" id="c0010">
<div epub:type="pagebreak" id="page_43" aria-label="Page 43" role="doc-pagebreak"/>
<div id="CN"><a id="c0010tit1"/>
</div>
<div class="chapter-head two">
<header>
<hgroup>
<h1 class="chaptitle" id="c0010tit"><span class="label">2:</span> <span class="title">Programming Language Syntax<sup><span class="anychar dropdown def-img anychar-tooltip pos-bottom" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_fn001">&#x235F;</span></sup></span></h1>
</hgroup>
</header>
</div>
<section epub:type="preamble">
<div class="abstract">
<h2 class="h1hd" id="ab0010"><a id="st0010"/>Abstract</h2>
<p class="abspara">Syntax represents the form a programming language, and must be specified precisely so that programmers and computers can tell what a program is supposed to do. The syntax of a program is typically made up of tokens such as keywords, identifiers, and symbols. Regular expressions specify how tokens are made up of characters, using sequencing, alternatives, and repetition constructs. A scanner analyses input text and organizes it into tokens using the theory of deterministic finite automata. Grammars specify how tokens are composed into higher-level language constructs. A grammar is made up of production rules, which show how abstract language forms are constructed from simpler parts. Parsers analyze a sequence of tokens and transform it into a syntax tree that makes the program's structure explicit. Parsers can be constructed in multiple ways, influencing language expressiveness, parsing efficiency, and error handling.</p>
</div>
</section>
<section id="ks0010">
<h3 class="h2hd" id="st0015">Keywords</h3>
<p class="keywords">Syntax; Parsing; Keyword; Identifier; Token; Symbol; Grammar; Regular expression; Production; Deterministic finite automaton</p>
</section>
<p id="p0010" class="textfl"><b>Unlike natural languages such as English or Chinese,</b> computer languages must be precise. Both their form (syntax) and meaning (semantics) must be specified without ambiguity, so that both programmers and computers can tell what a program is supposed to do. To provide the needed degree of precision, language designers and implementors use formal syntactic and semantic notation. To facilitate the discussion of language features in later chapters, we will cover this notation first: syntax in the current chapter and semantics in Chapter 4.</p>
<p id="p0015" class="text"/>
<div class="boxg1" id="enun0010">
<p class="b1num">Example 2.1 </p>
<p class="b1title">Syntax of Arabic numerals</p>
<div>
<p id="p0020" class="b1textfl">As a motivating example, consider the Arabic numerals with which we represent numbers. These numerals are composed of digits, which we can enumerate as follows (&#x2018; | &#x2019; means &#x201C;or&#x201D;):</p>
<pre>  <em>digit</em> &#x27F6; 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9</pre>
<p class="textfl"> Digits are the syntactic building blocks for numbers. In the usual notation, we say that a natural number is represented by an arbitrary-length (nonempty) string of digits, beginning with a nonzero digit:</p>
<pre>  <em>non_zero_digit</em> &#x27F6; 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
  <em>natural_number</em> &#x27F6; <em>non_zero_digit digit</em>*</pre>
<p class="textfl"> Here the &#x201C;Kleene <sup><a href="#fn002" id="cf0100" epub:type="noteref" role="doc-noteref">1</a></sup> star&#x201D; metasymbol (<b>*</b>) is used to indicate zero or more repetitions of the symbol to its left. &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p0025" class="text">Of course, digits are only symbols: ink blobs on paper or pixels on a screen. They carry no meaning in and of themselves. We add semantics to digits when we say that they represent the natural numbers from zero to nine, as defined by mathematicians. Alternatively, we could say that they represent colors, or the days of the week in a decimal calendar. These would constitute <span epub:type="pagebreak" id="page_44" aria-label="Page 44" role="doc-pagebreak"/>alternative semantics for the same syntax. In a similar fashion, we define the semantics of natural numbers by associating a base-10, place-value interpretation with each string of digits. Similar syntax rules and semantic interpretations can be devised for rational numbers, (limited-precision) real numbers, arithmetic, assignments, control flow, declarations, and indeed all of programming languages.</p>
<p id="p0030" class="text">Distinguishing between syntax and semantics is useful for at least two reasons. First, different programming languages often provide features with very similar semantics but very different syntax. It is generally much easier to learn a new language if one is able to identify the common (and presumably familiar) semantic ideas beneath the unfamiliar syntax. Second, there are some very efficient and elegant algorithms that a compiler or interpreter can use to discover the syntactic structure (but not the semantics!) of a computer program, and these algorithms can be used to drive the rest of the compilation or interpretation process.</p>
<p id="p0035" class="text">In the current chapter we focus on syntax: how we specify the structural rules of a programming language, and how a compiler identifies the structure of a given input program. These two tasks&#x2014;specifying syntax rules and figuring out how (and whether) a given program was built according to those rules&#x2014;are distinct. The first is of interest mainly to programmers, who want to write valid programs. The second is of interest mainly to compilers, which need to analyze those programs. The first task relies on <i>regular expressions</i> and <i>context-free grammars</i>, which specify how to generate valid programs. The second task relies on <i>scanners</i> and <i>parsers</i>, which recognize program structure. We address the first of these tasks in Section <span class="hovered-ribbon ribbon-right"><a href="#s0010" id="cf0105">2.1</a></span> and the second in Sections <a href="#s0040" id="cf0110">2.2</a> and <a href="#s0085" id="cf0115">2.3</a>.</p>
<p id="p0040" class="text">In Section <span class="hovered-ribbon ribbon-right"><a href="#s0140" id="cf0120">2.4</a></span> (largely on the companion site) we take a deeper look at the formal theory underlying scanning and parsing. In theoretical parlance, a scanner is a <i>deterministic finite automaton</i> (DFA) that recognizes the tokens of a programming language. A parser is a deterministic <i>push-down automaton</i> (PDA) that recognizes the language's context-free syntax. It turns out that one can generate scanners and parsers automatically from regular expressions and context-free grammars. This task is performed by tools like Unix's <span class="inlinecode">lex</span> and <span class="inlinecode">yacc</span>, <sup><a href="#fn003" id="cf0125" epub:type="noteref" role="doc-noteref">2</a></sup> among others. Possibly nowhere else in computer science is the connection between theory and practice so clear and so compelling.</p>
<section>
<h2 class="h1hd" id="s0010"><a id="st0025"/>2.1 Specifying Syntax: Regular Expressions and Context-Free Grammars</h2>
<p id="p0045" class="textfl">Formal specification of syntax requires a set of rules. How complicated (expressive) the syntax can be depends on the kinds of rules we are allowed to use. It turns out that what we intuitively think of as tokens can be constructed from individual characters using just three kinds of formal rules: concatenation, alternation (choice among a finite set of alternatives), and so-called &#x201C;Kleene closure&#x201D; <span epub:type="pagebreak" id="page_45" aria-label="Page 45" role="doc-pagebreak"/>(repetition an arbitrary number of times). Specifying most of the rest of what we intuitively think of as syntax requires one additional kind of rule: recursion (creation of a construct from simpler instances of the same construct). Any set of strings that can be defined in terms of the first three rules is called a <i>regular set</i>, or sometimes a <i>regular language</i>. Regular sets are generated by <i>regular expressions</i> and recognized by scanners. Any set of strings that can be defined if we add recursion is called a <i>context-free language</i> (CFL). Context-free languages are generated by <i>context-free grammars</i> (CFGs) and recognized by parsers. (Terminology can be confusing here. The meaning of the word &#x201C;language&#x201D; varies greatly, depending on whether we're talking about &#x201C;formal&#x201D; languages [e.g., regular or context-free], or programming languages. A formal language is just a set of strings, with no accompanying semantics.)</p>
<section>
<h3 id="s0015" class="h2hd"><a id="st0030"/>2.1.1 Tokens and Regular Expressions</h3>
<p id="p0050" class="textfl">Tokens are the basic building blocks of programs&#x2014;the shortest strings of characters with individual meaning. Tokens come in many <i>kinds</i>, including keywords, identifiers, symbols, and constants of various types. Some kinds of token (e.g., the increment operator) correspond to only one string of characters. Others (e.g., <i>identifier</i>) correspond to a set of strings that share some common form. (In most languages, keywords are special strings of characters that have the right form to be identifiers, but are reserved for special purposes.) We will use the word &#x201C;token&#x201D; informally to refer to both the generic kind (an identifier, the increment operator) and the specific string (<span class="inlinecode">foo</span>, <span class="inlinecode">++</span>); the distinction between these should be clear from context.</p>
<p id="p0055" class="text"/>
<div class="boxg1" id="b0010">
<p class="b1title">Design &amp; Implementation</p>
<p id="p0060" class="b1textfl"/>
</div>

<div class="boxg1" id="enun0015">
<p class="b1title">2.1 Contextual keywords</p>
<div>
<p id="p0065" class="b1textfl">In addition to distinguishing between keywords and identifiers, some languages define so-called <i>contextual keywords</i>, which function as keywords in certain specific places in a program, but as identifiers elsewhere. In C#, for example, the word <span class="inlinecode">yield</span> can appear immediately before <span class="inlinecode">return</span> or <span class="inlinecode">break</span>&#x2014;a place where an identifier can never appear. In this context, it is interpreted as a keyword; anywhere else it is an identifier. It is therefore perfectly acceptable to have a local variable named <span class="inlinecode">yield</span>: the compiler can distinguish it from the keyword by where it appears in the program.</p>
<p id="p0070" class="b1text">C++11 has a small handful of contextual keywords. C# 4.0 has 26. Most were introduced in the course of revising the language to create a new standard version. Given a large user community, any short, intuitively appealing word is likely to have been used as an identifier by someone, in some existing program. Making that word a contextual keyword in the new version of the language, rather than a full keyword, reduces the risk that existing programs will suddenly fail to compile.</p>
</div></div>
<p class="textfl"/>
<p id="p0075" class="text">Some languages have only a few kinds of token, of fairly simple form. Other languages are more complex. </p>
<div class="boxg1" id="enun0020">
<p class="b1num">Example 2.2 </p>
<p class="b1title">Lexical structure of C17</p>
<div>
<p id="p0080" class="b1textfl">C, <span epub:type="pagebreak" id="page_46" aria-label="Page 46" role="doc-pagebreak"/>for example, has more than 100 kinds of tokens, including 44 keywords (<span class="inlinecode">double</span>, <span class="inlinecode">if</span>, <span class="inlinecode">return</span>, <span class="inlinecode">struct</span>, etc.); identifiers (<span class="inlinecode">my_variable</span>, <span class="inlinecode">your_type</span>, <span class="inlinecode">sizeof</span>, <span class="inlinecode">printf</span>, etc.); integer (0765, 0x1f5, 501), floating-point (6.022e23), and character (<span class="inlinecode">'x'</span>, <span class="inlinecode">'&#xFE68;&#x201D;</span>, <span class="inlinecode">'&#xFE68;0170'</span>) constants; string literals (<span class="inlinecode">"snerk"</span>, <span class="inlinecode">"say &#xFE68;"hi&#xFE68;"&#xFE68;n"</span>); 54 &#x201C;punctuators&#x201D; (<span class="inlinecode">+</span>, <span class="inlinecode">]</span>, <span class="inlinecode">-&gt;</span>, <span class="inlinecode">*=</span>, <span class="inlinecode">:</span>, <span class="inlinecode">||</span>, etc.), and two different forms of comments. There are provisions for international character sets, string literals that span multiple lines of source code, constants of varying precision (width), alternative &#x201C;spellings&#x201D; for symbols that are missing on certain input devices, and preprocessor macros that build tokens from smaller pieces. Other large, modern languages (Java, Ada) are similarly complex. &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p0085" class="text">To specify tokens, we use the notation of <i>regular expressions</i>. A regular expression is one of the following:</p>
<div><ol>
<li id="o0010" class="numlist"><b>1.</b> &#xA0;A character</li>
<li id="o0015" class="numlist"><b>2.</b> &#xA0;The empty string, denoted <i>&#x3B5;</i></li>
<li id="o0020" class="numlist"><b>3.</b> &#xA0;Two regular expressions next to each other, meaning any string generated by the first one followed by (concatenated with) any string generated by the second one</li>
<li id="o0025" class="numlist"><b>4.</b> &#xA0;Two regular expressions separated by a vertical bar ( | ), meaning any string generated by the first one <i>or</i> any string generated by the second one</li>
<li id="o0030" class="numlist"><b>5.</b> &#xA0;A regular expression followed by a Kleene star, meaning the concatenation of zero or more strings generated by the expression in front of the star</li></ol>
</div>
<p class="textfl"/>
<p id="p0115" class="text">Parentheses are used to avoid ambiguity about where the various subexpressions start and end. <sup><a href="#fn004" id="cf0130" epub:type="noteref" role="doc-noteref">3</a></sup></p>
<p id="p0120" class="text"/>
<div class="boxg1" id="enun0025">
<p class="b1num">Example 2.3 </p>
<p class="b1title">Syntax of numeric constants</p>
<div>
<p id="p0125" class="b1textfl">Consider, for example, the syntax of numeric constants accepted by a simple hand-held calculator:</p>
<pre>
   <span class="serif"><em>number</em></span> &#x27F6; <em>integer</em> | <em>real</em>
   <em>integer</em> &#x27F6; <em>digit digit</em>*
   <em>real</em> &#x27F6; <em>integer exponent</em> | <em>decimal</em> (<em>exponent</em> | &#x03B5;)
   <em>decimal</em> &#x27F6; <em>digit</em> * (. <em>digit</em> | <em>digit</em> .) <em>digit</em> *
   <em>exponent</em> &#x27F6; ( e | E ) ( + | - | &#x03B5; ) <em>integer</em>
   <em>digit</em> &#x27F6; 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
</pre>
<p class="textfl"/>
<p id="p0130" class="b1text">The symbols to the left of the &#x27F6;<span style="display:inline-block; width: 0.45em;"/>signs provide names for the regular expressions. One of these (<i>number</i>) will serve as a token name; the others are simply for convenience in building larger expressions. <sup><a href="#fn005" id="cf0135" epub:type="noteref" role="doc-noteref">4</a></sup> Note that while we have allowed definitions to build on one another, <span epub:type="pagebreak" id="page_47" aria-label="Page 47" role="doc-pagebreak"/>nothing is ever defined in terms of itself, even indirectly. Such recursive definitions are the distinguishing characteristic of context-free grammars, described in Section <span class="hovered-ribbon ribbon-right"><a href="#s0030" id="cf0140">2.1.2</a></span>. To generate a valid number, we expand out the sub-definitions and then scan the resulting expression from left to right, choosing among alternatives at each vertical bar, and choosing a number of repetitions at each Kleene star. Within each repetition we may make different choices at vertical bars, generating different substrings. &#x2003;&#x25A0;</p>


</div></div>
<p class="textfl"/>
<section>
<h4 id="s0020" class="h3hd"><a id="st0050"/>Character Sets and Formatting Issues</h4>
<p id="p0135" class="textfl">Upper- and lowercase letters in identifiers and keywords are considered distinct in some languages (e.g., Perl, Python, and Ruby; C and its descendants), and identical in others (e.g., Ada, Common Lisp, and Fortran). Thus <span class="inlinecode">foo</span>, <span class="inlinecode">Foo</span>, and <span class="inlinecode">FOO</span> all represent the same identifier in Ada, but different identifiers in C. Modula-2 and Modula-3 require keywords and predefined (built-in) identifiers to be written in uppercase; C and its descendants require them to be written in lowercase. A few languages allow only letters and digits in identifiers. Most allow underscores. A few (notably Lisp) allow a variety of additional characters. Some languages (e.g., Java and C#) have standard (but optional) conventions on the use of upper- and lowercase letters in names.</p>
<p id="p0140" class="text">With the globalization of computing, non-Latin character sets have become increasingly important. Many modern languages, including C, have introduced explicit support for multibyte character sets, generally based on the Unicode and ISO/IEC 10646 international standards. Most modern programming languages allow non-Latin characters to appear within comments and character strings; an increasing number allow them in identifiers as well. Conventions for portability across character sets and for <i>localization</i> to a given character set can be surprisingly complex, particularly when various forms of backward compatibility are required (the C99 Rationale devotes five full pages to this subject <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br1100">[Int03, pp. 19&#x2013;23]</span>); for the most part we ignore such issues here.</p>
<p id="p0145" class="text"/>
<div class="boxg1" id="b0015">
<p class="b1title">Design &amp; Implementation</p>
<p id="p0150" class="b1textfl"/>
</div>
<div class="boxg1" id="enun0030">
<p class="b1title">2.2 Formatting restrictions</p>
<div>
<p id="p0155" class="b1textfl">Formatting limitations inspired by implementation concerns&#x2014;as in the punch-card-oriented rules of Fortran 77 and its predecessors&#x2014;have a tendency to become unwanted anachronisms as implementation techniques improve. Given the tendency of certain word processors to &#x201C;fill&#x201D; or auto-format text, the line break and indentation rules of languages like Haskell, Occam, and Python are somewhat controversial.</p>
</div></div>
<p class="textfl"/>
<p id="p0160" class="text">Some language implementations impose limits on the maximum length of identifiers, but most avoid such unnecessary restrictions. Most modern languages are also more or less <i>free format</i>, meaning that a program is simply a sequence of tokens: what matters is their order with respect to one another, not their physical position within a printed line or page. &#x201C;White space&#x201D; <span epub:type="pagebreak" id="page_48" aria-label="Page 48" role="doc-pagebreak"/>(blanks, tabs, carriage returns, and line- and page-feed characters) between tokens is usually ignored, except to the extent that it is needed to separate one token from the next.</p>
<p id="p0165" class="text">There are a few noteworthy exceptions to these rules. Some language implementations limit the maximum length of a line, to allow the compiler to store the current line in a fixed-length buffer. Dialects of Fortran prior to Fortran 90 use a <i>fixed format</i>, with 72 characters per line (the width of a paper punch card, on which programs were once stored), and with different columns within the line reserved for different purposes. Line breaks serve to separate statements in several other languages, including Go, Haskell, Python, and Swift. Haskell and Python also give special significance to indentation. The body of a loop, for example, consists of precisely those subsequent lines that are indented farther than the header of the loop.</p>
</section>
<section>
<h4 id="s0025" class="h3hd"><a id="st0060"/>Other Uses of Regular Expressions</h4>
<p id="p0170" class="textfl">Many readers will be familiar with regular expressions from the <span class="inlinecode">grep</span> family of tools in Unix, the search facilities of various text editors, or such scripting languages and tools as Perl, Python, Ruby, <span class="inlinecode">awk</span>, and <span class="inlinecode">sed</span>. Most of these provide a rich set of extensions to the notation of regular expressions. Some extensions, such as shorthand for &#x201C;zero or one occurrences&#x201D; or &#x201C;anything other than white space,&#x201D; do not change the power of the notation. Others, such as the ability to require a second occurrence, later in the input string, of the same character sequence that matched an earlier part of the expression, increase the power of the notation, so that it is no longer restricted to generating regular sets. Still other extensions are designed not to increase the expressiveness of the notation but rather to tie it to other language facilities. In many tools, for example, one can bracket portions of a regular expression in such a way that when a string is matched against it the contents of the corresponding substrings are assigned into named local variables. We will return to these issues in Section 14.4.2, in the context of scripting languages.</p>
</section></section>
<section>
<h3 id="s0030" class="h2hd"><a id="st0065"/>2.1.2 Context-Free Grammars</h3>
<p id="p0175" class="textfl">Regular expressions work well for defining tokens. They are unable, however, to specify <i>nested</i> constructs, which are central to programming languages. </p>
<div class="boxg1" id="enun0035">
<p class="b1num">Example 2.4 </p>
<p class="b1title">Syntactic nesting in expressions</p>
<div>
<p id="p0180" class="b1textfl">Consider for example the structure of an arithmetic expression:</p>
<pre>  <em>expr</em> &#x27F6; id | number | - <em>expr</em> | (<em>expr</em> )
	 | <em>expr op expr</em>
  <em>op</em> &#x27F6; + | - | * | /</pre>
<p class="textfl"> Here the ability to define a construct in terms of itself is crucial. Among other things, it allows us to ensure that left and right parentheses are matched, something that cannot be accomplished with regular expressions (see Section C-2.4.3 for more details). The arrow symbol (&#x27F6;&#x2009;) means &#x201C;can have the form&#x201D;; for brevity it is sometimes pronounced &#x201C;goes to.&#x201D; &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p0185" class="text"><span epub:type="pagebreak" id="page_49" aria-label="Page 49" role="doc-pagebreak"/>Each of the rules in a context-free grammar is known as a <em>production</em>. The symbols on the left-hand sides of the productions are known as <em>variables</em>, or <em>nonterminals</em>. There may be any number of productions with the same left-hand side. Symbols that are to make up the strings derived from the grammar are known as <em>terminals</em> (shown here in <span class="inlinecode">typewriter</span> font). They cannot appear on the left-hand side of any production. In a programming language, the terminals of the context-free grammar are the language's tokens. One of the nonterminals, usually the one on the left-hand side of the first production, is called the <em>start symbol</em>. It names the construct defined by the overall grammar.</p>
<p id="p0190" class="text">The notation for context-free grammars is sometimes called Backus&#x2013;Naur Form (BNF), in honor of John Backus and Peter Naur, who devised it for the definition of the Algol-60 programming language <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br1595">[NBB+63]</span>. <sup><a href="#fn006" id="cf0155" epub:type="noteref" role="doc-noteref">5</a></sup> Strictly speaking, the Kleene star and meta-level parentheses of regular expressions are not allowed in BNF, but they do not change the expressive power of the notation, and are commonly included for convenience. Sometimes one sees a &#x201C;Kleene plus&#x201D; (&#x207A;) as well; it indicates one or more instances of the symbol or group of symbols in front of it. <sup><a href="#fn007" id="cf0160" epub:type="noteref" role="doc-noteref">6</a></sup> When augmented with these extra operators, the notation is often called extended BNF (EBNF). </p>
<div class="boxg1" id="enun0040">
<p class="b1num">Example 2.5 </p>
<p class="b1title">Extended BNF (EBNF)</p>
<div>
<p id="p0195" class="b1textfl">The construct</p>
<pre>  <em>id_list</em> &#x27F6; id (, id)*</pre>
<p class="textfl"> is shorthand for</p>
<pre>  <em>id_list</em> &#x27F6; id
  <em>id_list</em> &#x27F6; <em>id_list</em> , id</pre>
<p class="textfl"> &#x201C;Kleene plus&#x201D; is analogous. Note that the parentheses here are metasymbols. In Example 2.4 they were part of the language being defined, and were written in fixed-width font.<sup><a href="#fn008" id="cf0165" epub:type="noteref" role="doc-noteref">7</a></sup></p>
<p id="p0200" class="b1text">Like the Kleene star and parentheses, the vertical bar is in some sense superfluous, though it was provided in the original BNF. The construct</p>
<pre>  <em>op</em> &#x27F6; + | - | * | /</pre>
<p class="textfl"> can be considered shorthand for <span epub:type="pagebreak" id="page_50" aria-label="Page 50" role="doc-pagebreak"/></p>
<pre>  <em>op</em> &#x27F6; +
  <em>op</em> &#x27F6; -
  <em>op</em> &#x27F6; *
  <em>op</em> &#x27F6; /</pre>
<p class="textfl"> which is also sometimes written</p>
<pre>  <em>op</em> &#x27F6; +
     &#x27F6; -
     &#x27F6; *
     &#x27F6; /</pre>
<p class="textfl"> &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p0205" class="text">Many tokens, such as <span class="inlinecode">id</span> and <span class="inlinecode">number</span> above, have many possible spellings (i.e., may be represented by many possible strings of characters). The parser is oblivious to these; it does not distinguish one identifier from another. The semantic analyzer does distinguish them, however; the scanner must save the spelling of each such &#x201C;interesting&#x201D; token for later use.</p>
</section>
<section>
<h3 id="s0035" class="h2hd"><a id="st0080"/>2.1.3 Derivations and Parse Trees</h3>
<p id="p0210" class="textfl">A context-free grammar shows us how to generate a syntactically valid string of terminals: Begin with the start symbol. Choose a production with the start symbol on the left-hand side; replace the start symbol with the right-hand side of that production. Now choose a nonterminal <em>A</em> in the resulting string, choose a production <em>P</em> with <em>A</em> on its left-hand side, and replace <em>A</em> with the right-hand side of <em>P</em>. Repeat this process until no nonterminals remain.</p>
<p id="p0215" class="text"/>
<div class="boxg1" id="enun0045">
<p class="b1num">Example 2.6 </p>
<p class="b1title">Derivation of <span class="inlinecode">slope * x + intercept</span></p>
<div>
<p id="p0220" class="b1textfl">As an example, we can use our grammar for expressions to generate the string &#x201C;<span class="inlinecode">slope * x + intercept</span>&#x201D;:</p>
<pre><em>expr</em> &#x27F9; <em>expr op <u>expr</u></em>
     &#x27F9; <em>expr <u>op</u></em> id
     &#x27F9; <em><u>expr</u></em> + id
     &#x27F9; <em>expr op <u>expr</u></em> + id
     &#x27F9; <em>expr <u>op</u></em> id + id
     &#x27F9; <em><u>expr</u></em> * id + id
     &#x27F9;    id   *   id +      id
         (slope)   (x)   (intercept)</pre>
<p class="textfl"> The &#x27F9;&#x2009;metasymbol is often pronounced &#x201C;derives.&#x201D; It indicates that the right-hand side was obtained by using a production to replace some nonterminal in the left-hand side. At each line we have underlined the symbol <em>A</em> that is replaced in the following line. &#x2003;&#x25A0;</p>

</div></div>
<p class="textfl"/>
<p id="p0225" class="text"><span epub:type="pagebreak" id="page_51" aria-label="Page 51" role="doc-pagebreak"/>A series of replacement operations that shows how to derive a string of terminals from the start symbol is called a <em>derivation</em>. Each string of symbols along the way is called a <em>sentential form</em>. The final sentential form, consisting of only terminals, is called the <em>yield</em> of the derivation. We sometimes elide the intermediate steps and write <em>expr</em> &#x27F9;*&#x2009;<span class="inlinecode">slope * x + intercept</span>, where the metasymbol &#x27F9;*&#x2009;means &#x201C;derives after zero or more replacements.&#x201D; In this particular derivation, we have chosen at each step to replace the right-most nonterminal with the right-hand side of some production. This replacement strategy leads to a <em>right-most</em> derivation. There are many other possible derivations, including <em>left-most</em> and options in-between.</p>
<p id="p0230" class="text">We saw in Chapter 1 that we can represent a derivation graphically as a <em>parse tree</em>. The root of the parse tree is the start symbol of the grammar. The leaves of the tree are its yield. Each internal node, together with its children, represents the use of a production.</p>
<p id="p0235" class="text"/>
<div class="boxg1" id="enun0050">
<p class="b1num">Example 2.7 </p>
<p class="b1title">Parse trees for <span class="inlinecode">slope * x + intercept</span></p>
<div>
<p id="p0240" class="b1textfl">A parse tree for our example expression appears in Figure <a href="#f0060" id="cf0170">2.1</a>. This tree is not unique. At the second level of the tree, we could have chosen to turn the operator into a <span class="inlinecode">*</span> instead of a <span class="inlinecode">+</span>, and to further expand the expression on the right, rather than the one on the left (see Figure <a href="#f0065" id="cf0175">2.2</a>). A grammar that allows the construction of more than one parse tree for some string of terminals is said to be <em>ambiguous</em>. Ambiguity turns out to be a problem when trying to build a parser: it requires some extra mechanism to drive a choice between equally acceptable alternatives. &#x2003;&#x25A0;</p>
<div class="pageavoid"><figure class="fig" id="f0060"><img src="images/B9780323999663000118/gr001.png" alt="Figure 2.1"/><figcaption class="figleg"><span class="fignum"><a href="#cf0170">Figure 2.1</a></span> <b>Parse tree for <span class="inlinecode">slope * x + intercept</span></b> <span class="sansserif">(grammar in Example 2.4).</span></figcaption></figure>
</div>
<div class="pageavoid"><figure class="fig" id="f0065"><img src="images/B9780323999663000118/gr002.png" alt="Figure 2.2"/><figcaption class="figleg"><span class="fignum"><a href="#cf0175">Figure 2.2</a></span> <b>Alternative (less desirable) parse tree for <span class="inlinecode">slope * x + intercept</span></b> (grammar in Example 2.4). The fact that more than one tree exists implies that our grammar is ambiguous.</figcaption></figure>
</div></div>
</div>
<p class="textfl"/>
<p id="p0245" class="text"><span epub:type="pagebreak" id="page_52" aria-label="Page 52" role="doc-pagebreak"/>A moment's reflection will reveal that there are infinitely many context-free grammars for any given context-free language. <sup><a href="#fn009" id="cf0180" epub:type="noteref" role="doc-noteref">8</a></sup> Some grammars, however, are much more useful than others. In this text we will avoid the use of ambiguous grammars (though most parser generators allow them, by means of <em>disambiguating</em> rules). We will also avoid the use of so-called <em>useless</em> symbols: nonterminals that cannot generate any string of terminals, or terminals that cannot appear in the yield of any derivation.</p>
<p id="p0250" class="text">When designing the grammar for a programming language, we generally try to find one that reflects the internal structure of programs in a way that is useful to the rest of the compiler. (We shall see in Section <a href="#s0095" id="cf0185">2.3.2</a> that we also try to find one that can be parsed efficiently, which can be a bit of a challenge.) One place in which structure is particularly important is in arithmetic expressions, where we can use productions to capture the <em>associativity</em> and <em>precedence</em> of the various operators. Associativity tells us that the operators in most languages group left to right, so <span class="inlinecode">10 - 4 - 3</span> means <span class="inlinecode">(10 - 4) - 3</span> rather than <span class="inlinecode">10 - (4 - 3)</span>. Precedence tells us that multiplication and division in most languages group more tightly than addition and subtraction, so <span class="inlinecode">3 + 4 * 5</span> means <span class="inlinecode">3 + (4 * 5)</span> rather than <span class="inlinecode">(3 + 4) * 5</span>. (These rules are not universal; we will consider them again in Section 6.1.1.)</p>
<p id="p0255" class="text"/>
<div class="boxg1" id="enun0055">
<p class="b1num">Example 2.8 </p>
<p class="b1title">Expression grammar with precedence and associativity</p>
<div>
<p id="p0260" class="b1textfl">Here is a better version of our expression grammar:</p>
<pre>  1. <em>expr</em> &#x27F6; <em>term</em> | <em>expr add_op term</em>
  2. <em>term</em> &#x27F6; <em>factor</em> | <em>term mult_op factor</em>
  3. <em>factor</em> &#x27F6; id | number | - <em>factor</em> | (<em>expr</em> )
  4. <em>add_op</em> &#x27F6; + | -
  5. <em>mult_op</em> &#x27F6; * | /</pre>
<p class="textfl">This grammar is unambiguous. It captures precedence in the way <em>factor</em>, <em>term</em>, and <em>expr</em> build on one another, with different operators appearing at each level. It captures associativity in the second halves of lines 1 and 2, which build sub<em>expr</em>s and sub<em>term</em>s to the left of the operator, rather than to the right. In Figure <a href="#f0075" id="cf0190">2.3</a>, <span epub:type="pagebreak" id="page_53" aria-label="Page 53" role="doc-pagebreak"/> we can see how building the notion of precedence into the grammar makes it clear that multiplication groups more tightly than addition in <span class="inlinecode">3 + 4 * 5</span>, even without parentheses. In Figure <a href="#f0080" id="cf0195">2.4</a>, we can see that subtraction groups more tightly to the left, so that <span class="inlinecode">10 - 4 - 3</span> would evaluate to <span class="inlinecode">3</span>, rather than to <span class="inlinecode">9</span>. &#x2003;&#x25A0;</p>
<div class="pageavoid"><figure class="fig" id="f0075"><img src="images/B9780323999663000118/gr003.png" alt="Figure 2.3"/><figcaption class="figleg"><span class="fignum"><a href="#cf0190">Figure 2.3</a></span> <b>Parse tree for 3 + 4 * 5, with precedence</b> (grammar in Example <a href="#enun0055" id="cf0010">2.8</a>).</figcaption></figure>
</div>
<div class="pageavoid"><figure class="fig" id="f0080"><img src="images/B9780323999663000118/gr004.png" alt="Figure 2.4"/><figcaption class="figleg"><span class="fignum"><a href="#cf0195">Figure 2.4</a></span> <b>Parse tree for 10 &#x2013; 4 &#x2013; 3, with left associativity</b> (grammar in Example <a href="#enun0055" id="cf0015">2.8</a>).</figcaption></figure>
</div></div>
</div>
<p class="textfl"/>
<p id="p0265" class="text"/>
<div class="boxg1" id="enun0060">
<p class="b1num"><img alt="Image 12" src="images/B9780323999663000118/fx012.png"/> Check Your Understanding </p>
<div>
<p id="p0270" class="b1textfl"/>
<div><ol>
<li id="o0035" class="b1numlist"><b>1.</b> &#xA0;What is the difference between syntax and semantics?</li>
<li id="o0040" class="b1numlist"><b>2.</b> &#xA0;What are the three basic operations that can be used to build complex regular expressions from simpler regular expressions?</li>
<li id="o0045" class="b1numlist"><b>3.</b> &#xA0;What additional operation (beyond the three of regular expressions) is provided in context-free grammars?</li>
<li id="o0050" class="b1numlist"><b>4.</b> &#xA0;What is <em>Backus&#x2013;Naur form</em>? When and why was it devised?</li>
<li id="o0055" class="b1numlist"><b>5.</b> &#xA0;Name a language in which indentation affects program syntax.</li>
<li id="o0060" class="b1numlist"><b>6.</b> &#xA0;When discussing context-free languages, what is a <em>derivation</em>? What is a <em>sentential form</em>?</li>
<li id="o0065" class="b1numlist"><b>7.</b> &#xA0;What is the difference between a <em>right-most</em> derivation and a <em>left-most</em> derivation?</li>
<li id="o0070" class="b1numlist"><b>8.</b> &#xA0;What does it mean for a context-free grammar to be <em>ambiguous</em>?</li>
<li id="o0075" class="b1numlist"><b>9.</b> &#xA0;What are <em>associativity</em> and <em>precedence</em>? Why are they significant in parse trees?</li></ol>
</div>
<p class="b1textfl"/>
</div></div>
<p class="textfl"><span epub:type="pagebreak" id="page_54" aria-label="Page 54" role="doc-pagebreak"/></p>
</section></section>
<section>
<h2 class="h1hd" id="s0040"><a id="st0100"/>2.2 Scanning</h2>
<p id="p0320" class="textfl">Together, the scanner and parser for a programming language are responsible for discovering the syntactic structure of a program. This process of discovery, or <em>syntax analysis</em>, is a necessary first step toward translating the program into an equivalent program in the target language. (It's also the first step toward interpreting the program directly. In general, we will focus on compilation, rather than interpretation, for the remainder of the book. Most of what we shall discuss either has an obvious application to interpretation, or is obviously irrelevant to it.)</p>
<p id="p0325" class="text">By grouping input characters into tokens, the scanner dramatically reduces the number of individual items that must be inspected by the more computationally intensive parser. In addition, the scanner typically removes comments (so the parser doesn't have to worry about them appearing throughout the context-free grammar&#x2014;see Exercise <a href="#o0510" id="cf0200">2.20</a>); saves the text of &#x201C;interesting&#x201D; tokens like identifiers, strings, and numeric literals; and tags tokens with line and column numbers, to make it easier to generate high-quality error messages in subsequent phases.</p>
<p id="p0330" class="text"/>
<div class="boxg1" id="enun0065">
<p class="b1num">Example 2.9 </p>
<p class="b1title">Tokens for a calculator language</p>
<div>
<p id="p0335" class="b1textfl">In Examples 2.4 and <a href="#enun0055" id="cf0205">2.8</a> we considered a simple language for arithmetic expressions. In Section <a href="#s0090" id="cf0210">2.3.1</a> we will extend this to create a simple &#x201C;calculator language&#x201D; with input, output, variables, and assignment. For this language we will use the following set of tokens:</p>
<pre>  <em>assign</em> &#x27F6; :=
  <em>plus</em> &#x27F6; +
  <em>minus</em> &#x27F6; -
  <em>times</em> &#x27F6; *
  <em>div</em> &#x27F6; /
  <em>lparen</em> &#x27F6; (
  <em>rparen</em> &#x27F6;  )
  <em>id</em> &#x27F6; <em>letter</em> (<em>letter</em> | <em>digit</em> )*
       except for read and write
  <em>number</em> &#x27F6; <em>digit digit</em> * | <em>digit</em> * (. <em>digit</em> | <em>digit</em> .) <em>digit</em> *</pre>
<p class="textfl"> In keeping with Algol and its descendants (and in contrast to the C-family languages), we have used <span class="inlinecode">:=</span> rather than <span class="inlinecode">=</span> for assignment. For simplicity, we have omitted the exponent notation for numbers found in Example <a href="#enun0025" id="cf0215">2.3</a>. We have also listed the tokens <span class="inlinecode">read</span> and <span class="inlinecode">write</span> as exceptions to the rule for <span class="inlinecode">id</span> (more on this in Section <a href="#s0065" id="cf0220">2.2.2</a>). To make the task of the scanner a little more realistic, we borrow the two styles of comment from C:</p>
<pre>  <em>comment</em> &#x27F6; /* (<em>non-</em>* | <em>non</em>-*/ | *<sup>+</sup> <em>non</em>-(*,/))* *+ /
	    | // (<em>non-newline</em> )* <em>newline</em></pre>
<p class="textfl"> Here we have used <em>non-</em><span class="inlinecode">*</span>, <em>non-</em><span class="inlinecode">/</span>, <em>non-(<span class="inlinecode">*</span>,<span class="inlinecode">/</span>)</em>, and <em>non-newline</em> as shorthand for the alternation of all characters other than <span class="inlinecode">*</span>, <span class="inlinecode">/</span>, <span class="inlinecode">*</span> <em>or</em> <span class="inlinecode">/</span>, and <em>newline</em>, respectively. The first style of comment is tricky: the regular expression allows anything inside a comment other than a <span class="inlinecode">*/</span> end delimiter. &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p0340" class="text"/>
<div class="boxg1" id="b0020">
<p class="b1title">Design &amp; Implementation</p>
<p id="p0345" class="b1textfl"/>
</div>
<div class="boxg1" id="enun0070">
<p class="b1title">2.3 Nested comments</p>
<div>
<p id="p0350" class="b1textfl">Nested comments can be handy for the programmer (e.g., for temporarily &#x201C;commenting out&#x201D; large blocks of code). Scanners normally deal only with nonrecursive constructs, however, so nested comments require special treatment. Some languages disallow them. Others require the language implementor to augment the scanner with special-purpose comment-handling code. C and C++ strike a compromise: <span class="inlinecode">/*&#x2009;...&#x2009;*/</span> style comments are not allowed to nest, but <span class="inlinecode">/*&#x2009;...&#x2009;*/</span> and <span class="inlinecode">//...</span> style comments can appear inside each other. The programmer can thus use one style for &#x201C;normal&#x201D; comments and the other for &#x201C;commenting out.&#x201D; (The C99 designers note, however, that conditional compilation (<span class="inlinecode">#if</span>) is preferable <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br1100">[Int03, p. 58]</span>.)</p>
</div></div>
<p class="textfl"/>
<p id="p0355" class="text"/>
<div class="boxg1" id="enun0075">
<p class="b1num">Example 2.10 </p>
<p class="b1title">An ad hoc scanner for calculator tokens</p>
<div>
<p id="p0360" class="b1textfl">How might we go about recognizing the tokens of our calculator language? The simplest approach is entirely ad hoc. Pseudocode appears in Figure <a href="#f0095" id="cf0230">2.5</a>. We can structure the code however we like, but it seems reasonable to check the simpler and more common cases first, to peek ahead when we need to, and to embed loops for comments and for long tokens such as identifiers and numbers.</p>
<div class="pageavoid"><figure class="fig" id="f0095"><img src="images/B9780323999663000118/gr005.png" alt="Figure 2.5"/><figcaption class="figleg"><span class="fignum">Figure 2.5</span> <b>Outline of an ad hoc scanner for tokens in our calculator language</b>.</figcaption></figure>
</div>
<p id="p0365" class="b1text"><span epub:type="pagebreak" id="page_55" aria-label="Page 55" role="doc-pagebreak"/>After finding a token the scanner returns to the parser. When invoked again it repeats the algorithm from the beginning, using the next available characters of input (including any that were peeked at but not consumed the last time). &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p0370" class="text">As a rule, we accept the longest possible token in each invocation of the scanner. Thus <span class="inlinecode">foobar</span> is always <span class="inlinecode">foobar</span> and never <span class="inlinecode">f</span> or <span class="inlinecode">foo</span> or <span class="inlinecode">foob</span>. More to the point, in a language like C, <span class="inlinecode">3.14159</span> is a real number and never <span class="inlinecode">3</span>, <span class="inlinecode">.</span>, and <span class="inlinecode">14159</span>. White space (blanks, tabs, newlines, comments) is generally ignored, except to the extent that it separates tokens (e.g., <span class="inlinecode">foo bar</span> is different from <span class="inlinecode">foobar</span>).</p>
<p id="p0375" class="text">Figure <a href="#f0095" id="cf0235">2.5</a> could be extended fairly easily to outline a scanner for some larger programming language. The result could then be fleshed out, by hand, to create code in some implementation language. Production compilers often use such ad hoc scanners; the code is fast and compact. During language development, however, it is usually preferable to build a scanner in a more structured way, as an explicit representation of a <em>finite automaton</em>. Finite automata can be generated automatically from a set of regular expressions, making it easy to regenerate a scanner when token definitions change.</p>
<p id="p0380" class="text"/>
<div class="boxg1" id="enun0080">
<p class="b1num">Example 2.11 </p>
<p class="b1title">Finite automaton for a calculator scanner</p>
<div>
<p id="p0385" class="b1textfl">An automaton for the tokens of our calculator language appears in pictorial form in Figure <a href="#f0100" id="cf0240">2.6</a>. The automaton starts in a distinguished initial state. It then moves from state to state based on the next available character of input. When it reaches one of a designated set of final states it recognizes the token associated with that state. The &#x201C;longest possible token&#x201D; rule means that the scanner returns to the parser only when the next character cannot be used to continue the current token. &#x2003;&#x25A0;</p>
<div class="pageavoid"><figure class="fig" id="f0100"><img src="images/B9780323999663000118/gr006.png" alt="Figure 2.6"/><figcaption class="figleg"><span class="fignum">Figure 2.6</span> <b>Pictorial representation of a scanner for calculator tokens, in the form of a finite automaton.</b> This figure roughly parallels the code in Figure <a href="#f0095" id="cf0020">2.5</a>. States are numbered for reference in Figure <a href="#f0135" id="cf0025">2.12</a>. Scanning for each token begins in the state marked &#x201C;Start.&#x201D; The <em>final</em> states, in which a token is recognized, are indicated by double circles. Comments, when recognized, send the scanner back to its start state, rather than a final state.</figcaption></figure>
</div></div>
</div>
<p class="textfl"/>
<section>
<h3 id="s0045" class="h2hd"><a id="st0125"/>2.2.1 Generating a Finite Automaton</h3>
<p id="p0390" class="textfl">While a finite automaton can in principle be written by hand, it is more common to build one automatically from a set of regular expressions, using a <em>scanner generator</em> tool. <span epub:type="pagebreak" id="page_56" aria-label="Page 56" role="doc-pagebreak"/>For our calculator language, we should like to covert the regular expressions of Example <a href="#enun0065" id="cf0245">2.9</a> into the automaton of Figure <a href="#f0100" id="cf0250">2.6</a>. That automaton has the desirable property that its actions are <em>deterministic</em>: in any given state with a given input character there is never more than one possible outgoing transition (arrow) labeled by that character. As it turns out, however, there is no obvious one-step algorithm to convert a set of regular expressions into an equivalent deterministic finite automaton (DFA). The typical scanner generator implements the conversion as a series of three separate steps.</p>
<p id="p0395" class="text">The first step converts the regular expressions into a <em>nondeterministic</em> finite automaton (NFA). An NFA is like a DFA except that (1) there may be more than one transition out of a given state labeled by a given character, and (2) there may be so-called <em>epsilon transitions</em>: arrows labeled by the empty string symbol, <em>&#x3B5;</em>. The NFA is said to accept an input string (token) if there exists a path from the start state to a final state whose non-epsilon transitions are labeled, in order, by the characters of the token.</p>
<p id="p0400" class="text">To avoid the need to search all possible paths for one that &#x201C;works,&#x201D; the second step of a scanner generator translates the NFA into an equivalent DFA: an automaton that accepts the same language, but in <span epub:type="pagebreak" id="page_57" aria-label="Page 57" role="doc-pagebreak"/>which there are no epsilon transitions, and no states with more than one outgoing transition labeled by the same character. The third step is a space optimization that generates a final DFA with the minimum possible number of states.</p>
<section>
<h4 id="s0050" class="h3hd"><a id="st0130"/>From a Regular Expression to an NFA</h4>
<p id="p0405" class="textfl"/>
<div class="boxg1" id="enun0085">
<p class="b1num">Example 2.12 </p>
<p class="b1title">Constructing an NFA for a given regular expression</p>
<div>
<p id="p0410" class="b1textfl">A trivial regular expression consisting of a single character <span class="inlinecode">c</span> is equivalent to a simple two-state NFA (in fact, a DFA), illustrated in part (a) of Figure <a href="#f0105" id="cf0255">2.7</a>. <span epub:type="pagebreak" id="page_58" aria-label="Page 58" role="doc-pagebreak"/> Similarly, the regular expression <em>&#x3B5;</em> is equivalent to a two-state NFA whose arc is labeled by <em>&#x3B5;</em>. Starting with this base we can use three subconstructions, illustrated in parts (b) through (d) of the same figure, to build larger NFAs to represent the concatenation, alternation, or Kleene closure of the regular expressions represented by smaller NFAs. Each step preserves three invariants: there are no transitions into the initial state, there is a single final state, and there are no transitions out of the final state. These invariants allow smaller automata to be joined into larger ones without any ambiguity about where to create the connections, and without creating any unexpected paths. &#x2003;&#x25A0;</p>
<div class="pageavoid"><figure class="fig" id="f0105"><img src="images/B9780323999663000118/gr007.png" alt="Figure 2.7"/><figcaption class="figleg"><span class="fignum">Figure 2.7</span> <b>Construction of an NFA equivalent to a given regular expression.</b> Part (a) shows the base case: the automaton for the single letter <span class="inlinecode">c</span>. Parts (b), (c), and (d), respectively, show the constructions for concatenation, alternation, and Kleene closure. Each construction retains a unique start state and a single final state. Internal detail is hidden in the diamond-shaped center regions.</figcaption></figure>
</div></div>
</div>
<p class="textfl"/>
<p id="p0415" class="text"/>
<div class="boxg1" id="enun0090">
<p class="b1num">Example 2.13 </p>
<p class="b1title">NFA for <em>d</em><b>&#xFF0A;(</b>&#x2009;<span class="inlinecode">.</span><em>d</em> | <em>d</em><span class="inlinecode">.</span>&#x2009;<b>)</b>&#x2009;<em>d</em><b>&#xFF0A;</b></p>
<div>
<p id="p0420" class="b1textfl">To make these constructions concrete, we consider a small but nontrivial example&#x2014;the <em>decimal</em> strings of Example <a href="#enun0025" id="cf0260">2.3</a>. These consist of a string of decimal digits containing a single decimal point. With only one digit, the point can come at the beginning or the end: <b>(</b>&#x2009;<span class="inlinecode">.</span><em>d</em> | <em>d</em><span class="inlinecode">.</span>&#x2009;<b>)</b>&#x2009;, where for brevity we use <em>d</em> to represent any decimal digit. <span epub:type="pagebreak" id="page_59" aria-label="Page 59" role="doc-pagebreak"/>Arbitrary numbers of digits can then be added at the beginning or the end: <em>d</em><b>&#xFF0A;(</b>&#x2009;<span class="inlinecode">.</span><em>d</em> | <em>d</em><span class="inlinecode">.</span>&#x2009;<b>)</b>&#x2009;<em>d</em><b>&#xFF0A;</b>. Starting with this regular expression and using the constructions of Figure <a href="#f0105" id="cf0265">2.7</a>, we illustrate the construction of an equivalent NFA in Figure <a href="#f0110" id="cf0270">2.8</a>. &#x2003;&#x25A0;</p>
<div class="pageavoid"><figure class="fig" id="f0110"><img src="images/B9780323999663000118/gr008.png" alt="Figure 2.8"/><figcaption class="figleg"><span class="fignum">Figure 2.8</span> <b>Construction of an NFA equivalent to the regular expression <em>d</em>&#xFF0A;(&#x2009;<span class="inlinecode">.</span><em>d</em> |&#x2002;<em>d</em><span class="inlinecode">.</span>&#x2009;)&#x2009;<em>d</em>&#xFF0A;.</b> In the top row are the primitive automata for <span class="inlinecode">.</span> and <em>d</em>, and the Kleene closure construction for <em>d</em><b>&#xFF0A;</b>. In the second and third rows we have used the concatenation and alternation constructions to build <span class="inlinecode">.</span><em>d</em>, <em>d</em><span class="inlinecode">.</span>, and <b>(</b>&#x2009;<span class="inlinecode">.</span><em>d</em> | <em>d</em><span class="inlinecode">.</span>&#x2009;<b>)</b>&#x2009;. The fourth row uses concatenation again to complete the NFA. We have labeled the states in the final automaton for reference in subsequent figures.</figcaption></figure>
</div></div>
</div>
<p class="textfl"/>
</section>
<section>
<h4 id="s0055" class="h3hd"><a id="st0145"/>From an NFA to a DFA</h4>
<p id="p0425" class="textfl"/>
<div class="boxg1" id="enun0095">
<p class="b1num">Example 2.14 </p>
<p class="b1title">DFA for <em>d</em><b>&#xFF0A;(</b>&#x2009;<span class="inlinecode">.</span><em>d</em> | <em>d</em><span class="inlinecode">.</span>&#x2009;<b>)</b>&#x2009;<em>d</em><b>&#xFF0A;</b></p>
<div>
<p id="p0430" class="b1textfl">With no way to &#x201C;guess&#x201D; the right transition to take from any given state, any practical implementation of an NFA would need to explore all possible transitions, concurrently or via backtracking. To avoid such a complex and time-consuming strategy, we can use a &#x201C;set of subsets&#x201D; construction to transform the NFA into an equivalent DFA. The key idea is for the state of the DFA after reading a given input to represent the <em>set</em> of states that the NFA might have reached on <span epub:type="pagebreak" id="page_60" aria-label="Page 60" role="doc-pagebreak"/>the same input. We illustrate the construction in Figure <a href="#f0115" id="cf0275">2.9</a> using the NFA from Figure <a href="#f0110" id="cf0280">2.8</a>. Initially, before it consumes any input, the NFA may be in State 1, or it may make epsilon transitions to States 2, 4, 5, or 8. We thus create an initial State <em>A</em> for our DFA to represent this set. On an input of <em>d</em>, our NFA may move from State 2 to State 3, or from State 8 to State 9. It has no other transitions on this input from any of the states in <em>A</em>. From State 3, however, the NFA may make epsilon transitions to any of States 2, 4, 5, or 8. We therefore create DFA State <em>B</em> as shown.</p>
<div class="pageavoid"><figure class="fig" id="f0115"><img src="images/B9780323999663000118/gr009.png" alt="Figure 2.9"/><figcaption class="figleg"><span class="fignum">Figure 2.9</span> <b>A DFA equivalent to the NFA at the bottom of Figure</b> <a href="#f0110" id="cf0030">2.8</a><b>.</b> Each state of the DFA represents the <em>set</em> of states that the NFA could be in after seeing the same input.</figcaption></figure>
</div>
<p id="p0435" class="b1text">On a <span class="inlinecode">.</span>, our NFA may move from State 5 to State 6. There are no other transitions on this input from any of the states in <em>A</em>, and there are no epsilon transitions out of State 6. We therefore create the singleton DFA State <em>C</em> as shown. None of States <em>A</em>, <em>B</em>, or <em>C</em> is marked as final, because none contains a final state of the original NFA.</p>
<p id="p0440" class="b1text">Returning to State <em>B</em> of the growing DFA, we note that on an input of <em>d</em> the original NFA may move from State 2 to State 3, or from State 8 to State 9. From State 3, in turn, it may move to States 2, 4, 5, or 8 via epsilon transitions. As these are exactly the states already in <em>B</em>, we create a self-loop in the DFA. Given a <span class="inlinecode">.</span>, on the other hand, the original NFA may move from State 5 to State 6, or from State 9 to State 10. From State 10, in turn, it may move to States 11, 12, or 14 via epsilon transitions. We therefore create DFA State <em>D</em> as shown, with a transition on <span class="inlinecode">.</span> from <em>B</em> to <em>D</em>. State <em>D</em> is marked as final because it contains state 14 of the original NFA. That is, given input <em>d</em><span class="inlinecode">.</span>, there exists a path from the start state to the end state of the original NFA. Continuing our enumeration of state sets, we end up creating three more, labeled <em>E</em>, <em>F</em>, and <em>G</em> in Figure <a href="#f0115" id="cf0285">2.9</a>. Like State <em>D</em>, these all contain State 14 of the original NFA, and thus are marked as final. &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p0445" class="text">In our example, the DFA ends up being smaller than the NFA, but this is only because our regular language is so simple. In theory, the number of <span epub:type="pagebreak" id="page_61" aria-label="Page 61" role="doc-pagebreak"/>states in the DFA may be exponential in the number of states in the NFA, but this extreme is also uncommon in practice. For a programming language scanner, the DFA tends to be larger than the NFA, but not outlandishly so. We consider space complexity in more detail in Section C-2.4.1.</p>
</section>
<section>
<h4 id="s0060" class="h3hd"><a id="st0155"/>Minimizing the DFA</h4>
<p id="p0450" class="textfl">Starting from a regular expression, we have now constructed an equivalent DFA. </p>
<div class="boxg1" id="enun0100">
<p class="b1num">Example 2.15 </p>
<p class="b1title">Minimal DFA for <em>d</em><b>&#xFF0A;(</b>&#x2009;<span class="inlinecode">.</span><em>d</em> | <em>d</em><span class="inlinecode">.</span>&#x2009;<b>)</b>&#x2009;<em>d</em><b>&#xFF0A;</b></p>
<div>
<p id="p0455" class="b1textfl">While this DFA has seven states, a bit of thought suggests that a smaller one should exist. In particular, once we have seen both a <em>d</em> and a <span class="inlinecode">.</span>, the only valid transitions are on <em>d</em>, and we ought to be able to make do with a single final state. We can formalize this intuition, allowing us to apply it to any DFA, via the following inductive construction.</p>
<p id="p0460" class="b1text">Initially we place the states of the (not necessarily minimal) DFA into two equivalence classes: final states and nonfinal states. We then consider, in some arbitrary order, all the character labels that appear on transitions of the DFA. For each label <span class="inlinecode">c</span>, we check to see whether there exists an equivalence class <span class="hiddenClass"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="script">X</mi></math><!--<mml:math><mml:mi mathvariant="script">X</mml:mi></mml:math>--></span><span><img alt="Equation" class="icon1" src="images/B9780323999663000118/si3.png"/></span> such that when given <span class="inlinecode">c</span> as input, the states in <span class="hiddenClass"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="script">X</mi></math><!--<mml:math><mml:mi mathvariant="script">X</mml:mi></mml:math>--></span><span><img alt="Equation" class="icon1" src="images/B9780323999663000118/si3.png"/></span> make transitions to states in <span class="hiddenClass"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi><mo linebreak="goodbreak" linebreakstyle="after">&gt;</mo><mn>1</mn></math><!--<mml:math><mml:mi>k</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">&gt;</mml:mo><mml:mn>1</mml:mn></mml:math>--></span><span><img alt="Equation" class="icon1" src="images/B9780323999663000118/si4.png"/></span> different equivalence classes. If we find such a class, we partition it into <em>k</em> classes in such a way that all states in a given new class would move to a member of the same old class on <span class="inlinecode">c</span>. When we have considered all possible characters we are done, and the remaining equivalence classes constitute the states of the minimal DFA.</p>
<p id="p0465" class="b1text">In our example, the original placement puts states <em>D</em>, <em>E</em>, <em>F</em>, and <em>G</em> in one class (final states) and States <em>A</em>, <em>B</em>, and <em>C</em> in another, as shown on the first line of the table in Figure <a href="#f0120" id="cf0290">2.10</a>. If we then consider transitions on <span class="inlinecode">.</span>, we observe that our DFA transitions from <em>A</em> to <em>C</em>, remaining in the nonfinal equivalence class, but also from <em>B</em> to <em>D</em>, moving to the final class. This observation implies that <em>B</em> must be in a different equivalence class from <em>A</em>, as shown on the second line of the table. (There is no transition out of <em>C</em> on <span class="inlinecode">.</span>, so we treat it as staying in the same equivalence class.)</p>
<div class="pageavoid"><figure class="fig" id="f0120"><img src="images/B9780323999663000118/gr010.png" alt="Figure 2.10"/><figcaption class="figleg"><span class="fignum"><a href="#cf0290">Figure 2.10</a></span> <b>Minimization of the DFA of Figure</b> <a href="#f0115" id="cf0035">2.9</a><b>.</b> In each row of the table we split equivalence classes to reflect an observable difference among the states they contain. After considering all character labels in the original DFA (in this case only two), we are left with the four-state minimal DFA shown on the right.</figcaption></figure>
</div>
<p id="p0470" class="b1text">In a similar fashion, we consider digits <em>d</em>. (All digits trigger the same transitions, so we can consider them together as a group.) Here we observe that while <em>A</em> and <em>C</em> are currently in the same equivalence class, our DFA transitions on <em>d</em> from <em>A</em> to <em>B</em> and from <em>C</em> to <em>E</em>, where <em>B</em> and <em>E</em> are in different equivalence classes. This observation implies that <em>A</em> and <em>C</em> must be in separate equivalence classes, as shown on the third line of the table. In our simple example, we have now considered all character labels in the DFA, leaving us with the four-state minimal DFA shown at right.&#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"><span epub:type="pagebreak" id="page_62" aria-label="Page 62" role="doc-pagebreak"/></p>
</section></section>
<section>
<h3 id="s0065" class="h2hd"><a id="st0165"/>2.2.2 Scanner Code</h3>
<p id="p0475" class="textfl">We can implement a scanner that explicitly captures the &#x201C;circles-and-arrows&#x201D; structure of a DFA in either of two main ways. One embeds the automaton in the control flow of the program using <span class="sans-serif">goto</span>s or nested <span class="sans-serif">case</span> (<span class="sans-serif">switch</span>) statements; the other, described in the following subsection, uses a table and a driver. As a general rule, handwritten automata tend to use nested <span class="sans-serif">case</span> statements, while most automatically generated automata use tables. Tables are hard to create by hand, but easier than code to create from within a program. Likewise, nested <span class="sans-serif">case</span> statements are easier to write and to debug than the ad hoc approach of Figure <a href="#f0095" id="cf0295">2.5</a>, if not quite as efficient. Unix's <span class="inlinecode">lex</span>/<span class="inlinecode">flex</span> tool produces C language output containing tables and a customized driver.</p>
<p id="p0480" class="text"/>
<div class="boxg1" id="b0025">
<p class="b1title">Design &amp; Implementation</p>
<p id="p0485" class="b1textfl"/>
</div>
<div class="boxg1" id="enun0105">
<p class="b1title">2.4 Recognizing multiple kinds of token</p>
<div>
<p id="p0490" class="b1textfl">One of the chief ways in which a scanner differs from a formal DFA is that it <em>identifies</em> tokens in addition to recognizing them. That is, it not only determines whether characters constitute a valid token; it also indicates <em>which one</em>. In practice, this means that it must have separate final states for every kind of token. We glossed over this issue in our RE-to-DFA constructions.</p>
<p id="p0495" class="b1text">To build a scanner for a language with <em>n</em> different kinds of tokens, we begin with an NFA of the sort suggested in the figure here. Given NFAs <span class="hiddenClass"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>M</mi></mrow><mrow><mi>i</mi></mrow></msub><mo>,</mo><mn>1</mn><mspace width="0.2em"/><mo>&#x2264;</mo><mspace width="0.2em"/><mi>i</mi><mspace width="0.2em"/><mo>&#x2264;</mo><mspace width="0.2em"/><mi>n</mi></math><!--<mml:math><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.2em"/><mml:mo>≤</mml:mo><mml:mspace width="0.2em"/><mml:mi>i</mml:mi><mml:mspace width="0.2em"/><mml:mo>≤</mml:mo><mml:mspace width="0.2em"/><mml:mi>n</mml:mi></mml:math>--></span><span><img alt="Equation" class="icon1" src="images/B9780323999663000118/si5.png"/></span> (one automaton for each kind of token), we create a new start state with epsilon transitions to the start states of the <span class="hiddenClass"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>M</mi></mrow><mrow><mi>i</mi></mrow></msub></math><!--<mml:math><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>--></span><span><img alt="Equation" class="icon1" src="images/B9780323999663000118/si6.png"/></span>s. In contrast to the alternation construction of Figure <a href="#f0105" id="cf0300">2.7</a>(c), however, we do <em>not</em> create a single final state; we keep the existing ones, each labeled by the token for which it is final.</p>
<p id="p0500" class="b1text"><img alt="Image 15" src="images/B9780323999663000118/fx015.png"/></p>
<p id="p0505" class="b1text">We then apply the NFA-to-DFA construction as before. (If final states for different tokens in the NFA ever end up in the same state of the DFA, then we have ambiguous token definitions. These may be resolved by changing the regular expressions from which the NFAs were derived, or by wrapping additional logic around the DFA.)</p>
<p id="p0510" class="b1text">In the DFA minimization construction, instead of starting with two equivalence classes (final and nonfinal states), we begin with <span class="hiddenClass"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo linebreak="goodbreak" linebreakstyle="after">+</mo><mn>1</mn></math><!--<mml:math><mml:mi>n</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">+</mml:mo><mml:mn>1</mml:mn></mml:math>--></span><span><img alt="Equation" class="icon1" src="images/B9780323999663000118/si7.png"/></span>, including a separate class for final states for each of the kinds of token. Exercise <a href="#o0315" id="cf0305">2.5</a> explores this construction for a scanner that recognizes both the <em>integer</em> and <em>decimal</em> types of Example <a href="#enun0025" id="cf0310">2.3</a>.</p>
</div></div>
<p class="textfl"/>
<p id="p0515" class="text"/>
<div class="boxg1" id="enun0110">
<p class="b1num">Example 2.16 </p>
<p class="b1title">Nested <span class="sans-serif">case</span> statement automaton</p>
<div>
<p id="p0520" class="b1textfl">The nested <span class="sans-serif">case</span> statement style of automaton has the following general structure:</p>
<p id="p0525" class="b1text"/>
<pre>  state := 1			&#x2013;&#x2013; start state
  loop
      read cur_char
      case state of
          1 : case cur_char of
		   &#x2018; &#x2019;, &#x2018;\t&#x2019;, &#x2018;\n&#x2019; : &#x2026;
		   &#x2018;a&#x2019;&#x2026; &#x2018;z&#x2019; :        &#x2026;
		   &#x2018;0&#x2019;&#x2026; &#x2018;9&#x2019; :        &#x2026;
		   &#x2018;>&#x2019; : &#x2026;
		   &#x2026;
          2 : case cur_char of
		   &#x2026;
 	       &#x2026;
          <em>n</em>: case cur_char of
		   &#x2026;</pre>

<p class="textfl"/>
<p id="p0530" class="b1text">The outer <span class="sans-serif">case</span> statement covers the states of the finite automaton. The inner <span class="sans-serif">case</span> statements cover the transitions out of each state. Most of the inner clauses simply set a new state. Some return from the scanner with the current token. (If the current character should not be part of that token, it is pushed back onto the input stream before returning.) &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p0535" class="text">Two aspects of the code typically deviate from the strict form of a formal finite automaton. One is the handling of keywords. The other is the need to peek ahead when a token can validly be extended by two or more additional characters, but not by only one.</p>
<p id="p0540" class="text"><span epub:type="pagebreak" id="page_63" aria-label="Page 63" role="doc-pagebreak"/>As noted at the beginning of Section <a href="#s0015" id="cf0315">2.1.1</a>, keywords in most languages look just like identifiers, but are reserved for a special purpose (some authors use the term <em>reserved word</em> instead of keyword). It is possible to write a finite automaton that distinguishes between keywords and identifiers, but it requires a <em>lot</em> of states (see Exercise <a href="#o0290" id="cf0320">2.3</a>). Most scanners, both handwritten and automatically generated, therefore treat keywords as &#x201C;exceptions&#x201D; to the rule for identifiers. Before returning an identifier to the parser, the scanner looks it up in a hash table or trie (a tree of branching paths) to make sure it isn't really a keyword.<sup><a href="#fn010" id="cf0325" epub:type="noteref" role="doc-noteref">9</a></sup></p>
<p id="p0545" class="text">Whenever one legitimate token is a prefix of another, the &#x201C;longest possible token&#x201D; rule says that we should continue scanning. <span epub:type="pagebreak" id="page_64" aria-label="Page 64" role="doc-pagebreak"/>If some of the intermediate strings are not valid tokens, however, we can't tell whether a longer token is possible without looking more than one character ahead. </p>
<div class="boxg1" id="enun0115">
<p class="b1num">Example 2.17 </p>
<p class="b1title">The nontrivial prefix problem</p>
<div>
<p id="p0550" class="b1textfl">This problem arises with dot characters (periods) in C. Suppose the scanner has just seen a <span class="inlinecode">3</span> and has a dot coming up in the input. It needs to peek at characters beyond the dot in order to distinguish between <span class="inlinecode">3.14</span> (a single token designating a real number), <span class="inlinecode">3 . foo</span> (three tokens that the scanner should accept, even though the parser will object to seeing them in that order), and <span class="inlinecode">3 ... foo</span> (again not syntactically valid, but three separate tokens nonetheless). In general, upcoming characters that a scanner must examine in order to make a decision are known as its <em>look-ahead</em>. In Section <a href="#s0085" id="cf0330">2.3</a> we will see a similar notion of look-ahead <em>tokens</em> in parsing. &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p0555" class="text"/>
<div class="boxg1" id="b0030">
<p class="b1title">Design &amp; Implementation</p>
<p id="p0560" class="b1textfl"/>
<div class="boxg1" id="enun0120">
</div>
<p class="b1title">2.5 Longest possible tokens</p>
<div>
<p id="p0565" class="b1textfl">A little care in syntax design&#x2014;avoiding tokens that are nontrivial prefixes of other tokens&#x2014;can dramatically simplify scanning. In straightforward cases of prefix ambiguity, the scanner can enforce the &#x201C;longest possible token&#x201D; rule automatically. In Fortran, however, the rules are sufficiently complex that no purely lexical solution suffices. Some of the problems, and a possible solution, are discussed in an article by Dyadkin <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0615">[Dya95]</span>.</p>
</div></div>
<p class="textfl"/>
<p id="p0570" class="text">In messier languages, a scanner may need to look an arbitrary distance ahead. </p>
<div class="boxg1" id="enun0125">
<p class="b1num">Example 2.18 </p>
<p class="b1title">Look-ahead in Fortran scanning</p>
<div>
<p id="p0575" class="b1textfl">In Fortran IV, for example, <span class="inlinecode">DO 5 I = 1,25</span> is the header of a loop (it executes the statements up to the one labeled <span class="inlinecode">5</span> for values of <span class="inlinecode">I</span> from 1 to 25), while <span class="inlinecode">DO 5 I = 1.25</span> is an assignment statement that places the value <span class="inlinecode">1.25</span> into the variable <span class="inlinecode">DO5I</span>. Spaces are ignored in (pre-Fortran 90) Fortran input, even in the middle of variable names. Moreover, variables need not be declared, and the terminator for a <span class="inlinecode">DO</span> loop is simply a label, which the parser can ignore. After seeing <span class="inlinecode">DO</span>, the scanner cannot tell whether the <span class="inlinecode">5</span> is part of the current token until it reaches the comma or dot. It has been widely (but apparently incorrectly) claimed that NASA's Mariner 1 space probe was lost due to accidental replacement of a comma with a dot in a case similar to this one in flight control software. <sup><a href="#fn011" id="cf0340" epub:type="noteref" role="doc-noteref">10</a></sup> Dialects of Fortran starting with Fortran 77 allow (in fact encourage) the use of alternative syntax for loop headers, in which an extra comma makes misinterpretation less likely: <span class="inlinecode">DO 5,I = 1,25</span>.&#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p0580" class="text">In C, the dot character problem can easily be handled as a special case. In languages requiring larger amounts of look-ahead, the scanner can take a more general approach. In any case of ambiguity, it assumes that a longer token will be possible, but remembers that a shorter token could have been recognized at some point in the past. It also buffers <span epub:type="pagebreak" id="page_65" aria-label="Page 65" role="doc-pagebreak"/>all characters read beyond the end of the shorter token. If the optimistic assumption leads the scanner into an error state, it &#x201C;unreads&#x201D; the buffered characters so that they will be seen again later, and returns the shorter token.</p>
</section>
<section>
<h3 id="s0070" class="h2hd"><a id="st0195"/>2.2.3 Table-Driven Scanning</h3>
<p id="p0585" class="textfl">In the preceding subsection we sketched how control flow&#x2014;a loop and nested <span class="sans-serif">case</span> statements&#x2014;can be used to represent a finite automaton. </p>
<div class="boxg1" id="enun0130">
<p class="b1num">Example 2.19 </p>
<p class="b1title">Table-driven scanning</p>
<div>
<p id="p0590" class="b1textfl">An alternative approach represents the automaton as a data structure: a two-dimensional <em>transition table</em>. A driver program (Figure <a href="#f0130" id="cf0355">2.11</a>) uses the current state and input character to index into the table. Each entry in the table specifies whether to move to a new state (and if so, which one), return a token, or announce an error. A second table indicates, for each state, whether we might be at the end of a token (and if so, which one). Separating this second table from the first allows us to notice when we pass a state that might have been the end of a token, so we can back up if we hit an error state. Example tables for our calculator tokens appear in Figure <a href="#f0135" id="cf0360">2.12</a>.</p>
<div class="pageavoid"><figure class="fig" id="f0130"><img src="images/B9780323999663000118/gr011.png" alt="Figure 2.11"/><figcaption class="figleg"><span class="fignum">Figure 2.11</span> <b>Driver for a table-driven scanner,</b> with code to handle the ambiguous case in which one valid token is a prefix of another, but some intermediate string is not.</figcaption></figure>
</div>
<div class="pageavoid"><figure class="fig" id="f0135"><img src="images/B9780323999663000118/gr012.png" alt="Figure 2.12"/><figcaption class="figleg"><span class="fignum">Figure 2.12</span> <b>Scanner tables for the calculator language.</b> These could be used by the code of Figure <a href="#f0130" id="cf0040">2.11</a>. States are numbered as in Figure <a href="#f0100" id="cf0045">2.6</a>, except for the addition of two states&#x2014;17 and 18&#x2014;to &#x201C;recognize&#x201D; white space and comments. The right-hand column represents table <span class="sans-serif">token_tab</span>; the rest of the figure is <span class="sans-serif">scan_tab</span>. Numbers in the table indicate an entry for which the corresponding <span class="sans-serif">action</span> is <span class="sans-serif">move</span>. Dashes appear where there is no way to extend the current token: if the corresponding entry in <span class="sans-serif">token_tab</span> is nonempty, then <span class="sans-serif">action</span> is <span class="sans-serif">recognize</span>; otherwise, <span class="sans-serif">action</span> is <span class="sans-serif">error</span>. Table <span class="sans-serif">keyword_tab</span> (not shown) contains the strings <span class="inlinecode">read</span> and <span class="inlinecode">write</span>.</figcaption></figure>
</div>
<p id="p0595" class="b1text">Like a handwritten scanner, the table-driven code of Figure <a href="#f0130" id="cf0365">2.11</a> looks tokens up in a table of keywords immediately before returning. An outer loop serves to filter out comments and &#x201C;white space&#x201D;&#x2014;spaces, tabs, and newlines. &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
</section>
<section>
<h3 id="s0075" class="h2hd"><a id="st0205"/>2.2.4 Lexical Errors</h3>
<p id="p0600" class="textfl">The code in Figure <a href="#f0130" id="cf0370">2.11</a> explicitly recognizes the possibility of <em>lexical errors</em>. In some cases the next character of input may be neither an acceptable continuation of the current token nor the start of another token. In such cases the scanner must print an error message and perform some sort of recovery so that compilation can continue, if only to look for additional errors. Fortunately, lexical errors are relatively rare&#x2014;most character sequences do correspond to token sequences&#x2014;and relatively easy to handle. The most common approach is simply to (1) throw away the current, invalid token; (2) skip forward until a character is found that can legitimately begin a new token; (3) restart the scanning algorithm; and (4) count on the error-recovery mechanism of the parser to cope with any cases in which the resulting sequence of tokens is not syntactically valid. Of course the need for error recovery is not unique to table-driven scanners; any scanner must cope with errors. We did not show the code in Figure <a href="#f0095" id="cf0375">2.5</a>, but it would have to be there in practice.</p>
<p id="p0605" class="text">The code in Figure <a href="#f0130" id="cf0380">2.11</a> also shows that the scanner must return both the kind of token found and its character-string image (spelling); again this requirement applies to all types of scanners. For some tokens the character-string image is redundant: all semicolons look the same, after all, as do all <span class="inlinecode">while</span> keywords. For other tokens, however (e.g., identifiers, character strings, and numeric constants), the image is needed for semantic analysis. It is also useful for error messages: &#x201C;undeclared identifier&#x201D; is not as nice as &#x201C;<span class="inlinecode">foo</span> has not been declared.&#x201D;<span epub:type="pagebreak" id="page_66" aria-label="Page 66" role="doc-pagebreak"/><span epub:type="pagebreak" id="page_67" aria-label="Page 67" role="doc-pagebreak"/></p>
</section>
<section>
<h3 id="s0080" class="h2hd"><a id="st0210"/>2.2.5 Pragmas</h3>
<p id="p0610" class="textfl">Some languages and language implementations allow a program to contain constructs called <em>pragmas</em> that provide directives or hints to the compiler. Pragmas that do not change program semantics&#x2014;only the compilation process&#x2014;are sometimes called <em>significant comments</em>. In some languages the name is also appropriate because, like comments, pragmas can appear anywhere in the source program. In this case they are usually processed by the scanner: allowing them anywhere in the grammar would greatly complicate the parser. In most languages, however, pragmas are permitted only at certain well-defined places in the grammar. In this case they are best processed by the parser or semantic analyzer.</p>
<p id="p0615" class="text">Pragmas that serve as directives may</p>
<div><ol>
<li id="u0010" class="numlist">&#x25A0; &#xA0;Turn various kinds of run-time checks (e.g., pointer or subscript checking) on or off</li>
<li id="u0015" class="numlist">&#x25A0; &#xA0;Turn certain code improvements on or off (e.g., on in inner loops to improve performance; off otherwise to improve compilation speed)</li>
<li id="u0020" class="numlist">&#x25A0; &#xA0;<span epub:type="pagebreak" id="page_68" aria-label="Page 68" role="doc-pagebreak"/> Enable or disable performance profiling (statistics gathering to identify program bottlenecks)</li></ol>
</div>
<p class="textfl"/>
<p id="p0635" class="text">Some directives &#x201C;cross the line&#x201D; and change program semantics. In Ada, for example, the <span class="inlinecode">unchecked</span> pragma can be used to disable type checking. In OpenMP, which we will consider in Chapter 13, pragmas specify significant parallel extensions to Fortran, C and C++: creating, scheduling, and synchronizing threads. In this case the principal rationale for expressing the extensions as pragmas rather than more deeply integrated changes is to sharply delineate the boundary between the core language and the extensions, and to share a common set of extensions across languages.</p>
<p id="p0640" class="text">Pragmas that serve (merely) as hints provide the compiler with information about the source program that may allow it to do a better job:</p>
<div><ol>
<li id="u0025" class="numlist">&#x25A0; &#xA0;Variable <span class="inlinecode">x</span> is very heavily used (it may be a good idea to keep it in a register).</li>
<li id="u0030" class="numlist">&#x25A0; &#xA0;Subroutine <span class="inlinecode">F</span> is a pure function: its only effect on the rest of the program is the value it returns.</li>
<li id="u0035" class="numlist">&#x25A0; &#xA0;Subroutine <span class="inlinecode">S</span> is not (indirectly) recursive (its storage may be statically allocated).</li>
<li id="u0040" class="numlist">&#x25A0; &#xA0;32 bits of precision (instead of 64) suffice for floating-point variable <span class="inlinecode">x</span>.</li></ol>
</div>
<p class="textfl"/>
<p id="p0665" class="text">The compiler may ignore these in the interest of simplicity, or in the face of contradictory information.</p>
<p id="p0670" class="text">Standard syntax for pragmas was introduced in C++11 (where they are known as &#x201C;attributes&#x201D;). A function that prints an error message and terminates execution, for example, can be labeled <span class="inlinecode">[[noreturn]]</span>, to allow the compiler to optimize code around calls, or to issue more helpful error or warning messages. As of this writing, the set of supported attributes can be extended by vendors (by modifying the compiler), but not by ordinary programmers. The extent to which these attributes should be limited to hints (rather than directives) has been somewhat controversial. New pragmas in Java (which calls them &#x201C;annotations&#x201D;) and C# (which calls them &#x201C;attributes&#x201D;) <em>can</em> be defined by the programmer; we will return to these in Section 16.3.1.</p>
<p id="p0675" class="text"/>
<div class="boxg1" id="enun0135">
<p class="b1num"><img alt="Image 12" src="images/B9780323999663000118/fx012.png"/> Check Your Understanding </p>
<div>
<p id="p0680" class="b1textfl"/>
<div><ol>
<li id="o0080" class="b1numlista"><b>10.</b> &#xA0;List the tasks performed by the typical scanner.</li>
<li id="o0085" class="b1numlista"><b>11.</b> &#xA0;What are the advantages of an automatically generated scanner, in comparison to a handwritten one? Why do many commercial compilers use a handwritten scanner anyway?</li>
<li id="o0090" class="b1numlista"><b>12.</b> &#xA0;Explain the difference between deterministic and nondeterministic finite automata. Why do we prefer the deterministic variety for scanning?</li>
<li id="o0095" class="b1numlista"><b>13.</b> &#xA0;Outline the constructions used to turn a set of regular expressions into a minimal DFA.</li>
<li id="o0100" class="b1numlista"><b>14.</b> &#xA0;What is the &#x201C;longest possible token&#x201D; rule?</li>
<li id="o0105" class="b1numlista"><b>15.</b> &#xA0;<span epub:type="pagebreak" id="page_69" aria-label="Page 69" role="doc-pagebreak"/> Why must a scanner sometimes &#x201C;peek&#x201D; at upcoming characters?</li>
<li id="o0110" class="b1numlista"><b>16.</b> &#xA0;What is the difference between a <em>keyword</em> and an <em>identifier</em>?</li>
<li id="o0115" class="b1numlista"><b>17.</b> &#xA0;Why must a scanner save the text of tokens?</li>
<li id="o0120" class="b1numlista"><b>18.</b> &#xA0;How does a scanner identify lexical errors? How does it respond?</li>
<li id="o0125" class="b1numlista"><b>19.</b> &#xA0;What is a <em>pragma</em>?</li></ol>
</div>
<p class="b1textfl"/>
</div></div>
<p class="textfl"/>
</section></section>
<section>
<h2 class="h1hd" id="s0085"><a id="st0215"/>2.3 Parsing</h2>
<p id="p0735" class="textfl">The parser is the heart of a typical compiler. It calls the scanner to obtain the tokens of the input program, assembles the tokens together into a syntax tree, and passes the tree (perhaps one subroutine at a time) to the later phases of the compiler, which perform semantic analysis and code generation and improvement. In effect, the parser is &#x201C;in charge&#x201D; of the entire compilation process; this style of compilation is sometimes referred to as <em>syntax-directed translation</em>.</p>
<p id="p0740" class="text">As noted in the introduction to this chapter, a context-free grammar (CFG) is a <em>generator</em> for a CF language. A parser is a language <em>recognizer</em>. It can be shown that for any CFG we can create a parser that runs in <span class="hiddenClass"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mrow><mi>n</mi></mrow><mrow><mn>3</mn></mrow></msup><mo stretchy="false">)</mo></math><!--<mml:math><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:math>--></span><span><img alt="Equation" class="icon1" src="images/B9780323999663000118/si8.png"/></span> time, where <em>n</em> is the length of the input program. <sup><a href="#fn012" id="cf0385" epub:type="noteref" role="doc-noteref">11</a></sup> There are two well-known parsing algorithms that achieve this bound: Earley's algorithm <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0625">[Ear70]</span> and the Cocke&#x2013;Younger&#x2013;Kasami (CYK) algorithm <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br1200">[Kas65</span>, <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br2275">You67]</span>. Cubic time is much too slow for parsing sizable programs, but fortunately not all grammars require such a general and slow parsing algorithm. There are large classes of grammars for which we can build parsers that run in linear time. The two most important of these classes are called LL and LR (Figure <a href="#f0140" id="cf0395">2.13</a>).</p>
<div class="pageavoid"><figure class="fig" id="f0140"><img src="images/B9780323999663000118/gr013.jpg" alt="Figure 2.13"/><figcaption class="figleg"><span class="fignum"><a href="#cf0395">Figure 2.13</a></span> <b>Principal classes of linear-time parsing algorithms</b>.</figcaption></figure>
</div>
<p id="p0745" class="text">LL stands for &#x201C;Left-to-right, Left-most derivation.&#x201D; LR stands for &#x201C;Left-to-right, Right-most derivation.&#x201D; In both classes the input is read left-to-right, and the parser attempts to discover (construct) a derivation of that input. For LL parsers, the derivation will be left-most; for LR parsers, right-most. We will cover LL parsers first. They are generally considered to be simpler and <span epub:type="pagebreak" id="page_70" aria-label="Page 70" role="doc-pagebreak"/>easier to understand. They can be written by hand or generated automatically from an appropriate grammar by a parser-generating tool. The class of LR grammars is larger (i.e., more grammars are LR than LL), and some people find the structure of the LR grammars more intuitive, especially in the handling of arithmetic expressions. LR parsers are almost always constructed by a parser-generating tool. Both classes of parsers are used in production compilers, though LR parsers are more common.</p>
<p id="p0750" class="text">LL parsers are also called &#x201C;top-down,&#x201D; or &#x201C;predictive&#x201D; parsers. They construct a parse tree from the root down, predicting at each step which production will be used to expand the current node, based on the next available token of input. LR parsers are also called &#x201C;bottom-up&#x201D; parsers. They construct a parse tree from the leaves up, recognizing when a collection of leaves or other nodes can be joined together as the children of a single parent.</p>
<p id="p0755" class="text"/>
<div class="boxg1" id="enun0140">
<p class="b1num">Example 2.20 </p>
<p class="b1title">Top-down and bottom-up parsing</p>
<div>
<p id="p0760" class="b1textfl">We can illustrate the difference between top-down and bottom-up parsing by means of a simple example. Consider the following grammar for a comma-separated list of identifiers, terminated by a semicolon:</p>
<pre>  <em>id_list</em> &#x27F6; id <em>id_list_tail</em>
  <em>id_list_tail</em> &#x27F6;  , id <em>id_list_tail</em>
  <em>id_list_tail</em> &#x27F6;  ;</pre>
<p class="textfl"/>
<p id="p0765" class="b1text">These are the productions that would normally be used for an identifier list in a top-down parser. They can also be parsed bottom-up (most top-down grammars can be). In practice they would not be used in a bottom-up parser, for reasons that will become clear in a moment, but the ability to handle them either way makes them good for this example.</p>
<p id="p0770" class="b1text">Progressive stages in the top-down and bottom-up construction of a parse tree for the string <span class="inlinecode">A, B, C;</span> appear in Figure <a href="#f0150" id="cf0400">2.14</a>. The top-down parser begins by predicting that the root of the tree (<em>id_list</em>) will expand to <em><span class="inlinecode">id</span> id_list_tail</em>. It then matches the <em><span class="inlinecode">id</span></em> against a token obtained from the scanner. (If the scanner produced something different, the parser would announce a syntax error.) The parser then moves down into the first (in this case only) nonterminal child and predicts that <em>id_list_tail</em> will expand to <em><span class="inlinecode">, id</span> id_list_tail</em>. To make this prediction it needs to peek at the upcoming token (a comma), which allows it to choose between the two possible expansions for <em>id_list_tail</em>. It then matches the comma and the <em><span class="inlinecode">id</span></em> and moves down into the next <em>id_list_tail</em>. In a similar, recursive fashion, the top-down parser works down the tree, left-to-right, predicting and expanding nodes and tracing out a left-most derivation of the fringe of the tree.</p>
<div class="pageavoid"><figure class="fig" id="f0150"><img src="images/B9780323999663000118/gr014.png" alt="Figure 2.14"/><figcaption class="figleg"><span class="fignum">Figure 2.14</span> <b>Top-down (<em>left</em>) and bottom-up parsing (<em>right</em>)</b> of the input string <span class="inlinecode">A, B, C;</span>. Grammar appears at lower left.</figcaption></figure>
</div>
<p id="p0775" class="b1text">The bottom-up parser, by contrast, begins by noting that the left-most leaf of the tree is an <em><span class="inlinecode">id</span></em>. The next leaf is a comma and the one after that is another <em><span class="inlinecode">id</span></em>. The parser continues in this fashion, shifting new leaves from the scanner into a forest of partially completed parse tree fragments, until it realizes that some of those fragments constitute a complete right-hand side. In this grammar, that doesn't occur until the parser has seen the semicolon&#x2014;the right-hand side of <em>id_list_tail</em> &#x27F6;<em>&#x2009;<span class="inlinecode">;</span></em>. With this right-hand side in hand, the parser reduces the semicolon to an <em>id_list_tail</em>. It then reduces <em><span class="inlinecode">,</span></em> <span epub:type="pagebreak" id="page_71" aria-label="Page 71" role="doc-pagebreak"/><em><span class="inlinecode">id</span> id_list_tail</em> into another <em>id_list_tail</em>. After doing this one more time it is able to reduce <em><span class="inlinecode">id</span> id_list_tail</em> into the root of the parse tree, <em>id_list</em>.</p>
<p id="p0780" class="b1text">At no point does the bottom-up parser predict what it will see next. Rather, it shifts tokens into its forest until it recognizes a right-hand side, which it then reduces to a <span epub:type="pagebreak" id="page_72" aria-label="Page 72" role="doc-pagebreak"/>left-hand side. Because of this behavior, bottom-up parsers are sometimes called <em>shift-reduce</em> parsers. Moving up the figure, from bottom to top, we can see that the shift-reduce parser traces out a right-most derivation, in reverse. Because bottom-up parsers were the first to receive careful formal study, right-most derivations are sometimes called <em>canonical</em>. &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p0785" class="text">There are several important subclasses of LR parsers, including SLR, LALR, and &#x201C;full LR.&#x201D; SLR and LALR are important for their ease of implementation, full LR for its generality. LL parsers can also be grouped into SLL and &#x201C;full LL&#x201D; subclasses. We will cover the differences among them only briefly here; for further information see any of the standard compiler-construction or parsing theory textbooks <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0105">[App97</span>, <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0065">ALSU07</span>, <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0150">AU72</span>, <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0480">CT11</span>, <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0665">FCL10</span>, <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0750">GBJ+12]</span>.</p>
<p id="p0790" class="text">One commonly sees LL or LR (or whatever) written with a number in parentheses after it: LL(2) or LALR(1), for example. This number indicates how many tokens of look-ahead are required in order to parse. Most real compilers use just one token of look-ahead, though more can sometimes be helpful. The open-source ANTLR tool, in particular, uses multitoken look-ahead to enlarge the class of languages amenable to top-down parsing <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br1710">[PQ95]</span>. In Section <a href="#s0090" id="cf0410">2.3.1</a> we will look at LL(1) grammars and handwritten parsers in more detail. In Sections <a href="#s0100" id="cf0415">2.3.3</a> and <a href="#s0105" id="cf0420">2.3.4</a> we will consider automatically generated LL(1) and LR(1) (actually SLR(1)) parsers.</p>
<p id="p0795" class="text"/>
<div class="boxg1" id="enun0145">
<p class="b1num">Example 2.21 </p>
<p class="b1title">Bounding space with a bottom-up grammar</p>
<div>
<p id="p0800" class="b1textfl">The problem with our example grammar, for the purposes of bottom-up parsing, is that it forces the compiler to shift all the tokens of an <em>id_list</em> into its forest before it can reduce any of them. In a very large program we might run out of space. Sometimes there is nothing that can be done to avoid a lot of shifting. In this case, however, we can use an alternative grammar that allows the parser to reduce prefixes of the <em>id_list</em> into nonterminals as it goes along:</p>
<pre>  <em>id_list</em> &#x27F6; <em>id_list_prefix</em> ;
  <em>id_list_prefix</em> &#x27F6; <em>id_list_prefix</em> , id
	     &#x27F6; id</pre>
<p class="textfl"> This grammar cannot be parsed top-down, because when we see an <span class="inlinecode">id</span> on the input and we're expecting an <em>id_list_prefix</em>, we have no way to tell which of the two possible productions we should predict (more on this dilemma in Section <a href="#s0095" id="cf0425">2.3.2</a>). As shown in Figure <a href="#f0160" id="cf0430">2.15</a>, however, the grammar works well bottom-up. &#x2003;&#x25A0;</p>
<div class="pageavoid"><figure class="fig" id="f0160"><img src="images/B9780323999663000118/gr015.png" alt="Figure 2.15"/><figcaption class="figleg"><span class="fignum">Figure 2.15</span> <b>Bottom-up parse of <span class="inlinecode">A, B, C;</span></b> using a grammar (lower left) that allows lists to be collapsed incrementally.</figcaption></figure>
</div></div>
</div>
<p class="textfl"/>
<section>
<h3 id="s0090" class="h2hd"><a id="st0230"/>2.3.1 Recursive Descent</h3>
<p id="p0805" class="textfl"/>
<div class="boxg1" id="enun0150">
<p class="b1num">Example 2.22 </p>
<p class="b1title">Top-down grammar for a calculator language</p>
<div>
<p id="p0810" class="b1textfl">To illustrate top-down (predictive) parsing, let us consider the grammar for a simple &#x201C;calculator&#x201D; language, shown in Figure <a href="#f0165" id="cf0435">2.16</a>. The calculator allows values to be read into named variables, which may then be used in expressions. Expressions in turn may be written to the output. Control flow is strictly linear (no loops, <span class="inlinecode">if</span> statements, or other jumps). In a pattern that will repeat in many of our examples, we have included an initial <em>augmenting</em> production, <em>program</em> &#x27F6;<em>&#x2009;stmt_list</em> <span class="inlinecode">$$</span>, which arranges for the <span epub:type="pagebreak" id="page_73" aria-label="Page 73" role="doc-pagebreak"/>&#x201C;real&#x201D; body of the program (<em>stmt_list</em>) to be followed by a special <em>end marker</em> token, <span class="inlinecode">$$</span>. The end marker is produced by the scanner at the end of the input. Its presence allows the parser to terminate cleanly once it has seen the entire program, and to decline to accept programs with extra garbage tokens at the end. As in regular expressions, we use the symbol <em>&#x3B5;</em> to denote the empty string. A production with <em>&#x3B5;</em> on the right-hand side is sometimes called an <em>epsilon production</em>.</p>
<div class="pageavoid"><figure class="fig" id="f0165"><img src="images/B9780323999663000118/gr016.png" alt="Figure 2.16"/><figcaption class="figleg"><span class="fignum">Figure 2.16</span> <b>LL(1) grammar for a simple calculator language</b>.</figcaption></figure>
</div>
<p id="p0815" class="b1text">It may be helpful to compare the <em>expr</em> portion of Figure <a href="#f0165" id="cf0440">2.16</a> to the expression grammar of Example <a href="#enun0055" id="cf0445">2.8</a>. Most people find that previous, LR grammar to be significantly more intuitive. It suffers, however, from a problem similar to that of the <em>id_list</em> grammar of Example <a href="#enun0145" id="cf0450">2.21</a>: if we see an <span class="inlinecode">id</span> on the input when expecting an <em>expr</em>, we have no way to tell which of the two possible productions to predict. The grammar of Figure <a href="#f0165" id="cf0455">2.16</a> avoids this problem by merging the common prefixes<span epub:type="pagebreak" id="page_74" aria-label="Page 74" role="doc-pagebreak"/> of right-hand sides into a single production, and by using new symbols (<em>term_tail</em> and <em>factor_tail</em>) to generate additional operators and operands as required. The transformation has the unfortunate side effect of placing the operands of a given operator in separate right-hand sides. In effect, we have sacrificed grammatical elegance in order to be able to parse predictively. &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p0820" class="text">So how do we parse a string with our calculator grammar? We saw the basic idea in Figure <a href="#f0150" id="cf0460">2.14</a>. We start at the top of the tree and predict needed productions on the basis of the current left-most nonterminal in the tree and the current input token. We can formalize this process in one of two ways. The first, described in the remainder of this subsection, is to build a <em>recursive descent parser</em> whose subroutines correspond, one-to-one, to the nonterminals of the grammar. Recursive descent parsers are typically constructed by hand, though the ANTLR parser generator constructs them automatically from an input grammar. The second approach, described in Section <a href="#s0100" id="cf0465">2.3.3</a>, is to build an <em>LL parse table</em>, which is then read by a driver program. Table-driven parsers are almost always constructed automatically by a parser generator. These two options&#x2014;recursive descent and table-driven&#x2014;are reminiscent of the nested <span class="sans-serif">case</span> statements and table-driven approaches to building a scanner that we saw in Sections <a href="#s0065" id="cf0470">2.2.2</a> and <a href="#s0070" id="cf0475">2.2.3</a>. It should be emphasized that they implement the same basic parsing algorithm.</p>
<p id="p0825" class="text">Historically, recursive descent parsers were often used when a parser-generator tool was not available, or when the language to be parsed was relatively simple. In recent years, however, recursive descent has been adopted by many production compilers, including <span class="inlinecode">gcc</span> and <span class="inlinecode">clang</span>. Earlier versions of <span class="inlinecode">gcc</span> used <span class="inlinecode">bison</span> to create a bottom-up parser automatically. The change was made in part for performance reasons and in part to enable the generation of higher-quality syntax error messages.</p>
<p id="p0830" class="text"/>
<div class="boxg1" id="enun0155">
<p class="b1num">Example 2.23 </p>
<p class="b1title">Recursive descent parser for the calculator language</p>
<div>
<p id="p0835" class="b1textfl">Pseudocode for a recursive descent parser for our calculator language appears in Figure <a href="#f0170" id="cf0480">2.17</a>. It has a subroutine for every nonterminal in the grammar. It also has a mechanism <span class="sans-serif">input_token</span> to inspect the next token available from the scanner and a subroutine (<span class="sans-serif">match</span>) to consume and update this token, and in the process verify that it is the one that was expected (as specified <span epub:type="pagebreak" id="page_75" aria-label="Page 75" role="doc-pagebreak"/>by an argument). If <span class="sans-serif">match</span> or any of the other subroutines sees an unexpected token, then a syntax error has occurred. For the time being let us assume that the <span class="sans-serif">parse_error</span> subroutine simply prints a message and terminates the parse. In Section <a href="#s0135" id="cf0485">2.3.5</a> we will consider how to recover from such errors and continue to parse the remainder of the input. &#x2003;&#x25A0;</p>
<div class="pageavoid">
<figure class="fig" id="f0170"><img src="images/B9780323999663000118/gr017.png" alt="Figure 2.17"/></figure>
<figure class="fig" id="f0175"><img src="images/B9780323999663000118/gr018.png" alt="Image"/>
<figcaption class="figleg"><span class="fignum">Figure 2.17</span> <b>Recursive descent parser for the calculator language.</b> Execution begins in procedure <span class="sans-serif">program</span>. The recursive calls trace out a traversal of the parse tree. Not shown is code to save this tree (or some similar structure) for use by later phases of the compiler. <em>(continued)</em></figcaption></figure>
</div></div>
</div>
<p class="textfl"/>
<p id="p0840" class="text"/>
<p class="textfl"/>
<p id="p0845" class="text"/>
<div class="boxg1" id="enun0160">
<p class="b1num">Example 2.24 </p>
<p class="b1title">Recursive descent parse of a &#x201C;sum and average&#x201D; program</p>
<div>
<p id="p0850" class="b1textfl">Suppose now that we are to parse a simple program to read two numbers and print their sum and average:</p>
<pre>  read A			
  read B
  sum := A + B
  write sum
  write sum / 2</pre>
<p id="p0880" class="b1text">The parse tree for this program appears in Figure <a href="#f0180" id="cf0490">2.18</a>. The parser begins by calling the subroutine <span class="sans-serif">program</span>. After noting that the initial token is a <span class="inlinecode">read</span>, <span class="sans-serif">program</span> calls <span class="sans-serif">stmt_list</span> and then attempts to match the end-of-file pseudotoken. (In the parse tree, the root, <em>program</em>, has two children, <em>stmt_list</em> and <span class="inlinecode">$$</span>.) Procedure <span class="sans-serif">stmt_list</span> again notes that the upcoming token is a <span class="inlinecode">read</span>. This observation allows it to determine that the current node (<em>stmt_list</em>) generates <em>stmt stmt_list</em> (rather than <em>&#x3B5;</em>). It therefore calls <span class="sans-serif">stmt</span> and <span class="sans-serif">stmt_list</span> before returning. Continuing in this fashion, the execution path of the parser traces out a left-to-right depth-first traversal of the parse tree. This correspondence between the dynamic execution trace and the structure of the parse tree is the distinguishing characteristic of recursive descent parsing. Note that because the <em>stmt_list</em> nonterminal appears in the right-hand side of a <em>stmt_list</em> production, the <span class="sans-serif">stmt_list</span> subroutine must call itself. This recursion accounts for the name of the parsing technique.&#x2003;&#x2003;&#x25A0;</p>
<div class="pageavoid"><figure class="fig" id="f0180"><img src="images/B9780323999663000118/gr019.png" alt="Figure 2.18"/><figcaption class="figleg"><span class="fignum"><a href="#cf0490">Figure 2.18</a></span> <b>Parse tree for the sum-and-average program of Example</b> <a href="#enun0160" id="cf0050">2.24</a><b>, using the grammar of Figure</b> <a href="#f0165" id="cf0055">2.16</a><b>.</b></figcaption></figure>
</div></div>
</div>
<p class="textfl"/>
<p id="p0885" class="text">Without additional code (not shown in Figure <a href="#f0170" id="cf0495">2.17</a>), the parser merely verifies that the program is syntactically correct (i.e., that none of the <span class="sans-serif">otherwise parse_ error</span> clauses in the <span class="sans-serif">case</span> statements are executed and that <span class="sans-serif">match</span> always sees what it expects to see). To be of use to the rest of the compiler&#x2014;which must produce an equivalent target program in some other language&#x2014;the parser must save the parse tree or some other representation of program fragments as an explicit data structure. To save the parse tree itself, we can allocate and link together records to represent the children of a node immediately before executing the recursive subroutines and <span class="sans-serif">match</span> invocations that represent those children. We shall need to pass each recursive routine an argument that points to the record that is to be expanded (i.e., whose children are to be discovered). Procedure <span class="sans-serif">match</span> will also need to save information about certain tokens (e.g., character-string representations of identifiers and literals) in the leaves of the tree.</p>
<p id="p0890" class="text">As we saw in Chapter 1, the parse tree contains a great deal of irrelevant detail that need not be saved for the rest of the compiler. It is therefore rare for a parser to construct a full parse tree explicitly. More often it produces an abstract syntax tree or some other more terse representation. In a recursive descent compiler, a syntax tree can be created by allocating and linking together records in only a subset of the recursive calls.</p>
<p id="p0895" class="text">The trickiest part of writing a recursive descent parser is figuring out which tokens should label the arms of the <span class="sans-serif">case</span> statements. <span epub:type="pagebreak" id="page_76" aria-label="Page 76" role="doc-pagebreak"/><span epub:type="pagebreak" id="page_77" aria-label="Page 77" role="doc-pagebreak"/>Each arm represents one production: one possible expansion of the symbol for which the subroutine was named. The tokens that label a given arm are those that <em>predict</em> the production. A token <span class="inlinecode">X</span> may predict a production for either of two reasons: (1) the right-hand side of the production, when recursively expanded, may yield a string beginning with <span class="inlinecode">X</span>, or (2) the right-hand side may yield nothing (i.e., it is <em>&#x3B5;</em>, or a string of nonterminals that may recursively yield <em>&#x3B5;</em>), and <span class="inlinecode">X</span> may begin the yield of what comes <em>next</em>. We will formalize this notion of prediction in Section <a href="#s0100" id="cf0500">2.3.3</a>, using sets called FIRST and FOLLOW, and show how to derive them automatically from an LL(1) CFG.</p>
<p id="p0900" class="text"/>
<div class="boxg1" id="enun0165">
<p class="b1num"><img alt="Image 12" src="images/B9780323999663000118/fx012.png"/> Check Your Understanding </p>
<div>
<p id="p0905" class="b1textfl"/>
<div><ol>
<li id="o0130" class="b1numlista"><b>20.</b> &#xA0;What is the inherent &#x201C;big-O&#x201D; complexity of parsing? What is the complexity of parsers used in real compilers?</li>
<li id="o0135" class="b1numlista"><b>21.</b> &#xA0;Summarize the difference between LL and LR parsing. Which one of them is also called &#x201C;bottom-up&#x201D;? &#x201C;Top-down&#x201D;? Which one is also called &#x201C;predictive&#x201D;? &#x201C;Shift-reduce&#x201D;? What do &#x201C;LL&#x201D; and &#x201C;LR&#x201D; stand for?</li>
<li id="o0140" class="b1numlista"><b>22.</b> &#xA0;What kind of parser (top-down or bottom-up) is most common in production compilers?</li>
<li id="o0145" class="b1numlista"><b>23.</b> &#xA0;<span epub:type="pagebreak" id="page_78" aria-label="Page 78" role="doc-pagebreak"/> Why are right-most derivations sometimes called <em>canonical</em>?</li>
<li id="o0150" class="b1numlista"><b>24.</b> &#xA0;What is the significance of the &#x201C;1&#x201D; in LR(1)?</li>
<li id="o0155" class="b1numlista"><b>25.</b> &#xA0;Why might we want (or need) different grammars for different parsing algorithms?</li>
<li id="o0160" class="b1numlista"><b>26.</b> &#xA0;What is an <em>epsilon production</em>?</li>
<li id="o0165" class="b1numlista"><b>27.</b> &#xA0;What are <em>recursive descent</em> parsers? Why are they used mostly for small languages?</li>
<li id="o0170" class="b1numlista"><b>28.</b> &#xA0;How might a parser construct an explicit parse tree or syntax tree? <span epub:type="pagebreak" id="page_79" aria-label="Page 79" role="doc-pagebreak"/></li></ol>
</div>
<p class="b1textfl"/>
</div></div>
<p class="textfl"/>
</section>
<section>
<h3 id="s0095" class="h2hd"><a id="st0250"/>2.3.2 Writing an LL(1) Grammar</h3>
<p id="p0955" class="textfl">When designing a recursive descent parser, one has to acquire a certain facility in writing and modifying LL(1) grammars. The two most common obstacles to &#x201C;LL(1)-ness&#x201D; are <em>left recursion</em> and <em>common prefixes</em>.</p>
<p id="p0960" class="text"/>
<div class="boxg1" id="enun0170">
<p class="b1num">Example 2.25 </p>
<p class="b1title">Left recursion</p>
<div>
<p id="p0965" class="b1textfl">A grammar is said to be left recursive if there is a nonterminal <em>A</em> such that <em>A</em> <span class="hiddenClass"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mo stretchy="false">&#x27F9;</mo></mrow><mrow><mo linebreak="badbreak" linebreakstyle="after">+</mo></mrow></msup></math><!--<mml:math><mml:msup><mml:mrow><mml:mo stretchy="false">⟹</mml:mo></mml:mrow><mml:mrow><mml:mo linebreak="badbreak" linebreakstyle="after">+</mml:mo></mml:mrow></mml:msup></mml:math>--></span><span><img alt="Equation" class="icon1" src="images/B9780323999663000118/si14.png"/></span><em>&#x2009;A &#x3B1;</em> for some <em>&#x3B1;</em>. <sup><a href="#fn013" id="cf0505" epub:type="noteref" role="doc-noteref">12</a></sup> The trivial case occurs when the first symbol on the right-hand side of a production is the same as the symbol on the left-hand side. Here again is the grammar from Example <a href="#enun0145" id="cf0510">2.21</a>, which cannot be parsed top-down:</p>
<pre>  <em>id_list</em> &#x27F6; <em>id_list_prefix</em> ;
  <em>id_list_prefix</em> &#x27F6; <em>id_list_prefix</em> , id
	     &#x27F6; id</pre>
<p class="textfl"> The problem is in the second and third productions; in the <em>id_list_prefix</em> parsing routine, with <span class="inlinecode">id</span> on the input, a predictive parser cannot tell which of the productions it should use. (Recall that left recursion is <em>desirable</em> in bottom-up grammars, because it allows recursive constructs to be discovered incrementally, as in Figure <a href="#f0160" id="cf0515">2.15</a>.) &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p0970" class="text"/>
<div class="boxg1" id="enun0175">
<p class="b1num">Example 2.26 </p>
<p class="b1title">Common prefixes</p>
<div>
<p id="p0975" class="b1textfl">Common prefixes occur when two different productions with the same left-hand side begin with the same symbol or symbols. Here is an example that commonly appears in languages descended from Algol:</p>
<pre>  <em>stmt</em> &#x27F6; id := <em>expr</em>
      &#x27F6; id (<em>argument_list</em> )             &#x2013;&#x2013; procedure call</pre>
<p class="textfl"> With <span class="inlinecode">id</span> at the beginning of both right-hand sides, we cannot choose between them on the basis of the upcoming token. &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p0980" class="text">Both left recursion and common prefixes can be removed from a grammar mechanically. The general case is a little tricky (Exercise <a href="#o0535" id="cf0520">2.25</a>), because the prediction problem may be an indirect one (e.g., <em>S</em> &#x27F6;<em>&#x2009;A &#x3B1;</em> and <em>A</em> &#x27F6;<em>&#x2009;S &#x3B2;</em>, or <em>S</em> &#x27F6;<em>&#x2009;A &#x3B1;</em>, <em>S</em> &#x27F6;<em>&#x2009;B &#x3B2;</em>, <em>A</em> &#x27F9;*<em>&#x2009;<span class="inlinecode">c</span> &#x3B3;</em>, and <em>B</em> &#x27F9;*<em>&#x2009;<span class="inlinecode">c</span> &#x3B4;</em>). We can see the general idea in the examples above, however.</p>
<p id="p0985" class="text"/>
<div class="boxg1" id="enun0180">
<p class="b1num">Example 2.27 </p>
<p class="b1title">Eliminating left recursion</p>
<div>
<p id="p0990" class="b1textfl">Our left-recursive definition of <em>id_list</em> can be replaced by the right-recursive variant we saw in Example <a href="#enun0140" id="cf0525">2.20</a>:</p>
<pre>  <em>id_list</em> &#x27F6; id <em>id_list_tail</em>
  <em>id_list_tail</em> &#x27F6; , id <em>id_list_tail</em>
  <em>id_list_tail</em> &#x27F6; ;</pre>
<p class="textfl"> &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p0995" class="text"><span epub:type="pagebreak" id="page_80" aria-label="Page 80" role="doc-pagebreak"/></p>
<div class="boxg1" id="enun0185">
<p class="b1num">Example 2.28 </p>
<p class="b1title">Left factoring</p>
<div>
<p id="p1000" class="b1textfl">Our common-prefix definition of <em>stmt</em> can be made LL(1) by a technique called <em>left factoring</em>:</p>
<pre>  <em>stmt</em> &#x27F6; id <em>stmt_list_tail</em>
  <em>stmt_list_tail</em> &#x27F6; := <em>expr</em> | (<em>argument_list</em> )</pre>
<p class="textfl"> &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p1005" class="text">Of course, simply eliminating left recursion and common prefixes is <em>not</em> guaranteed to make a grammar LL(1), because other obstacles may remain. There are infinitely many non-LL <em>languages</em> &#x2014;languages for which no LL grammar exists. Fortunately, the few non-LL languages that arise in practice can generally be handled by augmenting the parsing algorithm with one or two simple heuristics.</p>
<p id="p1010" class="text"/>
<div class="boxg1" id="enun0190">
<p class="b1num">Example 2.29 </p>
<p class="b1title">Parsing a &#x201C;dangling <span class="inlinecode">else</span>&#x201D;</p>
<div>
<p id="p1015" class="b1textfl">The best known example of a &#x201C;not quite LL&#x201D; construct arises in languages like Pascal, in which the <span class="inlinecode">else</span> part of an <span class="inlinecode">if</span> statement is optional. The natural grammar fragment</p>
<pre>  <em>stmt</em> &#x27F6; if <em>condition then_clause else_clause</em> | <em>other_stmt</em>
  <em>then_clause</em> &#x27F6; then <em>stmt</em>
  <em>else_clause</em> &#x27F6; else <em>stmt</em> | <em>&#x03B5;</em></pre>
<p class="textfl"> is ambiguous (and thus neither LL nor LR); it allows the <span class="inlinecode">else</span> in <span class="inlinecode">if C</span><sub>1</sub> <span class="inlinecode">then if C</span><sub>2</sub> <span class="inlinecode">then S</span><sub>1</sub> <span class="inlinecode">else S</span><sub>2</sub> to be paired with either <span class="inlinecode">then</span>. The less natural grammar fragment</p>

<pre>  <em>stmt</em> &#x27F6; <em>balanced_stmt</em> | <em>unbalanced_stmt</em>
  <em>balanced_stmt</em> &#x27F6; if <em>condition</em> then <em>balanced_stmt</em> else <em>balanced_stmt</em>
		 | <em>other_stmt</em>
  <em>unbalanced_stmt</em> &#x27F6; if <em>condition</em> then <em>stmt</em>
			| if <em>condition</em> then <em>balanced_stmt</em> else <em>unbalanced_stmt</em></pre>
<p class="textfl"> can be parsed bottom-up but not top-down (there is <em>no</em> pure top-down grammar for Pascal <span class="inlinecode">else</span> statements). A <em>balanced_stmt</em> is one with the same number of <span class="inlinecode">then</span>s and <span class="inlinecode">else</span>s. An <em>unbalanced_stmt</em> has more <span class="inlinecode">then</span>s. &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p1020" class="text">The usual approach, whether parsing top-down or bottom-up, is to use the ambiguous grammar together with a &#x201C;disambiguating rule,&#x201D; which says that in the case of a conflict between two possible productions, the one to use is the one that occurs first, textually, in the grammar. In the ambiguous fragment above, the fact that <em>else_clause</em> &#x27F6;<em>&#x2009;<span class="inlinecode">else</span> stmt</em> comes before <em>else_clause</em> &#x27F6;&#x2009;<em>&#x3B5;</em> ends up pairing the <span class="inlinecode">else</span> with the nearest <span class="inlinecode">then</span>.</p>
<p id="p1025" class="text">Better yet, a language designer can avoid this sort of problem by choosing different syntax. </p>
<div class="boxg1" id="enun0195">
<p class="b1num">Example 2.30 </p>
<p class="b1title">&#x201C;Dangling <span class="inlinecode">else</span>&#x201D; program bug</p>
<div>
<p id="p1030" class="b1textfl">The ambiguity of the <em>dangling else</em> problem in Pascal leads to problems not only in parsing, but in writing and maintaining correct programs. Most Pascal programmers at one time or another ended up writing a program like this one:</p>
<pre>  if P &lt;&gt; nil then			
      if P^.val = goal then
          foundIt := true
  else
      endOfList := true</pre>
<p id="p1060" class="b1text"><span epub:type="pagebreak" id="page_81" aria-label="Page 81" role="doc-pagebreak"/> Indentation notwithstanding, the Pascal manual states that an <span class="inlinecode">else</span> clause matches the closest unmatched <span class="inlinecode">then</span>&#x2014;in this case the inner one&#x2014;which is clearly not what the programmer intended. To get the desired effect, the Pascal programmer needed to write</p>
<pre>  if P &lt;&gt; nil then begin			
      if P^.val = goal then
          foundIt := true
  end
  else
      endOfList := true</pre>
<p class="textfl"> &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"> Many other Algol-family languages (including Modula, Modula-2, and Oberon, all more recent inventions of Pascal's designer, Niklaus Wirth) require explicit <em>end markers</em> on all structured statements. </p>
<div class="boxg1" id="enun0200">
<p class="b1num">Example 2.31 </p>
<p class="b1title">End markers for structured statements</p>
<div>
<p id="p1065" class="b1textfl">The grammar fragment for <span class="inlinecode">if</span> statements in Modula-2 looks something like this:</p>
<pre>  <em>stmt</em> &#x27F6; IF <em>condition then_clause else_clause</em> END | <em>other_stmt</em>
  <em>then_clause</em> &#x27F6; THEN <em>stmt_list</em>
  <em>else_clause</em> &#x27F6; ELSE <em>stmt_list</em> | <em>&#x03B5;</em></pre>
<p class="textfl"> The addition of the <span class="inlinecode">END</span> eliminates the ambiguity. &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p1070" class="text"/>
<div class="boxg1" id="b0035">
<p class="b1title">Design &amp; Implementation</p>
<p id="p1075" class="b1textfl"/>
</div>
<div class="boxg1" id="enun0205">
<p class="b1title">2.6 The dangling <span class="inlinecode">else</span></p>
<div>
<p id="p1080" class="b1textfl">A simple change in language syntax&#x2014;eliminating the dangling <span class="inlinecode">else</span>&#x2014;not only reduces the chance of programming errors, but also significantly simplifies parsing. For more on the dangling <span class="inlinecode">else</span> problem, see Exercise <a href="#o0530" id="cf0530">2.24</a> and Section 6.4.</p>
</div></div>
<p class="textfl"/>
<p id="p1085" class="text">Modula-2 uses <span class="inlinecode">END</span> to terminate all its structured statements. Ada and Fortran 77 end an <span class="inlinecode">if</span> with <span class="inlinecode">end if</span> (and a <span class="inlinecode">while</span> with <span class="inlinecode">end while</span>, etc.). Algol 68 creates its terminators by spelling the initial keyword backward (<span class="inlinecode">if</span>&#x2026;<span class="inlinecode">fi</span>, <span class="inlinecode">case</span>&#x2026;<span class="inlinecode">esac</span>, <span class="inlinecode">do</span>&#x2026;<span class="inlinecode">od</span>, etc.).</p>
<p id="p1090" class="text">One problem with end markers is that they tend to bunch up. </p>
<div class="boxg1" id="enun0210">
<p class="b1num">Example 2.32 </p>
<p class="b1title">The need for <span class="inlinecode">elsif</span></p>
<div>
<p id="p1095" class="b1textfl">In Pascal one could write</p>
<pre>  if A = B then ...			
  else if A = C then ...
  else if A = D then ...
  else if A = E then ...
  else ...</pre>
<p id="p1125" class="b1text"><span epub:type="pagebreak" id="page_82" aria-label="Page 82" role="doc-pagebreak"/>With end markers this becomes</p>
<pre>  if A = B then ...			
  else if A = C then ...
  else if A = D then ...
  else if A = E then ...
  else ...
  end end end end</pre>
<p id="p1160" class="b1text">To avoid this awkwardness, languages with end markers generally provide an <span class="inlinecode">elsif</span> keyword (sometimes spelled <span class="inlinecode">elif</span>):</p>
<pre>  if A = B then ...			
  elsif A = C then ...
  elsif A = D then ...
  elsif A = E then ...
  else ...
  end</pre>
<p class="textfl"> &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
</section>
<section>
<h3 id="s0100" class="h2hd"><a id="st0300"/>2.3.3 Table-Driven Top-Down Parsing</h3>
<p id="p1165" class="textfl"/>
<div class="boxg1" id="enun0215">
<p class="b1num">Example 2.33 </p>
<p class="b1title">Driver and table for top-down parsing</p>
<div>
<p id="p1170" class="b1textfl">In a recursive descent parser, each arm of a <span class="sans-serif">case</span> statement corresponds to a production, and contains parsing routine and <span class="sans-serif">match</span> calls corresponding to the symbols on the right-hand side of that production. At any given point in the parse, if we consider the calls beyond the program counter (the ones that have yet to occur) in the parsing routine invocations currently in the call stack, we obtain a list of the symbols that the parser expects to see between here and the end of the program. A table-driven top-down parser maintains an explicit stack containing this same list of symbols.</p>
<p id="p1175" class="b1text">Pseudocode for such a parser appears in Figure <a href="#f0230" id="cf0535">2.19</a>. The code is language independent. It requires a language-<em>dependent</em> parsing table, generally produced by an automatic tool. For the calculator grammar of Figure <a href="#f0165" id="cf0540">2.16</a>, the table appears in Figure <a href="#f0235" id="cf0545">2.20</a>. &#x2003;&#x25A0;</p>
<div class="pageavoid"><figure class="fig" id="f0230"><img src="images/B9780323999663000118/gr020.png" alt="Figure 2.19"/><figcaption class="figleg"><span class="fignum">Figure 2.19</span> <b>Driver for a table-driven LL(1) parser</b>.</figcaption></figure>
</div>
<div class="pageavoid"><figure class="fig" id="f0235"><img src="images/B9780323999663000118/gr021.png" alt="Figure 2.20"/><figcaption class="figleg"><span class="fignum">Figure 2.20</span> <b>LL(1) parse table for the calculator language.</b> Table entries indicate the production to predict (as numbered in Figure <a href="#f0250" id="cf0060">2.23</a>). A dash indicates an error. When the top-of-stack symbol is a terminal, the appropriate action is always to match it against an incoming token from the scanner. An auxiliary table, not shown here, gives the right-hand-side symbols for each production.</figcaption></figure>
</div></div>
</div>
<p class="textfl"/>
<p id="p1180" class="text"/>
<div class="boxg1" id="enun0220">
<p class="b1num">Example 2.34 </p>
<p class="b1title">Table-driven parse of the &#x201C;sum and average&#x201D; program</p>
<div>
<p id="p1185" class="b1textfl">To illustrate the algorithm, Figure <a href="#f0240" id="cf0550">2.21</a> shows a trace of the stack and the input over time, for the sum-and-average program of Example <a href="#enun0160" id="cf0555">2.24</a>. The parser iterates around a loop in which it pops the top symbol off the stack and performs the following actions: If the popped symbol is a terminal, the parser attempts to match it against an incoming token from the scanner. If the match fails, the parser announces a syntax error and initiates some sort of error recovery (see Section <a href="#s0135" id="cf0560">2.3.5</a>). If the popped symbol is a nonterminal, the parser uses that nonterminal together with the next available input token to index into a two-dimensional table that tells it which production to predict (or whether to announce a syntax error and initiate recovery).</p>
<div class="pageavoid"><figure class="fig" id="f0240"><img src="images/B9780323999663000118/gr022.png" alt="Figure 2.21"/><figcaption class="figleg"><span class="fignum">Figure 2.21</span> <b>Trace of a table-driven LL(1) parse of the sum-and-average program of Example</b> <a href="#enun0160" id="cf0065">2.24</a><b>.</b></figcaption></figure>
</div>
<p id="p1190" class="b1text">Initially, the parse stack contains the start symbol of the grammar (in our case, <em>program</em>). When it predicts a production, the parser pushes the right-hand-side symbols onto the parse stack in reverse <span epub:type="pagebreak" id="page_83" aria-label="Page 83" role="doc-pagebreak"/><span epub:type="pagebreak" id="page_84" aria-label="Page 84" role="doc-pagebreak"/>order, so the first of those symbols ends up at top-of-stack. The parse completes successfully when we match the end marker token, <span class="inlinecode">$$</span>. Assuming that <span class="inlinecode">$$</span> appears only once in the grammar, at the end of the first production, and that the scanner returns this token only at end-of-file, any syntax error is guaranteed to manifest itself either as a failed <span class="sans-serif">match</span> or as an error entry in the table. &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p1195" class="text">As we hinted at the end of Section <a href="#s0090" id="cf0565">2.3.1</a>, predict sets are defined in terms of simpler sets called FIRST and FOLLOW, where FIRST(<em>A</em>) is the set of all tokens that could be the start of an <em>A</em> and FOLLOW(<em>A</em>) is the set of all tokens that could come after an <em>A</em> in some valid program. If we extend the domain of FIRST in the obvious way to include <em>strings</em> of symbols, we then say that the predict set of a production <em>A</em> &#x27F6;<em>&#x2009;&#x3B2;</em> is FIRST(<em>&#x3B2;</em>), plus FOLLOW(<em>A</em>) if <em>&#x3B2;</em> &#x27F9;*&#x2009;<em>&#x3B5;</em>. For notational convenience, we define the predicate EPS such that EPS(<em>&#x3B2;</em>) &#x2261; <em>&#x3B2;</em> &#x27F9;*&#x2009;<em>&#x3B5;</em>.</p>
<p id="p1200" class="text"><span epub:type="pagebreak" id="page_85" aria-label="Page 85" role="doc-pagebreak"/></p>
<p id="p1205" class="text"/>
<div class="boxg1" id="enun0225">
<p class="b1num">Example 2.35 </p>
<p class="b1title">Predict sets for the calculator language</p>
<div>
<p id="p1210" class="b1textfl">We can illustrate the algorithm to construct these sets using our calculator grammar (Figure <a href="#f0165" id="cf0570">2.16</a>). We begin with &#x201C;obvious&#x201D; facts about the grammar and build on them inductively. If we recast the grammar in plain BNF (no EBNF &#x2018; | &#x2019; constructs), then it has 19 productions. The &#x201C;obvious&#x201D; facts arise from adjacent pairs of symbols in right-hand sides. In the first production, we can see that <span class="inlinecode">$$</span> &#x2208; FOLLOW(<em>stmt_list</em>). In the third (<em>stmt_list</em> &#x27F6;&#x2009;<em>&#x3B5;</em>), EPS(<em>stmt_list</em>) = <span class="sans-serif">true</span>. In the fourth production (<em>stmt</em> &#x27F6;<em>&#x2009;<span class="inlinecode">id :=</span> expr</em>), <span class="inlinecode">id</span> &#x2208; FIRST(<em>stmt</em>) (also <span class="inlinecode">:=</span> &#x2208; FOLLOW(<span class="inlinecode">id</span>), but it turns out we don't need FOLLOW sets for terminals). In the fifth and sixth productions (<em>stmt</em> &#x27F6;<em>&#x2009;<span class="inlinecode">read id</span> | <span class="inlinecode">write</span> expr</em>), {<span class="inlinecode">read</span>, <span class="inlinecode">write</span>} &#x2282; FIRST(<em>stmt</em>). The complete set of &#x201C;obvious&#x201D; facts appears in Figure <a href="#f0245" id="cf0575">2.22</a>.<span epub:type="pagebreak" id="page_86" aria-label="Page 86" role="doc-pagebreak"/></p>
<div class="pageavoid"><figure class="fig" id="f0245"><img src="images/B9780323999663000118/gr023.png" alt="Figure 2.22"/><figcaption class="figleg"><span class="fignum"><a href="#cf0575">Figure 2.22</a></span> <b>&#x201C;Obvious&#x201D; facts (right) about the LL(1) calculator grammar (left)</b>.</figcaption></figure>
</div>
<p id="p1215" class="b1text">From the &#x201C;obvious&#x201D; facts we can deduce a larger set of facts during a second pass over the grammar. For example, in the second production (<em>stmt_list</em> &#x27F6;<em>&#x2009;stmt stmt_list</em>) we can deduce that {<span class="inlinecode">id</span>, <span class="inlinecode">read</span>, <span class="inlinecode">write</span>} &#x2282; FIRST(<em>stmt_list</em>), because we already know that {<span class="inlinecode">id</span>, <span class="inlinecode">read</span>, <span class="inlinecode">write</span>} &#x2282; FIRST(<em>stmt</em>), and a <em>stmt_list</em> can begin with a <em>stmt</em>. Similarly, in the first production, we can deduce that <span class="inlinecode">$$</span> &#x2208; FIRST(<em>program</em>), because we already know that EPS(<em>stmt_list</em>) = <span class="sans-serif">true</span>.</p>
<p id="p1220" class="b1text">In the eleventh production (<em>factor_tail</em> &#x27F6;<em>&#x2009;mult_op factor factor_tail</em>), we can deduce that {<span class="inlinecode">(</span>, <span class="inlinecode">id</span>, <span class="inlinecode">number</span>} &#x2282; FOLLOW(<em>mult_op</em>), because we already know that {<span class="inlinecode">(</span>, <span class="inlinecode">id</span>, <span class="inlinecode">number</span>} &#x2282; FIRST(<em>factor</em>), and <em>factor</em> follows <em>mult_op</em> in the right-hand side. In the production <em>expr</em> &#x27F6;<em>&#x2009;term term_tail</em>, we can deduce that <span class="inlinecode">)</span> &#x2208; FOLLOW(<em>term_tail</em>), because we already know that <span class="inlinecode">)</span> &#x2208; FOLLOW(<em>expr</em>), and a <em>term_tail</em> can be the last part of an <em>expr</em>. In this same production, we can also deduce that <span class="inlinecode">)</span> &#x2208; FOLLOW(<em>term</em>), because the <em>term_tail</em> can generate <em>&#x3B5;</em> (EPS(<em>term_tail</em>) = <span class="sans-serif">true</span>), allowing a <em>term</em> to be the last part of an <em>expr</em>.</p>
<p id="p1225" class="b1text">There is more that we can learn from our second pass through the grammar, but the examples above cover all the different kinds of cases. To complete our calculation, we continue with additional passes over the grammar until we don't learn any more (i.e., we don't add anything to any of the FIRST and FOLLOW sets). We then construct the PREDICT sets. Final versions of all three sets appear in Figure <a href="#f0250" id="cf0580">2.23</a>. The parse table of Figure <a href="#f0235" id="cf0585">2.20</a> follows directly from PREDICT. &#x2003;&#x25A0;</p>
<div class="pageavoid"><figure class="fig" id="f0250"><img src="images/B9780323999663000118/gr024.png" alt="Figure 2.23"/><figcaption class="figleg"><span class="fignum">Figure 2.23</span> <b>FIRST, FOLLOW, and PREDICT sets for the calculator language.</b> FIRST(<span class="inlinecode">c</span>) = {<span class="inlinecode">c</span>} for all tokens <span class="inlinecode">c</span>. EPS(<em>A</em>) is <span class="sans-serif">true</span> if and only if <em>A</em> &#x2208; {<em>stmt_list</em>, <em>term_tail</em>, <em>factor_tail</em>}.</figcaption></figure>
</div></div>
</div>
<p class="textfl"/>
<p id="p1230" class="text"><span epub:type="pagebreak" id="page_87" aria-label="Page 87" role="doc-pagebreak"/>The algorithm to compute EPS, FIRST, FOLLOW, and PREDICT sets appears, a bit more formally, in Figure <a href="#f0255" id="cf0590">2.24</a>. It relies on the following definitions:</p>
<div><ul>
<li id="u0045" class="unnumlist">EPS(<em>&#x3B1;</em>) &#x2261; if <em>&#x3B1;</em> &#x27F9;*&#x2009;<em>&#x3B5;</em>&#x2009; then <span class="inlinecode">true</span> else <span class="inlinecode">false</span></li>
<li id="u0050" class="unnumlist">FIRST(<em>&#x3B1;</em>) &#x2261; {<span class="inlinecode">c</span> : <em>&#x3B1;</em> &#x27F9;*&#x2009;<span class="inlinecode">c</span> &#x3B2;}</li>
<li id="u0055" class="unnumlist">FOLLOW(<em>A</em>) &#x2261; {<span class="inlinecode">c</span> : S &#x27F9; +&#x2009;<em>&#x3B1; A</em> <span class="inlinecode">c</span> &#x3B2;}</li>
<li id="u0060" class="unnumlist">PREDICT(<em>A</em> &#x27F6;&#x2009;<em>&#x3B1;</em>) &#x2261; FIRST (&#x3B1;) &#x222A; (if EPS(<em>&#x3B1;</em>) then FOLLOW(<em>A</em>) else &#x2205;) <span epub:type="pagebreak" id="page_88" aria-label="Page 88" role="doc-pagebreak"/></li></ul>
</div>
<p class="textfl"/>
<div class="pageavoid"><figure class="fig" id="f0255"><img src="images/B9780323999663000118/gr025.jpg" alt="Figure 2.24"/><figcaption class="figleg"><span class="fignum"><a href="#cf0590">Figure 2.24</a></span> <b>Algorithm to calculate FIRST, FOLLOW, and PREDICT sets.</b> The grammar is LL(1) if and only if all PREDICT sets for productions with the same left-hand side are disjoint.</figcaption></figure>
</div>
<p id="p1255" class="text">The definition of PREDICT assumes that the language has been augmented with an end marker&#x2014; that is, that FOLLOW(<em>S</em>) = {<span class="inlinecode">$$</span>}. Note that FIRST sets and EPS values for strings of length greater than one are calculated on demand; they are not stored explicitly. The algorithm is guaranteed to terminate (i.e., converge on a solution), because the sizes of the FIRST and FOLLOW sets are bounded by the number of terminals in the grammar.</p>
<p id="p1260" class="text">If in the process of calculating PREDICT sets we find that some token belongs to the PREDICT set of more than one production with the same left-hand side, then the grammar is not LL(1), because we will not be able to choose which of the productions to employ when the left-hand side is at the top of the parse stack (or we are in the left-hand side's subroutine in a recursive descent parser) and we see the token coming up in the input. This sort of ambiguity is known as a <em>predict-predict conflict</em>; it can arise either because the same token can begin more than one right-hand side, or because it can begin one right-hand side and can also appear after the left-hand side in some valid program, and one possible right-hand side can generate <em>&#x3B5;</em>.</p>
<p id="p1265" class="text"/>
<div class="boxg1" id="b0040">
<p class="b1title">Design &amp; Implementation</p>
<p id="p1270" class="b1textfl"/>
</div>
<div class="boxg1" id="enun0230">
<p class="b1title">2.7 Recursive descent and table-driven LL parsing</p>
<div>
<p id="p1275" class="b1textfl">When trying to understand the connection between recursive descent and table-driven LL parsing, it is tempting to imagine that the explicit stack of the table-driven parser mirrors the implicit call stack of the recursive descent parser, but this is not the case.</p>
<p id="p1280" class="b1text">A better way to visualize the two implementations of top-down parsing is to remember that both are discovering a parse tree via depth-first left-to-right traversal. When we are at a given point in the parse&#x2014;say the circled node in the tree shown here&#x2014;the implicit call stack of a recursive descent parser holds a frame for each of the nodes on the path back to the root, created when the routine corresponding to that node was called. (This path is shown in gray.)</p>
<div class="pageavoid" id="p1285"><figure class="fig">
<img alt="Image 28" src="images/B9780323999663000118/fx028.png"/></figure></div>
<p id="p1290" class="b1text">But these nodes are immaterial. What matters for the rest of the parse&#x2014;as shown on the white path here&#x2014;are the <em>upcoming calls</em> on the <span class="sans-serif">case</span> statement arms of the recursive descent routines. Those calls&#x2014;those parse tree nodes&#x2014;are precisely the contents of the explicit stack of a table-driven LL parser.</p>
</div></div>
<p class="textfl"/>
<p id="p1295" class="text"/>
<div class="boxg1" id="enun0235">
<p class="b1num"><img alt="Image 12" src="images/B9780323999663000118/fx012.png"/> Check Your Understanding </p>
<div>
<p id="p1300" class="b1textfl"/>
<div><ol>
<li id="o0175" class="b1numlista"><b>29.</b> &#xA0;Describe two common idioms in context-free grammars that cannot be parsed top-down.</li>
<li id="o0180" class="b1numlista"><b>30.</b> &#xA0;What is the &#x201C;dangling <span class="inlinecode">else</span>&#x201D; problem? How is it avoided in modern languages?</li>
<li id="o0185" class="b1numlista"><b>31.</b> &#xA0;<span epub:type="pagebreak" id="page_89" aria-label="Page 89" role="doc-pagebreak"/> Discuss the similarities and differences between recursive descent and table-driven top-down parsing.</li>
<li id="o0190" class="b1numlista"><b>32.</b> &#xA0;What are FIRST and FOLLOW sets? What are they used for?</li>
<li id="o0195" class="b1numlista"><b>33.</b> &#xA0;Under what circumstances does a top-down parser predict the production <em>A</em> &#x27F6;<em>&#x2009;&#x3B1;</em>?</li>
<li id="o0200" class="b1numlista"><b>34.</b> &#xA0;What sorts of &#x201C;obvious&#x201D; facts form the basis of FIRST set and FOLLOW set construction?</li>
<li id="o0205" class="b1numlista"><b>35.</b> &#xA0;Outline the algorithm used to complete the construction of FIRST and FOLLOW sets. How do we know when we are done?</li>
<li id="o0210" class="b1numlista"><b>36.</b> &#xA0;How do we know when a grammar is not LL(1)?</li></ol>
</div>
<p class="b1textfl"/>
</div></div>
<p class="textfl"/>
</section>
<section>
<h3 id="s0105" class="h2hd"><a id="st0325"/>2.3.4 Bottom-Up Parsing</h3>
<p id="p1345" class="textfl">Conceptually, as we saw at the beginning of Section <a href="#s0085" id="cf0595">2.3</a>, a bottom-up parser works by maintaining a forest of partially completed subtrees of the parse tree, which it joins together whenever it recognizes the symbols on the right-hand side of some production used in the right-most derivation of the input string. It creates a new internal node and makes the roots of the joined-together trees the children of that node.</p>
<p id="p1350" class="text">In practice, a bottom-up parser is almost always table driven. It keeps the roots of its partially completed subtrees on a stack. When it accepts a new token from the scanner, it <em>shifts</em> the token into the stack. When it recognizes that the top few symbols on the stack constitute a right-hand side, it <em>reduces</em> those symbols to their left-hand side by popping them off the stack and pushing the left-hand side in their place. The role of the stack is the first important difference between top-down and bottom-up parsing: a top-down parser's stack contains a list of what the parser expects to see in the future; a bottom-up parser's stack contains a record of what the parser has already seen in the past.</p>
<section>
<h4 id="s0110" class="h3hd"><a id="st0330"/>Canonical Derivations</h4>
<p id="p1355" class="textfl">We also noted earlier that the actions of a bottom-up parser trace out a right-most (canonical) derivation in reverse. The roots of the partial subtrees, left-to-right, together with the remaining input, constitute a sentential form of the right-most derivation. </p>
<div class="boxg1" id="enun0240">
<p class="b1num">Example 2.36 </p>
<p class="b1title">Derivation of an <span class="inlinecode">id</span> list</p>
<div>
<p id="p1360" class="b1textfl">On the right-hand side of Figure <a href="#f0150" id="cf0600">2.14</a>, for example, we have the following series of steps:</p>
<p id="p1365" class="b1text"><span epub:type="pagebreak" id="page_90" aria-label="Page 90" role="doc-pagebreak"/></p>
<div class="pageavoid"><table class="tbody1"><tbody><tr><td class="tb0a">Stack contents (roots of partial trees)</td><td class="tb0a">Remaining input</td></tr><tr><td class="tb0a"><em>&#x3B5;</em></td><td class="tb0a"><span class="inlinecode">A, B, C;</span></td></tr><tr><td class="tb0a"><span class="inlinecode">id (A)</span></td><td class="tb0a"><span class="inlinecode">, B, C;</span></td></tr><tr><td class="tb0a"><span class="inlinecode">id (A) ,</span></td><td class="tb0a"><span class="inlinecode">B, C;</span></td></tr><tr><td class="tb0a"><span class="inlinecode">id (A) , id (B)</span></td><td class="tb0a"><span class="inlinecode">, C;</span></td></tr><tr><td class="tb0a"><span class="inlinecode">id (A) , id (B) ,</span></td><td class="tb0a"><span class="inlinecode">C;</span></td></tr><tr><td class="tb0a"><span class="inlinecode">id (A) , id (B) , id (C)</span></td><td class="tb0a"><span class="inlinecode">;</span></td></tr><tr><td class="tb0a"><span class="inlinecode">id (A) , id (B) , id (C) <span class="underline">;</span></span></td><td class="tb0a"/></tr><tr><td class="tb0a"><span class="inlinecode">id (A) , id (B)</span> <span class="underline"><span class="inlinecode">, id (C)</span><em>id_list_tail</em></span></td><td class="tb0a"/></tr><tr><td class="tb0a"><span class="inlinecode">id (A)</span> <span class="underline"><span class="inlinecode">, id (B)</span><em>id_list_tail</em></span></td><td class="tb0a"/></tr><tr><td class="tb0a"><span class="underline"><span class="inlinecode">id (A)</span><em>id_list_tail</em></span></td><td class="tb0a"/></tr><tr><td class="tb0a"><em>id_list</em></td><td class="tb0a"/></tr></tbody></table>
</div>
<p class="b1textfl"/>
<p id="p1370" class="b1text">The last four lines (the ones that don't just shift tokens into the forest) correspond to the right-most derivation:</p>
<pre>
  <em>id_list</em> &#x27F9; id <em>id_list_tail</em>
	&#x27F9; id , id <em>id_list_tail</em>
        &#x27F9; id , id , id <em>id_list_tail</em>
        &#x27F9; id , id , id ;</pre>
<p class="textfl"> The symbols that need to be joined together at each step of the parse to represent the next step of the backward derivation are called the <em>handle</em> of the sentential form. In the parse trace above, the handles are underlined. &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p1375" class="text"/>
<div class="boxg1" id="enun0245">
<p class="b1num">Example 2.37 </p>
<p class="b1title">Bottom-up grammar for the calculator language</p>
<div>
<p id="p1380" class="b1textfl">In our <em>id_list</em> example, no handles were found until the entire input had been shifted onto the stack. In general this will not be the case. We can obtain a more realistic example by examining an LR version of our calculator language, shown in Figure <a href="#f0265" id="cf0605">2.25</a>. While the LL grammar of Figure <a href="#f0165" id="cf0610">2.16</a> can be parsed bottom-up, the version in Figure <a href="#f0265" id="cf0615">2.25</a> is preferable for two reasons. First, it uses a left-recursive production for <em>stmt_list</em>. Left recursion allows the parser to collapse long statement lists as it goes along, rather than waiting until the entire list is on the stack and then collapsing it from the end. Second, it uses left-recursive productions for <em>expr</em> and <em>term</em>. These productions capture left associativity while still keeping an operator and its operands together in the same right-hand side, something we were unable to do in a top-down grammar. &#x2003;&#x25A0;</p>
<div class="pageavoid"><figure class="fig" id="f0265"><img src="images/B9780323999663000118/gr026.png" alt="Figure 2.25"/><figcaption class="figleg"><span class="fignum">Figure 2.25</span> <b>LR(1) grammar for the calculator language.</b> Productions have been numbered for reference in future figures.</figcaption></figure>
</div></div>
</div>
<p class="textfl"/>
</section>
<section>
<h4 id="s0115" class="h3hd"><a id="st0345"/>Modeling a Parse with LR Items</h4>
<p id="p1385" class="textfl"/>
<div class="boxg1" id="enun0250">
<p class="b1num">Example 2.38 </p>
<p class="b1title">Bottom-up parse of the &#x201C;sum and average&#x201D; program</p>
<div>
<p id="p1390" class="b1textfl">Suppose we are to parse the sum-and-average program from Example <a href="#enun0160" id="cf0620">2.24</a>:</p>
<pre>  read A			
  read B
  sum := A + B
  write sum
  write sum / 2</pre>
<p id="p1420" class="b1text">The key to success will be to figure out when we have reached the end of a right-hand side&#x2014;that is, when we have a handle at the top of the <span epub:type="pagebreak" id="page_91" aria-label="Page 91" role="doc-pagebreak"/>parse stack. The trick is to keep track of the set of productions we might be &#x201C;in the middle of&#x201D; at any particular time, together with an indication of where in those productions we might be.</p>
<p id="p1425" class="b1text">When we begin execution, the parse stack is empty and we are at the beginning of the production for <em>program</em>. (In general, we can assume that there is only one production with the start symbol on the left-hand side; it is easy to modify any grammar to make this the case.) We can represent our location&#x2014;more specifically, the location represented by the top of the parse stack&#x2014;with a in the right-hand side of the production:</p>
<pre>  <em>program</em> &#x27F6; &#x2022; <em>stmt_list</em> $$</pre>
<p class="textfl"> When augmented with a a production is called an LR <em>item</em>. Since the in this item is immediately in front of a nonterminal&#x2014;namely <em>stmt_list</em>&#x2014;we may be about to see the yield of that nonterminal coming up on the input. This possibility implies that we may be at the beginning of some production with <em>stmt_list</em> on the left-hand side:</p>
<pre>  <em>program</em> &#x27F6; &#x2022; <em>stmt_list</em> $$
  <em>stmt_list</em> &#x27F6; &#x2022; <em>stmt_list stmt</em>
  <em>stmt_list</em> &#x27F6; &#x2022; <em>stmt_list</em></pre>
<p class="textfl"> And, since <em>stmt</em> is a nonterminal, we may also be at the beginning of any production whose left-hand side is <em>stmt</em>: <span epub:type="pagebreak" id="page_92" aria-label="Page 92" role="doc-pagebreak"/></p>
<pre>  <em>program</em> &#x27F6; &#x2022; <em>stmt_list</em> $$				(State 0)
  <em>stmt_list</em> &#x27F6; &#x2022; <em>stmt_list stmt</em>
  <em>stmt_list</em> &#x27F6; &#x2022; <em>stmt</em>
  <em>stmt</em> &#x27F6; &#x2022; id := <em>expr</em>
  <em>stmt</em> &#x27F6; &#x2022; read id
  <em>stmt</em> &#x27F6; &#x2022; write <em>expr</em></pre>
<p class="textfl"> Since all of these last productions begin with a terminal, no additional items need to be added to our list. The original item (<em>program</em> &#x27F6;<em>&#x2009;stmt_list <span class="inlinecode">$$</span></em>) is called the <em>basis</em> of the list. The additional items are its <em>closure</em>. The list represents the initial state of the parser. As we shift and reduce, the set of items will change, always indicating which productions <em>may</em> be the right one to use next in the derivation of the input string. If we reach a state in which some item has the at the end of the right-hand side, we can reduce by that production. Otherwise, as in the current situation, we must shift. Note that if we need to shift, but the incoming token cannot follow the in any item of the current state, then a syntax error has occurred. We will consider error recovery in more detail in Section C-2.3.5.</p>
<p id="p1430" class="b1text">Our upcoming token is a <span class="inlinecode">read</span>. Once we shift it onto the stack, we know we are in the following state:</p>
<pre>  <em>stmt</em> &#x27F6; read &#x2022; id					(State 1)</pre>
<p class="textfl"> This state has a single basis item and an empty closure&#x2014;the precedes a terminal. After shifting the <span class="inlinecode">A</span>, we have</p>
<pre>  <em>stmt</em> &#x27F6; read id &#x2022;					(State 1&#x2032;)</pre>
<p class="textfl"> We now know that <span class="inlinecode">read id</span> is the handle, and we must reduce. The reduction pops two symbols off the parse stack and pushes a <em>stmt</em> in their place, but what should the new state be? We can see the answer if we imagine moving back in time to the point at which we shifted the <span class="inlinecode">read</span>&#x2014;the first symbol of the right-hand side. At that time we were in the state labeled &#x201C;State 0&#x201D; above, and the upcoming tokens on the input (though we didn't look at them at the time) were <span class="inlinecode">read id</span>. We have now consumed these tokens, and we know that they constituted a <em>stmt</em>. By pushing a <em>stmt</em> onto the stack, we have in essence replaced <span class="inlinecode">read id</span> with <em>stmt</em> on the input stream, and have then &#x201C;shifted&#x201D; the nonterminal, rather than its yield, into the stack. Since one of the items in State 0 was</p>
<pre>  <em>stmt_list</em> &#x27F6; &#x2022; <em>stmt</em></pre>
<p class="textfl"> we now have</p>
<pre>  <em>stmt_list</em> &#x27F6; <em>stmt</em> &#x2022;					(State 0&#x2032;)</pre>
<p class="textfl"> Again we must reduce. We remove the <em>stmt</em> from the stack and push a <em>stmt_list</em> in its place. Again we can see this as &#x201C;shifting&#x201D; a <em>stmt_list</em> when in State 0. Since two of the items in State 0 have a <span epub:type="pagebreak" id="page_93" aria-label="Page 93" role="doc-pagebreak"/><em>stmt_list</em> after the we don't know (without looking ahead) which of the productions will be the next to be used in the derivation, but we don't have to know. The key advantage of bottom-up parsing over top-down parsing is that we don't need to predict ahead of time which production we shall be expanding.</p>
<p id="p1435" class="b1text">Our new state is as follows:</p>
<pre>  <em>program</em> &#x27F6; <em>stmt_list</em> &#x2022; $$		    		 (State 2)
  <em>stmt_list</em> &#x27F6; <em>stmt_list</em> &#x2022; <em>stmt</em>
  <em>stmt</em> &#x27F6; &#x2022; id := <em>expr</em>
  <em>stmt</em> &#x27F6; &#x2022; read id
  <em>stmt</em> &#x27F6; &#x2022; write <em>expr</em></pre>
<p class="textfl"> The first two productions are the basis; the others are the closure. Since no item has a at the end, we shift the next token, which happens again to be a <span class="inlinecode">read</span>, taking us back to State 1. Shifting the <span class="inlinecode">B</span> takes us to State 1&#x2032; again, at which point we reduce. This time, however, we go back to State 2 rather than State 0 before shifting the left-hand-side <em>stmt</em>. Why? Because we were in State 2 when we began to read the right-hand side. &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
</section>
<section>
<h4 id="s0120" class="h3hd"><a id="st0355"/>The Characteristic Finite-State Machine and LR Parsing Variants</h4>
<p id="p1440" class="textfl">An LR-family parser keeps track of the states it has traversed by pushing them into the parse stack, along with the grammar symbols. It is in fact the states (rather than the symbols) that drive the parsing algorithm: they tell us what state we were in at the beginning of a right-hand side. Specifically, when the combination of state and input tells us we need to reduce using production <em>A</em> &#x27F6;<em>&#x2009;&#x3B1;</em>, we pop <em>length</em>(<em>&#x3B1;</em>) symbols off the stack, together with the record of states we moved through while shifting those symbols. These pops expose the state we were in immediately prior to the shifts, allowing us to return to that state and proceed as if we had seen <em>A</em> in the first place.</p>
<p id="p1445" class="text">We can think of the shift rules of an LR-family parser as the transition function of a finite automaton, much like the automata we used to model scanners. Each state of the automaton corresponds to a list of items that indicate where the parser might be at some specific point in the parse. The transition for input symbol <em>X</em> (which may be either a terminal or a nonterminal) moves to a state whose basis consists of items in which the has been moved across an <em>X</em> in the right-hand side, plus whatever items need to be added as closure. The lists are constructed by a bottom-up parser generator in order to build the automaton, but are not needed during parsing.</p>
<p id="p1450" class="text">Simpler members of the LR family of parsers&#x2014;SLR(1) and LALR(1) in particular&#x2014;use an automaton known as the <em>characteristic finite-state machine</em>, or CFSM. Full LR parsers use a machine with (for most grammars) a much larger number of states. The differences among the algorithms lie in how they deal with states that contain a <em>shift-reduce conflict</em>&#x2014; one item with the in front of a terminal (suggesting the need for a shift) and another with the at the end of the right-hand side (suggesting the need for a reduction).</p>
<p id="p1455" class="text"><span epub:type="pagebreak" id="page_94" aria-label="Page 94" role="doc-pagebreak"/>SLR (simple LR) parsers peek at upcoming input and use FOLLOW sets to resolve conflicts. An SLR parser will call for a reduction via <em>A</em> &#x27F6;<em>&#x2009;&#x3B1;</em> only if the upcoming token(s) are in FOLLOW(<em>&#x3B1;</em>). It will still see a conflict, however, if the tokens are also in the FIRST set of any of the symbols that follow a in other items of the state. As it turns out, there are important cases in which a token may follow a given nonterminal somewhere in a valid program, but never in a context described by the current state. For these cases global FOLLOW sets are too crude. LALR (look-ahead LR) parsers improve on SLR by using <em>local</em> (state-specific) look-ahead instead.</p>
<p id="p1460" class="text">Conflicts can still arise in an LALR parser when the same set of items can occur on two different paths through the CFSM. Both paths will end up in the same state, at which point state-specific look-ahead can no longer distinguish between them. A full LR parser duplicates states in order to keep paths disjoint when their local look-aheads are different.</p>
<p id="p1465" class="text">LALR parsers are the most common bottom-up parsers in practice. They are the same size and speed as SLR parsers, but are able to resolve more conflicts. Full LR parsers for real programming languages tend to be very large. Several researchers have developed techniques to reduce the size of full-LR tables, but LALR works sufficiently well in practice that the extra complexity of full LR is usually not required. For simplicity, we focus on SLR parsers in the remainder of this section. <span class="inlinecode">Yacc</span>/<span class="inlinecode">bison</span> produces C code for an LALR parser.</p>
</section>
<section>
<h4 id="s0125" class="h3hd"><a id="st0360"/>Bottom-Up Parsing Tables</h4>
<p id="p1470" class="textfl">Like a table-driven LL(1) parser, an SLR(1), LALR(1), or LR(1) parser executes a loop in which it repeatedly inspects a two-dimensional table to find out what action to take. However, instead of using the current input token and top-of-stack nonterminal to index into the table, an LR-family parser uses the current input token and the current parser state (which can be found at the top of the stack). &#x201C;Shift&#x201D; table entries indicate the state that should be pushed. &#x201C;Reduce&#x201D; table entries indicate the number of states that should be popped and the nonterminal that should be pushed back onto the input stream, to be shifted by the state uncovered by the pops. There is always one popped state for every symbol on the right-hand side of the reducing production. The state to be pushed next can be found by indexing into the table using the uncovered state and the newly recognized nonterminal.</p>
<p id="p1475" class="text"/>
<div class="boxg1" id="enun0255">
<p class="b1num">Example 2.39 </p>
<p class="b1title">CFSM for the bottom-up calculator grammar</p>
<div>
<p id="p1480" class="b1textfl">The CFSM for our bottom-up version of the calculator grammar appears in Figure <a href="#f0310" id="cf0625">2.26</a>. States 6, 7, 9, and 13 contain potential shift-reduce conflicts, but all of these can be resolved with global FOLLOW sets. SLR parsing therefore suffices. In State 6, for example, FIRST(<em>add_op</em>) &#x2229; FOLLOW(<em>stmt</em>) = &#x2205;. In addition to shift and reduce rules, we allow the parse table as an optimization to contain rules of the form &#x201C;shift and then reduce.&#x201D; This optimization serves to eliminate trivial states such as 1&#x2032; and 0&#x2032; in Example <a href="#enun0250" id="cf0630">2.38</a>, which had only a single item, with the at the end.</p>
<div class="pageavoid"><figure class="fig" id="f0310"><img src="images/B9780323999663000118/gr027.png" alt="Figure 2.26"/></figure>
<figure class="fig" id="f0315"><img src="images/B9780323999663000118/gr028.png" alt="Figure 2.26"/>
<figcaption class="figleg"><span class="fignum">Figure 2.26</span> <b>CFSM for the calculator grammar (Figure</b> <a href="#f0265" id="cf0070">2.25</a><b>).</b> Basis and closure items in each state are separated by a horizontal rule. Trivial reduce-only states have been eliminated by use of &#x201C;shift and reduce&#x201D; transitions. <em>(continued)</em></figcaption></figure>
</div>
<p id="p1485" class="b1text">A pictorial representation of the CFSM appears in Figure <a href="#f0320" id="cf0635">2.27</a>. A tabular representation, suitable for use in a table-driven parser, <span epub:type="pagebreak" id="page_95" aria-label="Page 95" role="doc-pagebreak"/>appears in Figure <a href="#f0325" id="cf0640">2.28</a>. Pseudocode for the (language-independent) parser driver appears in Figure <a href="#f0330" id="cf0645">2.29</a>. A trace of the parser's actions on the sum-and-average program appears in Figure <a href="#f0335" id="cf0650">2.30</a>.&#x2003;&#x25A0;</p>
<div class="pageavoid"><figure class="fig" id="f0320"><img src="images/B9780323999663000118/gr029.png" alt="Figure 2.27"/><figcaption class="figleg"><span class="fignum"><a href="#cf0635">Figure 2.27</a></span> <b>Pictorial representation of the CFSM of Figure</b> <a href="#f0310" id="cf0075">2.26</a><b>.</b> Reduce actions are not shown.</figcaption></figure>
</div>
<div class="pageavoid"><figure class="fig" id="f0325"><img src="images/B9780323999663000118/gr030.png" alt="Figure 2.28"/><figcaption class="figleg"><span class="fignum"><a href="#cf0640">Figure 2.28</a></span> <b>SLR(1) parse table for the calculator language.</b> Table entries indicate whether to shift (s), reduce (r), or shift and then reduce (b). The accompanying number is the new state when shifting, or the production that has been recognized when (shifting and) reducing. Production numbers are given in Figure <a href="#f0265" id="cf0080">2.25</a>. Symbol names have been abbreviated for the sake of formatting. A dash indicates an error. An auxiliary table, not shown here, gives the left-hand-side symbol and right-hand-side length for each production.</figcaption></figure>
</div>
<div class="pageavoid"><figure class="fig" id="f0330"><img src="images/B9780323999663000118/gr031.png" alt="Figure 2.29"/><figcaption class="figleg"><span class="fignum"><a href="#cf0645">Figure 2.29</a></span> <b>Driver for a table-driven SLR(1) parser.</b> We call the scanner directly, rather than using the global <span class="sans-serif">input_token</span> of Figures <a href="#f0170" id="cf0085">2.17</a> and <a href="#f0230" id="cf0090">2.19</a>, so that we can set <span class="sans-serif">cur_sym</span> to be an arbitrary symbol. We pass to the <span class="sans-serif">pop(<span style="display:inline-block; width: 0.05em;"/>)</span> routine a parameter that indicates the number of symbols to remove from the stack.</figcaption></figure>
</div>
<div class="pageavoid"><figure class="fig" id="f0335"><img src="images/B9780323999663000118/gr032.png" alt="Figure 2.30"/><figcaption class="figleg"><span class="fignum">Figure 2.30</span> <b>Trace of a table-driven SLR(1) parse of the sum-and-average program.</b> States in the parse stack are shown in boldface type. Symbols in the parse stack are for clarity only; they are not needed by the parsing algorithm. Parsing begins with the initial state of the CFSM (State 0) in the stack. It ends when we reduce by <em>program</em> &#x27F6;<em>&#x2009;stmt_list <span class="inlinecode">$$</span></em>, uncovering State 0 again and pushing <em>program</em> onto the input stream.</figcaption></figure>
</div></div>
</div>
<p class="textfl"/>
<p id="p1490" class="text"/>
<p class="textfl"/>
</section>
<section>
<h4 id="s0130" class="h3hd"><a id="st0370"/>Handling Epsilon Productions</h4>
<p id="p1495" class="textfl"/>
<div class="boxg1" id="enun0260">
<p class="b1num">Example 2.40 </p>
<p class="b1title">Epsilon productions in the bottom-up calculator grammar</p>
<div>
<p id="p1500" class="b1textfl">The careful reader may have noticed that the grammar of Figure <a href="#f0265" id="cf0655">2.25</a>, in addition to using left-recursive rules for <em>stmt_list</em>, <em>expr</em>, and <em>term</em>, differs from the grammar of Figure <a href="#f0165" id="cf0660">2.16</a> in one other way: it defines a <em>stmt_list</em> to be a sequence of one or more <em>stmt</em>s, rather than zero or more. (This means, of course, that it defines a different language.) To capture the same language as Figure <a href="#f0165" id="cf0665">2.16</a>, production 3 in Figure <a href="#f0265" id="cf0670">2.25</a>,</p>
<p id="p1505" class="b1text"/>
<pre>  <em>stmt_list</em> &#x27F6; <em>stmt</em></pre>
<p class="textfl"> would need to be replaced with</p>
<pre>  <em>stmt_list</em> &#x27F6; &#x03B5;</pre>
<p class="textfl"> &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p1510" class="text">Note that it does in general make sense to have an empty statement list. In the calculator language it simply permits an empty program, which is admittedly silly. In real languages, however, it allows the body of a structured statement to be empty, which can be very useful. One frequently wants one arm of a <span class="inlinecode">case</span> or multiway <span class="inlinecode">if</span>&#x2026;<span style="display:inline-block; width: 0.45em;"/><span class="inlinecode">then</span>&#x2009;&#x2026;<span style="display:inline-block; width: 0.45em;"/><span class="inlinecode">else</span> statement to be empty, and an empty <span class="inlinecode">while</span> loop allows a parallel program (or the operating system) to wait for a signal from another process or an I/O device.</p>
<p id="p1515" class="text"/>
<div class="boxg1" id="enun0265">
<p class="b1num">Example 2.41 </p>
<p class="b1title">CFSM with epsilon productions</p>
<div>
<p id="p1520" class="b1textfl">If we look at the CFSM for the calculator language, we discover that State 0 is the only state that needs to be changed in order to allow empty statement lists. The item</p>
<pre>  <em>stmt_list</em> &#x27F6; &#x2022; <em>stmt</em></pre>
<p class="textfl"> becomes</p>
<pre>  <em>stmt_list</em> &#x27F6; &#x2022; &#x03B5;</pre>
<p class="textfl"> which is equivalent to</p>
<pre>  <em>stmt_list</em> &#x27F6; &#x03B5; &#x2022;</pre>
<p class="textfl"> or simply</p>
<pre>  <em>stmt_list</em> &#x27F6; &#x2022;</pre>
<p class="textfl"> The entire state is then</p>
<pre>  <em>program</em> &#x27F6; &#x2022; <em>stmt_list</em> $$   <span class="serif">on <em>stmt_list</em> shift and goto 2</span>
  ________________
  <em>stmt_list</em> &#x27F6; &#x2022; <em>stmt_list stmt</em>
  <em>stmt_list</em> &#x27F6; &#x2022;			<span class="serif">on</span> $$ <span class="serif">reduce (pop 0 states, push <em>stmt_list</em> on input)</span>
  <em>stmt</em> &#x27F6; &#x2022; id := <em>expr</em>		<span class="serif">on</span> id <span class="serif">shift and goto 3</span>
  <em>stmt</em> &#x27F6; &#x2022; read id		<span class="serif">on</span> read <span class="serif">shift and goto 1</span>
  <em>stmt</em> &#x27F6; &#x2022; write <em>expr</em>		<span class="serif">on</span> write <span class="serif">shift and goto 4</span></pre>
<p class="b1textfl"><span epub:type="pagebreak" id="page_96" aria-label="Page 96" role="doc-pagebreak"/><span epub:type="pagebreak" id="page_97" aria-label="Page 97" role="doc-pagebreak"/><span epub:type="pagebreak" id="page_98" aria-label="Page 98" role="doc-pagebreak"/><span epub:type="pagebreak" id="page_99" aria-label="Page 99" role="doc-pagebreak"/>The look-ahead for item</p>
<pre>  <em>stmt_list</em> &#x27F6; &#x2022;</pre>
<p class="textfl"> is FOLLOW(<em>stmt_list</em>), which is the end-marker, <span class="inlinecode">$$</span>. Since <span class="inlinecode">$$</span> does not appear in the look-aheads for any other item in this state, our grammar is still SLR(1). &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p1525" class="text"><span epub:type="pagebreak" id="page_100" aria-label="Page 100" role="doc-pagebreak"/><span epub:type="pagebreak" id="page_101" aria-label="Page 101" role="doc-pagebreak"/></p>
<div class="boxg1" id="enun0270">
<p class="b1num"><img alt="Image 12" src="images/B9780323999663000118/fx012.png"/> Check Your Understanding </p>
<div>
<p id="p1530" class="b1textfl"/>
<div><ol>
<li id="o0215" class="b1numlista"><b>37.</b> &#xA0;What is the <em>handle</em> of a right sentential form?</li>
<li id="o0220" class="b1numlista"><b>38.</b> &#xA0;Explain the significance of the characteristic finite-state machine in LR parsing.</li>
<li id="o0225" class="b1numlista"><b>39.</b> &#xA0;What is the significance of the dot ( in an LR item?</li>
<li id="o0230" class="b1numlista"><b>40.</b> &#xA0;What distinguishes the <em>basis</em> from the <em>closure</em> of an LR state?</li>
<li id="o0235" class="b1numlista"><b>41.</b> &#xA0;What is a <em>shift-reduce conflict</em>? How is it resolved in the various kinds of LR-family parsers?</li>
<li id="o0240" class="b1numlista"><b>42.</b> &#xA0;Outline the steps performed by the driver of a bottom-up parser.</li>
<li id="o0245" class="b1numlista"><b>43.</b> &#xA0;What kind of parser is produced by <span class="inlinecode">yacc</span>/<span class="inlinecode">bison</span>? By ANTLR?</li></ol>
</div>
<p class="b1textfl"/>
</div></div>
<p class="textfl"/>
</section></section>
<section>
<h3 id="s0135" class="h2hd"><a id="st0385"/>2.3.5 Recovering from Syntax Errors</h3>
<p id="p1570" class="textfl"/>
<div class="boxg1" id="enun0275">
<p class="b1num">Example 2.42 </p>
<p class="b1title">A syntax error in C</p>
<div>
<p id="p1575" class="b1textfl">Suppose we are parsing a C program and see the following code fragment in a context where a statement is expected:</p>
<pre>  A = B : C + D;</pre>
<p id="p1585" class="b1text">We will detect a syntax error immediately after the <span class="inlinecode">B</span>, when the colon appears from the scanner. At this point the simplest thing to do is just to print an error message and halt. This naive approach is generally not acceptable, however: it would mean that every run of the compiler reveals no more than one syntax error. Since most programs, at least at first, contain numerous such errors, we really need to find as many as possible now (we'd also like to continue looking for semantic errors). To do so, we must modify the state of the parser and/or the input stream so that the upcoming token(s) are acceptable. We shall probably want to turn off code generation, disabling the back end of the compiler: since the input is not a valid program, the code will not be of use, and there's no point in spending time creating it. &#x2003;&#x25A0;</p>
</div></div>
<p class="textfl"/>
<p id="p1590" class="text">In general, the term <em>syntax error recovery</em> is applied to any technique that allows the compiler, in the face of a syntax error, to continue looking for other errors later in the program. High-quality syntax error recovery is essential in any production-quality compiler. The better the recovery technique, the more likely the compiler will be to recognize additional errors (especially nearby errors) correctly, and the less likely it will be to become confused and announce spurious <em>cascading errors</em> later in the program.</p>
<p id="p1595" class="text"/>
<div class="boxg1" id="enun0280">
<p class="b1num"><img alt="Image 45" src="images/B9780323999663000118/fx045.png"/> In More Depth </p>
<div>
<p id="p1600" class="b1textfl">On the companion site we explore several possible approaches to syntax error recovery. In <em>panic mode</em>, <span epub:type="pagebreak" id="page_102" aria-label="Page 102" role="doc-pagebreak"/> the compiler writer defines a small set of &#x201C;safe symbols&#x201D; that delimit clean points in the input. Semicolons, which typically end a statement, are a good choice in many languages. When an error occurs, the compiler deletes input tokens until it finds a safe symbol, and then &#x201C;backs the parser out&#x201D; (e.g., returns from recursive descent subroutines) until it finds a context in which that symbol might appear. <em>Phrase-level recovery</em> improves on this technique by employing different sets of &#x201C;safe&#x201D; symbols in different productions of the grammar (right parentheses when in an expression; semicolons when in a declaration). <em>Context-specific look-ahead</em> obtains additional improvements by differentiating among the various contexts in which a given production might appear in a syntax tree. To respond gracefully to certain common programming errors, the compiler writer may augment the grammar with <em>error productions</em> that capture language-specific idioms that are incorrect but are often written by mistake.</p>
<p id="p1605" class="b1text">Niklaus Wirth published an elegant implementation of phrase-level and context-specific recovery for recursive descent parsers in 1976 <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br2170">[Wir76, Sec. 5.9]</span>. <em>Exceptions</em> (to be discussed further in Section 9.4) provide a simpler alternative if supported by the language in which the compiler is written. For table-driven top-down parsers, Fischer, Milton, and Quiring published an algorithm in 1980 that automatically implements a well-defined notion of <em>locally least-cost syntax repair</em>. Locally least-cost repair is also possible in bottom-up parsers, but it is significantly more difficult. Most bottom-up parsers rely on more straightforward phrase-level recovery; a typical example can be found in <span class="inlinecode">yacc</span>/<span class="inlinecode">bison</span>.</p>
</div></div>
<p class="textfl"/>
</section></section>
<section>
<h2 class="h1hd" id="s0140"><a id="st0395"/>2.4 Theoretical Foundations</h2>
<p id="p1610" class="textfl">Our understanding of the relative roles and computational power of scanners, parsers, regular expressions, and context-free grammars is based on the formalisms of <em>automata theory</em>. In automata theory, a <em>formal language</em> is a set of strings of symbols drawn from a finite <em>alphabet</em>. A formal language can be specified either by a set of rules (such as regular expressions or a context-free grammar) that generates the language, or by a <em>formal machine</em> that <em>accepts</em> (<em>recognizes</em>) the language. A formal machine takes strings of symbols as input and outputs either &#x201C;yes&#x201D; or &#x201C;no.&#x201D; A machine is said to accept a language if it says &#x201C;yes&#x201D; to all and only those strings that are in the language. Alternatively, a language can be defined as the set of strings for which a particular machine says &#x201C;yes.&#x201D;</p>
<p id="p1615" class="text">Formal languages can be grouped into a series of successively larger classes known as the <em>Chomsky hierarchy</em>. <sup><a href="#fn014" id="cf0680" epub:type="noteref" role="doc-noteref">13</a></sup> Most of the classes can be characterized in two ways: by the types of rules that can be used to generate the set of strings, or by the type of formal machine that is capable of recognizing the language. As we have seen, <em>regular languages</em> are defined by using concatenation, alternation,<span epub:type="pagebreak" id="page_103" aria-label="Page 103" role="doc-pagebreak"/> and Kleene closure, and are recognized by a scanner. <em>Context-free languages</em> are a proper superset of the regular languages. They are defined by using concatenation, alternation, and recursion (which subsumes Kleene closure), and are recognized by a parser. A scanner is a concrete realization of a <em>finite automaton</em>, a type of formal machine. A parser is a concrete realization of a <em>push-down automaton</em>. Just as context-free grammars add recursion to regular expressions, push-down automata add a stack to the memory of a finite automaton. There are additional levels in the Chomsky hierarchy, but they are less directly applicable to compiler construction, and are not covered here.</p>
<p id="p1620" class="text">It can be proven, constructively, that regular expressions and finite automata are equivalent: one can construct a finite automaton that accepts the language defined by a given regular expression, and vice versa. Similarly, it is possible to construct a push-down automaton that accepts the language defined by a given context-free grammar, and vice versa. The grammar-to-automaton constructions are in fact performed by scanner and parser generators such as <span class="inlinecode">lex</span> and <span class="inlinecode">yacc</span>. Of course, a real scanner does not accept just one token; it is called in a loop so that it keeps accepting tokens repeatedly. As noted in Sidebar <a href="#enun0105" id="cf0685">2.4</a>, this detail is accommodated by having the scanner accept the alternation of all the tokens in the language (with distinguished final states), and by having it continue to consume characters until no longer token can be constructed.</p>
<p id="p1625" class="text"/>
<div class="boxg1" id="enun0285">
<p class="b1num"><img alt="Image 45" src="images/B9780323999663000118/fx045.png"/> In More Depth </p>
<div>
<p id="p1630" class="b1textfl">On the companion site we consider finite and push-down automata in more detail. We give an algorithm to convert a DFA into an equivalent regular expression. Combined with the constructions in Section <a href="#s0045" id="cf0690">2.2.1</a>, this algorithm demonstrates the equivalence of regular expressions and finite automata. We also consider the sets of grammars and languages that can and cannot be parsed by the various linear-time parsing algorithms.</p>
</div></div>
<p class="textfl"/>
</section>
<section>
<h2 class="h1hd" id="s0145"><a id="st0400"/>2.5 Summary and Concluding Remarks</h2>
<p id="p1635" class="textfl">In this chapter we have introduced the formalisms of regular expressions and context-free grammars, and the algorithms that underlie scanning and parsing in practical compilers. We also mentioned syntax error recovery, and presented a quick overview of relevant parts of automata theory. Regular expressions and context-free grammars are language <em>generators</em>: they specify how to construct valid strings of characters or tokens. Scanners and parsers are language <em>recognizers</em>: they indicate whether a given string is valid. The principal job of the scanner is to reduce the quantity of information that must be processed by the parser, by grouping characters together into tokens, and by removing comments and white space. Scanner and parser generators automatically translate regular expressions and context-free grammars into scanners and parsers.</p>
<p id="p1640" class="text"><span epub:type="pagebreak" id="page_104" aria-label="Page 104" role="doc-pagebreak"/>Practical parsers for programming languages (parsers that run in linear time) fall into two principal groups: top-down (also called LL or predictive) and bottom-up (also called LR or shift-reduce). A top-down parser constructs a parse tree starting from the root and proceeding in a left-to-right depth-first traversal. A bottom-up parser constructs a parse tree starting from the leaves, again working left-to-right, and combining partial trees together when it recognizes the children of an internal node. The stack of a top-down parser contains a prediction of what will be seen in the future; the stack of a bottom-up parser contains a record of what has been seen in the past.</p>
<p id="p1645" class="text">Top-down parsers tend to be simple, both in the parsing of valid strings and in the recovery from errors in invalid strings. Bottom-up parsers are more powerful, and in some cases lend themselves to more intuitively structured grammars, though they suffer from the inability to embed action routines at arbitrary points in a right-hand side (we discuss this point in more detail in Section C-4.6.4). Both varieties of parser are used in real compilers, though bottom-up parsers are more common. Top-down parsers tend to be smaller in terms of code and data size, but modern machines provide ample memory for either.</p>
<p id="p1650" class="text">Both scanners and parsers can be built by hand if an automatic tool is not available. Handbuilt scanners are simple enough to be relatively common. Handbuilt parsers are generally limited to top-down recursive descent, and are most commonly used for comparatively simple languages. Automatic generation of the scanner and parser has the advantage of increased reliability, reduced development time, and easy modification and enhancement.</p>
<p id="p1655" class="text">Various features of language design can have a major impact on the complexity of syntax analysis. In many cases, features that make it difficult for a compiler to scan or parse also make it difficult for a human being to write correct, maintainable code. Examples include the lexical structure of Fortran and the <span class="inlinecode">if</span>&#x2026;<span style="display:inline-block; width: 0.45em;"/><span class="inlinecode">then</span>&#x2009;&#x2026;<span style="display:inline-block; width: 0.45em;"/><span class="inlinecode">else</span> statement of languages like Pascal. This interplay among language design, implementation, and use will be a recurring theme throughout the remainder of the book.</p>
</section>
<section>
<h2 class="h1hd" id="s0150"><a id="st0405"/>2.6 Exercises</h2>
<p id="p1660" class="textfl"/>
<div><ol>
<li id="o0250" class="numlista">2.1. &#xA0;Write regular expressions to capture the following.<ol>
<li id="o0255" class="numlist1">(a) &#xA0;Strings in C. These are delimited by double quotes (<span class="inlinecode">"</span>), and may not contain newline characters. They may contain double-quote or backslash characters if and only if those characters are &#x201C;escaped&#x201D; by a preceding backslash. You may find it helpful to introduce shorthand notation to represent any character that is <em>not</em> a member of a small specified set.</li>
<li id="o0260" class="numlist1">(b) &#xA0;Comments in Pascal. These are delimited by <span class="inlinecode">(*</span> and <span class="inlinecode">*)</span> or by <span class="inlinecode">{</span> and <span class="inlinecode">}</span>. They are not permitted to nest.</li>
<li id="o0265" class="numlist1">(c) &#xA0;Numeric constants in C. These are octal, decimal, or hexadecimal integers, or decimal or hexadecimal floating-point values. <span epub:type="pagebreak" id="page_105" aria-label="Page 105" role="doc-pagebreak"/> An octal integer begins with <span class="inlinecode">0</span>, and may contain only the digits <span class="inlinecode">0</span>&#x2013;<span class="inlinecode">7</span>. A hexadecimal integer begins with <span class="inlinecode">0x</span> or <span class="inlinecode">0X</span>, and may contain the digits <span class="inlinecode">0</span>&#x2013;<span class="inlinecode">9</span> and <span class="inlinecode">a</span>/<span class="inlinecode">A</span>&#x2013;<span class="inlinecode">f</span>/<span class="inlinecode">F</span>. A decimal floating-point value has a fractional portion (beginning with a dot) or an exponent (beginning with <span class="inlinecode">E</span> or <span class="inlinecode">e</span>). Unlike a decimal integer, it is allowed to start with <span class="inlinecode">0</span>. A hexadecimal floating-point value has an optional fractional portion and a mandatory exponent (beginning with <span class="inlinecode">P</span> or <span class="inlinecode">p</span>). In either decimal or hexadecimal, there may be digits to the left of the dot, the right of the dot, or both, and the exponent itself is given in decimal, with an optional leading <span class="inlinecode">+</span> or <span class="inlinecode">-</span> sign. An integer may end with an optional <span class="inlinecode">U</span> or <span class="inlinecode">u</span> (indicating &#x201C;unsigned&#x201D;), and/or <span class="inlinecode">L</span> or <span class="inlinecode">l</span> (indicating &#x201C;long&#x201D;) or <span class="inlinecode">LL</span> or <span class="inlinecode">ll</span> (indicating &#x201C;long long&#x201D;). A floating-point value may end with an optional <span class="inlinecode">F</span> or <span class="inlinecode">f</span> (indicating &#x201C;float&#x201D;&#x2014;single precision) or <span class="inlinecode">L</span> or <span class="inlinecode">l</span> (indicating &#x201C;long&#x201D;&#x2014;double precision).</li>
<li id="o0270" class="numlist1">(d) &#xA0;Floating-point constants in Ada. These match the definition of <em>real</em> in Example <a href="#enun0025" id="cf0695">2.3</a>, except that (1) a digit is required on both sides of the decimal point, (2) an underscore is permitted between digits, and (3) an alternative numeric base may be specified by surrounding the nonexponent part of the number with pound signs, preceded by a base in decimal (e.g., <span class="inlinecode">16#6.a7#e+2</span>). In this latter case, the letters <span class="inlinecode">a</span>&#x2009;.&#x2009;.&#x2009;<span class="inlinecode">f</span> (both upper- and lowercase) are permitted as digits. Use of these letters in an inappropriate (e.g., decimal) number is an error, but need not be caught by the scanner.</li>
<li id="o0275" class="numlist1">(e) &#xA0;Inexact constants in Scheme. Scheme allows real numbers to be explicitly <em>inexact</em> (imprecise). A programmer who wants to express all constants using the same number of characters can use sharp signs (<span class="inlinecode">#</span>) in place of any lower-significance digits whose values are not known. A base-10 constant without exponent consists of one or more digits followed by zero of more sharp signs. An optional decimal point can be placed at the beginning, the end, or anywhere in-between. (For the record, numbers in Scheme are actually a good bit more complicated than this. For the purposes of this exercise, please ignore anything you may know about sign, exponent, radix, exactness and length specifiers, and complex or rational values.)</li>
<li id="o0280" class="numlist1">(f) &#xA0;Financial quantities in American notation. These have a leading dollar sign (<span class="inlinecode">$</span>), an optional string of asterisks (<span class="inlinecode">*</span>&#x2014;used on checks to discourage fraud), a string of decimal digits, and an optional fractional part consisting of a decimal point (<span class="inlinecode">.</span>) and two decimal digits. The string of digits to the left of the decimal point may consist of a single zero (<span class="inlinecode">0</span>). Otherwise it must not start with a zero. If there are more than three digits to the left of the decimal point, groups of three (counting from the right) must be separated by commas (<span class="inlinecode">,</span>). Example: <span class="inlinecode">$**2,345.67</span>. (Feel free to use &#x201C;productions&#x201D; to define abbreviations, so long as the language remains regular.)</li></ol></li>
<li id="o0285" class="numlista">2.2. &#xA0;Show (as &#x201C;circles-and-arrows&#x201D; diagrams) the finite automata for Exercise <a href="#o0250" id="cf0700">2.1</a>.</li>
<li id="o0290" class="numlista">2.3. &#xA0;<span epub:type="pagebreak" id="page_106" aria-label="Page 106" role="doc-pagebreak"/>Build a regular expression that captures all nonempty sequences of letters other than <span class="inlinecode">file</span>, <span class="inlinecode">for</span>, and <span class="inlinecode">from</span>. For notational convenience, you may assume the existence of a <b>not</b> operator that takes a set of letters as argument and matches any <em>other</em> letter. Comment on the practicality of constructing a regular expression for all sequences of letters other than the keywords of a large programming language.</li>
<li id="o0295" class="numlista">2.4. &#xA0;<ol>
<li id="o0300" class="numlist1">(a) &#xA0;Show the NFA that results from applying the construction of Figure <a href="#f0105" id="cf0705">2.7</a> to the regular expression <em>letter</em>&#x2009;<b>(</b>&#x2009;<em>letter</em> | <em>digit</em>&#x2009;<b>)</b>&#x2009;<b>&#xFF0A;</b>.</li>
<li id="o0305" class="numlist1">(b) &#xA0;Apply the transformation illustrated by Example <a href="#enun0095" id="cf0710">2.14</a> to create an equivalent DFA.</li>
<li id="o0310" class="numlist1">(c) &#xA0;Apply the transformation illustrated by Example <a href="#enun0100" id="cf0715">2.15</a> to minimize the DFA.</li></ol></li>
<li id="o0315" class="numlista">2.5. &#xA0;Starting with the regular expressions for <em>integer</em> and <em>decimal</em> in Example <a href="#enun0025" id="cf0720">2.3</a>, construct an equivalent NFA, the set-of-subsets DFA, and the minimal equivalent DFA. Be sure to keep separate the final states for the two different kinds of token (see Sidebar <a href="#enun0105" id="cf0725">2.4</a>). You may find the exercise easier if you undertake it by modifying the machines in Examples <a href="#enun0090" id="cf0730">2.13</a> through <a href="#enun0100" id="cf0735">2.15</a>.</li>
<li id="o0320" class="numlista">2.6. &#xA0;Build an ad hoc scanner for the calculator language. As output, have it print a list, in order, of the input tokens. For simplicity, feel free to simply halt in the event of a lexical error.</li>
<li id="o0325" class="numlista">2.7. &#xA0;Write a program in your favorite scripting language to remove comments from programs in the calculator language (Example <a href="#enun0065" id="cf0740">2.9</a>).</li>
<li id="o0330" class="numlista">2.8. &#xA0;Build a nested-<span class="sans-serif">case</span>-statements finite automaton that converts all letters in its input to lower case, except within Pascal-style comments and strings. A Pascal comment is delimited by <span class="inlinecode">{</span> and <span class="inlinecode">}</span>, or by <span class="inlinecode">(*</span> and <span class="inlinecode">*)</span>. Comments do not nest. A Pascal string is delimited by single quotes (<span class="inlinecode">'</span> &#x2026;<span class="inlinecode">'</span>). A quote character can be placed in a string by doubling it (<span class="inlinecode">'Madam, I&#x201D;m Adam.'</span>). This upper-to-lower mapping can be useful if feeding a program written in standard Pascal (which ignores case) to a compiler that considers upper- and lowercase letters to be distinct.</li>
<li id="o0335" class="numlista">2.9. &#xA0;<ol>
<li id="o0340" class="numlist1">(a) &#xA0;Describe in English the language defined by the regular expression <em><span class="inlinecode">a</span></em><b>&#xFF0A; (</b><em>&#x2009;<span class="inlinecode">b</span>&#x2009;<span class="inlinecode">a</span></em><b>&#xFF0A;</b> <em><span class="inlinecode">b</span>&#x2009;<span class="inlinecode">a</span></em><b>&#xFF0A;</b>&#x2009;<b>)</b>&#x2009;<b>&#xFF0A;</b>. Your description should be a high-level characterization&#x2014;one that would still make sense if we were using a different regular expression for the same language.</li>
<li id="o0345" class="numlist1">(b) &#xA0;Write an unambiguous context-free grammar that generates the same language.</li>
<li id="o0350" class="numlist1">(c) &#xA0;Using your grammar from part (b), give a canonical (right-most) derivation of the string <span class="inlinecode">b a a b a a a b b</span>.</li></ol></li>
<li id="o0355" class="numlistb">2.10. &#xA0;Give an example of a grammar that captures right associativity for an exponentiation operator (e.g., <span class="inlinecode">**</span> in Fortran).</li>
<li id="o0360" class="numlistb">2.11. &#xA0;Prove that the following grammar is LL(1): <span epub:type="pagebreak" id="page_107" aria-label="Page 107" role="doc-pagebreak"/></li></ol>
<pre>	  <em>decl</em> &#x27F6; ID <em>decl_tail</em>
	  <em>decl_tail</em> &#x27F6; , <em>decl</em>
		 &#x27F6; : ID ;</pre>
<ol>
<li class="numlistb"> (The final <span class="inlinecode">ID</span> is meant to be a type name.)</li>
<li id="o0365" class="numlistb">2.12. &#xA0;Consider the following grammar:</li></ol>
<pre>	  <em>G</em> &#x27F6; <em>S</em> $$
	  <em>S</em> &#x27F6; <em>A M</em>
	  <em>M</em> &#x27F6; <em>S</em> | <em>&#x03B5;</em>
	  <em>A</em> &#x27F6; a <em>E</em> | b <em>A A</em>
	  <em>E</em> &#x27F6; a <em>B</em> | b <em>A</em> | <em>&#x03B5;</em>
	  <em>B</em> &#x27F6; b <em>E</em> | a <em>B B</em></pre>
<ol>
<li class="numlistb"><ol>
<li id="o0370" class="numlist1">(a) &#xA0;Describe in English the language that the grammar generates.</li>
<li id="o0375" class="numlist1">(b) &#xA0;Show a parse tree for the string <span class="inlinecode">a b a a</span>.</li>
<li id="o0380" class="numlist1">(c) &#xA0;Is the grammar LL(1)? If so, show the parse table; if not, identify a prediction conflict.</li></ol></li>
<li id="o0385" class="numlistb">2.13. &#xA0;Consider the following grammar:</li></ol>

<pre>            <em>stmt</em> &#x27F6; <em>assignment</em>
                 &#x27F6; <em>subr_call</em>
            <em>assignment</em> &#x27F6; id := <em>expr</em>
            <em>subr_call</em> &#x27F6; id (<em>arg_list</em> )
            <em>expr</em> &#x27F6; <em>primary expr_tail</em>
            <em>expr_tail</em> &#x27F6; <em>op expr</em>
                    &#x27F6; <em>&#x03B5;</em>
            <em>primary</em>  &#x27F6; id
                    &#x27F6; <em>subr_call</em>
                    &#x27F6; (<em>expr</em> )
            <em>op</em> &#x27F6; + | - | * | /
            <em>arg_list</em>   &#x27F6; <em>expr args_tail</em>
            <em>args_tail</em>  &#x27F6; , <em>arg_list</em>
                    &#x27F6; <em>&#x03B5;</em></pre>
<ol>
<li class="numlistb"><ol>
<li id="o0390" class="numlist1">(a) &#xA0;Construct a parse tree for the input string<span class="inlinecode">foo(a, b)</span>.</li>
<li id="o0395" class="numlist1">(b) &#xA0;Give a canonical (right-most) derivation of this same string.</li>
<li id="o0400" class="numlist1">(c) &#xA0;Prove that the grammar is not LL(1).</li>
<li id="o0405" class="numlist1">(d) &#xA0;Modify the grammar so that it <em>is</em> LL(1).</li></ol></li>
<li id="o0410" class="numlistb">2.14. &#xA0;Consider the language consisting of all strings of properly balanced parentheses and brackets. <span epub:type="pagebreak" id="page_108" aria-label="Page 108" role="doc-pagebreak"/><ol>
<li id="o0415" class="numlist1">(a) &#xA0;Give LL(1) and SLR(1) grammars for this language.</li>
<li id="o0420" class="numlist1">(b) &#xA0;Give the corresponding LL(1) and SLR(1) parsing tables.</li>
<li id="o0425" class="numlist1">(c) &#xA0;For each grammar, show the parse tree for <span class="inlinecode">([]([]))[](())</span>.</li>
<li id="o0430" class="numlist1">(d) &#xA0;Give a trace of the actions of the parsers in constructing these trees.</li></ol></li>
<li id="o0435" class="numlistb">2.15. &#xA0;Consider the following context-free grammar.</li></ol>
<pre>	    <em>G</em> &#x27F6; <em>G B</em>
	      &#x27F6; <em>G N</em>
	      &#x27F6; <em>&#x03B5;</em>
	    <em>B</em> &#x27F6; (<em>E</em> )
	    <em>E</em> &#x27F6; <em>E</em> (<em>E</em> )
	      &#x27F6; <em>&#x03B5;</em>
	    <em>N</em> &#x27F6; (<em>L</em> ]
	    <em>L</em> &#x27F6; <em>L E</em>
	      &#x27F6; <em>L</em> (
	      &#x27F6; <em>&#x03B5;</em></pre>
<ol>
<li class="numlistb"><ol>
<li id="o0440" class="numlist1">(a) &#xA0;Describe, in English, the language generated by this grammar. (Hint: <em>B</em> stands for &#x201C;balanced&#x201D;; <em>N</em> stands for &#x201C;nonbalanced&#x201D;.) (Your description should be a high-level characterization of the language&#x2014;one that is independent of the particular grammar chosen.)</li>
<li id="o0445" class="numlist1">(b) &#xA0;Give a parse tree for the string <span class="inlinecode">((&#x2009;](&#x2009;)</span>.</li>
<li id="o0450" class="numlist1">(c) &#xA0;Give a canonical (right-most) derivation of this same string.</li>
<li id="o0455" class="numlist1">(d) &#xA0;What is FIRST(<em>E</em>) in our grammar? What is FOLLOW(<em>E</em>)? (Recall that FIRST and FOLLOW sets are defined for symbols in an arbitrary CFG, regardless of parsing algorithm.)</li>
<li id="o0460" class="numlist1">(e) &#xA0;Given its use of left recursion, our grammar is clearly not LL(1). Does this language have an LL(1) grammar? Explain.</li></ol></li>
<li id="o0465" class="numlistb">2.16. &#xA0;Give a grammar that captures all levels of precedence for arithmetic expressions in C, as shown in Figure 6.1. (Hint: This exercise is somewhat tedious. You'll probably want to attack it with a text editor rather than a pencil.)</li>
<li class="numlistb" id="o0470">2.17. &#xA0;Extend the grammar of Figure <a href="#f0265" id="cf0745">2.25</a> to include <span class="inlinecode">if</span> statements and <span class="inlinecode">while</span> loops, along the lines suggested by the following examples:
<pre>
   abs := n
   if n &lt; 0 then abs := 0 - abs fi

   sum := 0
   read count
   while count &gt; 0 do
<span style="display:inline-block; width: 4em;"/>   read n
<span style="display:inline-block; width: 4em;"/>   sum := sum + n
<span style="display:inline-block; width: 4em;"/>   count := count - 1
   od
   write sum</pre></li>

<li class="numlistb"><span epub:type="pagebreak" id="page_109" aria-label="Page 109" role="doc-pagebreak"/> Your grammar should support the six standard comparison operations in conditions, with arbitrary expressions as operands. It should also allow an arbitrary number of statements in the body of an <span class="inlinecode">if</span> or <span class="inlinecode">while</span> statement.</li>
<li id="o0475" class="numlistb">2.18. &#xA0;Consider the following LL(1) grammar for a simplified subset of Lisp:</li></ol>
<pre>            <em>P</em>  &#x27F6; <em>E</em> $$
            <em>E</em>  &#x27F6; atom
               &#x27F6; &#x2018; <em>E</em>
               &#x27F6; (<em>E Es</em> )
            <em>Es</em> &#x27F6; <em>E Es</em>
               &#x27F6;</pre>
<ol>
<li class="numlistb"><ol>
<li id="o0480" class="numlist1">(a) &#xA0;What is FIRST(<em>Es</em>)? FOLLOW(<em>E</em>)? PREDICT(<em>Es</em> &#x27F6;<span style="display:inline-block; width: 0.45em;"/><em>&#x3B5;</em>)?</li>
<li id="o0485" class="numlist1">(b) &#xA0;Give a parse tree for the string <span class="inlinecode">(cdr '(a b c)) $$</span>.</li>
<li id="o0490" class="numlist1">(c) &#xA0;Show the left-most derivation of <span class="inlinecode">(cdr '(a b c)) $$</span>.</li>
<li id="o0495" class="numlist1">(d) &#xA0;Show a trace, in the style of Figure <a href="#f0240" id="cf0750">2.21</a>, of a table-driven top-down parse of this same input.</li>
<li id="o0500" class="numlist1">(e) &#xA0;Now consider a recursive descent parser running on the same input. At the point where the quote token (<span class="inlinecode">'</span>) is matched, which recursive descent routines will be active (i.e., what routines will have a frame on the parser's run-time stack)?</li></ol></li>
<li id="o0505" class="numlistb">2.19. &#xA0;Write top-down and bottom-up grammars for the language consisting of all well-formed regular expressions. Arrange for all operators to be left-associative. Give Kleene closure the highest precedence and alternation the lowest precedence.</li>
<li id="o0510" class="numlistb">2.20. &#xA0;Suppose that the expression grammar in Example <a href="#enun0055" id="cf0755">2.8</a> were to be used in conjunction with a scanner that did <em>not</em> remove comments from the input, but rather returned them as tokens. How would the grammar need to be modified to allow comments to appear at arbitrary places in the input?</li>
<li id="o0515" class="numlistb">2.21. &#xA0;Build a complete recursive descent parser for the calculator language. As output, have it print a trace of its matches and predictions.</li>
<li id="o0520" class="numlistb">2.22. &#xA0;Extend your solution to Exercise <a href="#o0515" id="cf0760">2.21</a> to build an explicit parse tree.</li>
<li id="o0525" class="numlistb">2.23. &#xA0;Extend your solution to Exercise <a href="#o0515" id="cf0765">2.21</a> to build an abstract syntax tree directly, without constructing a parse tree first.</li>
<li class="numlistb" id="o0530">2.24. &#xA0;The dangling <span class="inlinecode">else</span> problem of Pascal was not shared by its predecessor Algol 60. To avoid ambiguity regarding which <span class="inlinecode">then</span> is matched by an <span class="inlinecode">else</span>, Algol 60 prohibited <span class="inlinecode">if</span> statements immediately inside a <span class="inlinecode">then</span> clause. The Pascal fragment
<pre>    if C1 then if C2 then S1 else S2</pre></li>
<li class="numlistb">had to be written as either
<pre>    if C1 then begin if C2 then S1 end else S2</pre></li>
<li class="numlistb"><span epub:type="pagebreak" id="page_110" aria-label="Page 110" role="doc-pagebreak"/>or
<pre>    if C1 then begin if C2 then S1 else S2 end</pre>
</li>
<li class="numlistb">in Algol 60. Show how to write a grammar for conditional statements that enforces this rule. (Hint: You will want to distinguish in your grammar between conditional statements and nonconditional statements; some contexts will accept either, some only the latter.)</li>
<li id="o0535" class="numlistb">2.25. &#xA0;Flesh out the details of an algorithm to eliminate left recursion and common prefixes in an arbitrary context-free grammar.</li>
<li id="o0540" class="numlistb">2.26. &#xA0;In some languages an assignment can appear in any context in which an expression is expected: the value of the expression is the right-hand side of the assignment, which is placed into the left-hand side as a side effect. Consider the following grammar fragment for such a language. Explain why it is not LL(1), and discuss what might be done to make it so.</li></ol>

<pre>            <em>expr</em> &#x27F6; id := <em>expr</em>
                 &#x27F6; <em>term term_tail</em>
            <em>term_tail</em> &#x27F6; + <em>term term_tail</em> | <em>&#x03B5;</em>
            <em>term</em> &#x27F6; <em>factor factor_tail</em>
            <em>factor_tail</em> &#x27F6; * <em>factor factor_tail</em> | <em>&#x03B5;</em>
            <em>factor</em> &#x27F6; (<em>expr</em> ) | id</pre>
<ol>
<li class="numlistb"/>
<li id="o0545" class="numlistb">2.27. &#xA0;Construct the CFSM for the <em>id_list</em> grammar in Example <a href="#enun0140" id="cf0770">2.20</a> and verify that it can be parsed bottom-up with <em>zero</em> tokens of look-ahead.</li>
<li id="o0550" class="numlistb">2.28. &#xA0;Modify the grammar in Exercise <a href="#o0545" id="cf0775">2.27</a> to allow an <em>id_list</em> to be empty. Is the grammar still LR(0)?</li>
<li id="o0555" class="numlistb">2.29. &#xA0;Repeat Example <a href="#enun0240" id="cf0780">2.36</a> using the grammar of Figure <a href="#f0160" id="cf0785">2.15</a>.</li>
<li class="numlistb" id="o0560">2.30. &#xA0;Consider the following grammar for a declaration list:</li></ol>
<pre>            <em>decl_list</em> &#x27F6; <em>decl_list decl</em> ; | <em>decl</em> ;
            <em>decl</em> &#x27F6; id : <em>type</em>
            <em>type</em> &#x27F6; int | real | char
                 &#x27F6; array const .. const of <em>type</em>
                 &#x27F6; record <em>decl_list</em> end</pre>
<ol>
<li class="numlistb"> Construct the CFSM for this grammar. Use it to trace out a parse (as in Figure <a href="#f0335" id="cf0790">2.30</a>) for the following input program:</li>
</ol>
<pre>
	foo : record
		a : char;
		b : array 1 .. 2 of real;
	end;
</pre>
<span epub:type="pagebreak" id="page_111" aria-label="Page 111" role="doc-pagebreak"/>
</div>
<p class="textfl"/>
<p id="p2090" class="text"><img alt="Image 53" src="images/B9780323999663000118/fx053.jpg"/> 2.31&#x2013;2.37 In More Depth.</p>
</section>
<section>
<h2 class="h1hd" id="s0155"><a id="st0410"/>2.7 Explorations</h2>
<p id="p2095" class="textfl"/>
<div><ol>
<li id="o0565" class="numlistb">2.38. &#xA0;Some languages (e.g., C) distinguish between upper- and lowercase letters in identifiers. Others (e.g., Ada) do not. Which convention do you prefer? Why?</li>
<li id="o0570" class="numlistb">2.39. &#xA0;The syntax for type casts in C and its descendants introduces potential ambiguity: is <span class="inlinecode">(x)-y</span> a subtraction, or the unary negation of <span class="inlinecode">y</span>, cast to type <span class="inlinecode">x</span>? Find out how C, C++, Java, and C# answer this question. Discuss how you would implement the answer(s).</li>
<li id="o0575" class="numlistb">2.40. &#xA0;What do you think of Haskell, Occam, and Python's use of indentation to delimit control constructs (Section <a href="#s0015" id="cf0795">2.1.1</a>)? Would you expect this convention to make program construction and maintenance easier or harder? Why?</li>
<li id="o0580" class="numlistb">2.41. &#xA0;Skip ahead to Section 14.4.2 and learn about the &#x201C;regular expressions&#x201D; used in scripting languages, editors, search tools, and so on. Are these really regular? What can they express that cannot be expressed in the notation introduced in Section <a href="#s0015" id="cf0800">2.1.1</a>?</li>
<li id="o0585" class="numlistb">2.42. &#xA0;Rebuild the automaton of Exercise <a href="#o0330" id="cf0805">2.8</a> using <span class="inlinecode">lex</span>/<span class="inlinecode">flex</span>.</li>
<li id="o0590" class="numlistb">2.43. &#xA0;Find a manual for <span class="inlinecode">yacc</span>/<span class="inlinecode">bison</span>, or consult a compiler textbook <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0065">[ALSU07, Secs. 4.8.1 and 4.9.2]</span> to learn about <em>operator precedence parsing</em>. Explain how it could be used to simplify the grammar of Exercise <a href="#o0465" id="cf0815">2.16</a>.</li>
<li id="o0595" class="numlistb">2.44. &#xA0;Use <span class="inlinecode">lex</span>/<span class="inlinecode">flex</span> and <span class="inlinecode">yacc</span>/<span class="inlinecode">bison</span> to construct a parser for the calculator language. Have it output a trace of its shifts and reductions.</li>
<li id="o0600" class="numlistb">2.45. &#xA0;Repeat the previous exercise using ANTLR.</li></ol>
</div>
<p class="textfl"/>
<p id="p2140" class="text"><img alt="Image 53" src="images/B9780323999663000118/fx053.jpg"/> 2.46&#x2013;2.47 In More Depth.</p>
</section>
<section>
<h2 class="h1hd" id="s0160"><a id="st0415"/>2.8 Bibliographic Notes</h2>
<p id="p2145" class="textfl">Our coverage of scanning and parsing in this chapter has of necessity been brief. Considerably more detail can be found in texts on parsing theory <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0150">[AU72]</span> and compiler construction <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0065">[ALSU07</span>, <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0665">FCL10</span>, <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0105">App97</span>, <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0750">GBJ+12</span>, <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0480">CT11]</span>. Many compilers of the early 1960s employed recursive descent parsers. Lewis and Stearns <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br1395">[LS68]</span> and Rosenkrantz and Stearns <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br1770">[RS70]</span> published early formal studies of LL grammars and parsing. The original formulation of LR parsing is due to Knuth <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br1245">[Knu65]</span>. Bottom-up parsing became practical with DeRemer's discovery of the SLR and LALR algorithms <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0515">[DeR71]</span>. W. L. Johnson et al. <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br1185">[JPAR68]</span> describe an early scanner generator. The Unix <span class="inlinecode">lex</span> tool is due to Lesk <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br1325">[Les75]</span>. <span class="inlinecode">Yacc</span> is due to S. C. Johnson <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br1175">[Joh75]</span>.</p>
<p id="p2150" class="text">Further details on formal language theory can be found in a variety of textbooks, including those of Hopcroft, Motwani, and Ullman <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0980">[HMU07]</span> and Sipser <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br1915">[Sip13]</span>. Kleene <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br1235">[Kle56]</span> and Rabin and Scott <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br1765">[RS59]</span> <span epub:type="pagebreak" id="page_112" aria-label="Page 112" role="doc-pagebreak"/>proved the equivalence of regular expressions and finite automata. <sup><a href="#fn015" id="cf0880" epub:type="noteref" role="doc-noteref">14</a></sup> The proof that finite automata are unable to recognize nested constructs is based on a theorem known as the <em>pumping lemma</em>, due to Bar-Hillel, Perles, and Shamir <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0265">[BHPS61]</span>. Context-free grammars were first explored by Chomsky <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0400">[Cho56]</span> in the context of natural language. Independently, Backus and Naur developed BNF for the syntactic description of Algol 60 <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br1595">[NBB+63]</span>. Ginsburg and Rice <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0835">[GR62]</span> recognized the equivalence of the two notations. Chomsky <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0405">[Cho62]</span> and Evey <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0660">[Eve63]</span> demonstrated the equivalence of context-free grammars and push-down automata.</p>
<p id="p2155" class="text">Fischer et al.'s text <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0665">[FCL10]</span> contains an excellent survey of error recovery and repair techniques, with references to other work. The phrase-level recovery mechanism for recursive descent parsers described in Section C-2.3.5 is due to Wirth <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br2170">[Wir76, Sec. 5.9]</span>. The locally least-cost recovery mechanism for table-driven LL parsers described in Section C-2.3.5 is due to Fischer, Milton, and Quiring <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0710">[FMQ80]</span>. Dion published a locally least-cost bottom-up repair algorithm in 1978 <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0570">[Dio78]</span>. It is quite complex, and requires very large precomputed tables. McKenzie, Yeatman, and De Vere subsequently showed how to effect the same repairs without the precomputed tables, at a higher but still acceptable cost in time <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br1585">[MYD95]</span>.</p>
</section><footer>
<section epub:type="bibliography" role="doc-bibliography">
<div id="bl0070">
<h2 class="reftitle" id="st0420">Bibliography</h2>
<ul>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br0065" class="reflist1">[ALSU07] Alfred V. Aho, Monica S. Lam, Ravi Sethi, Jeffrey D. Ullman,  <em>Compilers: Principles, Techniques, and Tools</em>. second edition Boston, MA: Addison-Wesley; 2007.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br0105" class="reflist1">[App97] Andrew W. Appel,  <em>Modern Compiler Implementation</em>.  Cambridge, England: Cambridge University Press; 1997 Text available in ML, Java, and C versions. C version specialized by Maia Ginsburg; Java version (second edition, 2002) specialized by Jens Palsberg.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br0150" class="reflist1">[AU72] Alfred V. Aho, Jeffrey D. Ullman,  <em>The Theory of Parsing, Translation and Compiling</em>.  Englewood Cliffs, NJ: Prentice-Hall; 1972 Two-volume set.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br0265" class="reflist1">[BHPS61] Yehoshua Bar-Hillel, Micha A. Perles, Eliahu Shamir,  On formal properties of simple phrase structure grammars,   <cite><em>Zeitschrift f&#xE8;ur Phonetik, Sprachwissenschaft und Kommunikationsforschung</em></cite> 1961;14:143&#x2013;172.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br0375" class="reflist1">[Cer89] Paul Ceruzzi,  <em>Beyond the Limits&#x2014;Flight Enters the Computer Age</em>.  Cambridge, MA: MIT Press; 1989.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br0400" class="reflist1">[Cho56] Noam Chomsky,  Three models for the description of language,   <cite><em>IRE Transactions on Information Theory</em></cite> September 1956;IT-2(3):113&#x2013;124.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br0405" class="reflist1">[Cho62] Noam Chomsky,  Context-free grammars and pushdown storage,   <em>Quarterly Progress Report No. 65</em>.  Cambridge, MA: MIT Research Laboratory for Electronics; 1962:187&#x2013;194.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br0480" class="reflist1">[CT11] Keith D. Cooper, Linda Torczon,  <em>Engineering a Compiler</em>. second edition San Francisco, CA: Morgan Kaufmann; 2011.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br0515" class="reflist1">[DeR71] Franklin L. DeRemer,  Simple LR(k) grammars,   <cite><em>Communications of the ACM</em></cite> July 1971;14(7):453&#x2013;460.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br0570" class="reflist1">[Dio78] Bernard A. Dion,  <em>Locally Least-Cost Error Correctors for Context-Free and Context-Sensitive Parsers</em>. [Ph.&#x2009;D. dissertation] University of Wisconsin&#x2013;Madison; 1978 Computer Sciences Technical Report #344.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br0615" class="reflist1">[Dya95] Lev J. Dyadkin,  Multibox parsers: No more handwritten lexical analyzers,   <cite><em>IEEE Software</em></cite> September 1995;12(5):61&#x2013;67.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br0625" class="reflist1">[Ear70] Jay Earley,  An efficient context-free parsing algorithm,   <cite><em>Communications of the ACM</em></cite> February 1970;13(2):94&#x2013;102.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br0660" class="reflist1">[Eve63] R. James Evey,  Application of pushdown store machines,   <em>Proceedings of the 1963 Fall Joint Computer Conference</em>.  <em>Las Vegas, NV</em>.  November 1963:215&#x2013;227 AFIPS Press, Montvale, NJ.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br0665" class="reflist1">[FCL10] Charles N. Fischer, Ron K. Cytron, Richard J. LeBlanc Jr.,  <em>Crafting a Compiler</em>. second edition Boston, MA: Addison-Wesley; 2010.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br0710" class="reflist1">[FMQ80] Charles N. Fischer, Donn R. Milton, Sam B. Quiring,  Efficient LL(1) error correction and recovery using only insertions,   <cite><em>Acta Informatica</em></cite> February 1980;13(2):141&#x2013;154.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br0750" class="reflist1">[GBJ<sup>+</sup>12] Dick Grune, Henri E. Bal, Ceriel J.H. Jacobs, Koen G. Langendoen, Kees van Reeuwijk,  <em>Modern Compiler Design</em>. second edition New York, NY: Springer-Verlag; 2012.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br0835" class="reflist1">[GR62] Seymour Ginsburg, H. Gordon Rice,  Two families of languages related to ALGOL,   <cite><em>Journal of the ACM</em></cite> 1962;9(3):350&#x2013;371.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br0980" class="reflist1">[HMU07] John E. Hopcroft, Rajeev Motwani, Jeffrey D. Ullman,  <em>Introduction to Automata Theory, Languages, and Computation</em>. third edition Boston, MA: Pearson/Addison-Wesley; 2007.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br1100" class="reflist1">[Int03] International Organization for Standardization, Geneva, Switzerland. <em>The C Rationale</em>, April 2003. ISO/IEC JTC 1/SC 22/WG 14, revision 5.10.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br1175" class="reflist1">[Joh75] Stephen C. Johnson,  <em>Yacc&#x2014;Yet another compiler compiler</em>. [Technical Report 32, Computing Science] Murray Hill, NJ: AT&amp;T Bell Laboratories; 1975.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br1185" class="reflist1">[JPAR68] Walter L. Johnson, James H. Porter, Stephanie I. Ackley, Douglas T. Ross,  Automatic generation of efficient lexical processors using finite state techniques,   <cite><em>Communications of the ACM</em></cite> December 1968;11(12):805&#x2013;813.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br1200" class="reflist1">[Kas65] T. Kasami,  <em>An efficient recognition and syntax analysis algorithm for context-free languages</em>. [Technical Report AFCRL-65-758] Bedford, MA: Air Force Cambridge Research Laboratory; 1965.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br1235" class="reflist1">[Kle56] Stephen C. Kleene,  Representation of events in nerve nets and finite automata,   Claude E. Shannon, John McCarthy, eds.  <em>Automata Studies</em>.   <cite><em>Annals of Mathematical Studies</em></cite>.  Princeton, NJ: Princeton University Press; 1956;vol. 34:3&#x2013;41.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br1245" class="reflist1">[Knu65] Donald E. Knuth,  On the translation of languages from left to right,   <cite><em>Information and Control</em></cite> December 1965;8(6):607&#x2013;639.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br1325" class="reflist1">[Les75] Michael E. Lesk,  <em>Lex&#x2014;A lexical analyzer generator</em>. [Technical Report 39, Computing Science] Murray Hill, NJ: AT&amp;T Bell Laboratories; 1975.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br1395" class="reflist1">[LS68] Philip M. Lewis II, Richard E. Stearns,  Syntax-directed transduction,   <cite><em>Journal of the ACM</em></cite> July 1968;15(3):465&#x2013;488.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br1585" class="reflist1">[MYD95] Bruce J. McKenzie, Corey Yeatman, Lorraine De Vere,  Error repair in shift-reduce parsers,   <cite><em>ACM Transactions on Programming Languages and Systems</em></cite> July 1995;17(4):672&#x2013;689.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br1595" class="reflist1">[NBB<sup>+</sup>63] Peter Naur (ed.) J.W. Backus, F.L. Bauer, J. Green, C. Katz, J. McCarthy, A.J. Perlis, H. Rutishauser, K. Samelson, B. Vauquois, J.H. Wegstein, A. van Wijngaarden, M. Woodger,  Revised report on the algorithmic language ALGOL 60,   <cite><em>Communications of the ACM</em></cite> January 1963;6(1):1&#x2013;23 Original version appeared in the May 1960 issue.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br1710" class="reflist1">[PQ95] Terrence J. Parr, R.W. Quong,  ANTLR: A predicated-<em>LL(k)</em>) parser generator,   <cite><em>Software&#x2014;Practice and Experience</em></cite> July 1995;25(7):789&#x2013;810.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br1765" class="reflist1">[RS59] Michael O. Rabin, Dana S. Scott,  Finite automata and their decision problems,   <cite><em>IBM Journal of Research and Development</em></cite> 1959;3(2):114&#x2013;125.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br1770" class="reflist1">[RS70] Daniel J. Rosenkrantz, Richard E. Stearns,  Properties of deterministic top-down grammars,   <cite><em>Information and Control</em></cite> October 1970;17(3):226&#x2013;256.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br1915" class="reflist1">[Sip13] Michael Sipser,  <em>Introduction to the Theory of Computation</em>. third edition Boston, MA: Cengage Learning; 2013.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br2130" class="reflist1">[Web89] Fred Webb. Fortran story&#x2014;The real scoop. Submitted to <span class="inlinecode">alt.folklore.computers</span>, 1989. Quoted by Mark Brader in the ACM <em>RISKS</em> on-line forum, volume 9, issue 54, December 12, 1989.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br2170" class="reflist1">[Wir76] Niklaus Wirth,  <em>Algorithms + Data Structures = Programs</em>.  Englewood Cliffs, NJ: Prentice-Hall; 1976.</li>
<li epub:type="biblioentry footnote" id="B9780323999663000118_br2275" class="reflist1">[You67] Daniel H. Younger,  Recognition and parsing of context-free languages in time <span class="hiddenClass"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi>n</mi></mrow><mrow><mn>3</mn></mrow></msup></math><!--<mml:math><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math>--></span><span><img alt="Equation" class="icon1" src="images/B9780323999663000118/si20.png"/></span>,   <cite><em>Information and Control</em></cite> February 1967;10(2):189&#x2013;208.</li></ul>
</div>
</section>
<section epub:type="rearnotes">
<div class="ftnote">
<hr/>
<p class="ftnote1" epub:type="footnote" role="doc-footnote" id="fn002"><sup><a epub:type="noteref" role="doc-noteref" href="#cf0100">1 </a></sup>&#xA0;<a id="np0015"/>&#x201C;Stephen Kleene (1909&#x2013;1994), a mathematician at the University of Wisconsin, was responsible for much of the early development of the theory of computation, including much of the material in Section C-2.4.&#x201D;</p>
<p class="ftnote1" epub:type="footnote" role="doc-footnote" id="fn003"><sup><a epub:type="noteref" role="doc-noteref" href="#cf0125">2 </a></sup>&#xA0;<a id="np0020"/>&#x201C;At most sites, <span class="inlinecode">lex</span> and <span class="inlinecode">yacc</span> have been superseded by the GNU <span class="inlinecode">flex</span> and <span class="inlinecode">bison</span> tools, which provide a superset of the original functionality.&#x201D;</p>
<p class="ftnote1" epub:type="footnote" role="doc-footnote" id="fn004"><sup><a epub:type="noteref" role="doc-noteref" href="#cf0130">3 </a></sup>&#xA0;<a id="np0025"/>&#x201C;Some authors use <em>&#x3BB;</em> to represent the empty string. Some use a period (<b>.</b>), rather than juxtaposition, to indicate concatenation. Some use a plus sign (<b>+</b>), rather than a vertical bar, to indicate alternation.&#x201D;</p>
<p class="ftnote1" epub:type="footnote" role="doc-footnote" id="fn005"><sup><a epub:type="noteref" role="doc-noteref" href="#cf0135">4 </a></sup>&#xA0;<a id="np0030"/>&#x201C;We have assumed here that all numeric constants are simply &#x201C;numbers.&#x201D; In many programming languages, integer and real constants are separate kinds of token. Their syntax may also be more complex than indicated here, to support such features are multiple lengths or nondecimal bases.&#x201D;</p>
<p class="ftnote1" epub:type="footnote" role="doc-footnote" id="fn006"><sup><a epub:type="noteref" role="doc-noteref" href="#cf0155">5 </a></sup>&#xA0;<a id="np0035"/>&#x201C;John Backus (1924&#x2013;2007) was also the inventor of Fortran. He spent most of his professional career at IBM Corporation, and was named an IBM Fellow in 1987. He received the ACM Turing Award in 1977. Peter Naur (1928&#x2013;2016) spent most of his career at the University of Copenhagen and was instrumental in establishing computer science (&#x201C;datalogi&#x201D;) as an academic discipline in Denmark. He received the ACM Turing Award in 2005.&#x201D;</p>
<p class="ftnote1" epub:type="footnote" role="doc-footnote" id="fn007"><sup><a epub:type="noteref" role="doc-noteref" href="#cf0160">6 </a></sup>&#xA0;<a id="np0040"/>&#x201C;Some authors use curly braces (<b>{ }</b>) to indicate zero or more instances of the symbols inside. Some use square brackets (<b>[ ]</b>) to indicate zero or one instances of the symbols inside&#x2014;that is, to indicate that those symbols are optional.&#x201D;</p>
<p class="ftnote1" epub:type="footnote" role="doc-footnote" id="fn008"><sup><a epub:type="noteref" role="doc-noteref" href="#cf0165">7 </a></sup>&#xA0;<a id="np0045"/>&#x201C;To avoid confusion, some authors place quote marks around any single character that is part of the language being defined: <em>id_list</em> &#x27F6;<em>&#x2009;<span class="inlinecode">id</span></em> <b>(</b><em>&#x2009;&#x2018;<span class="inlinecode">,</span>&#x2019; <span class="inlinecode">id</span>&#x2009;</em><b>)</b>&#x2009;<b>&#xFF0A;</b>; <em>expr</em> &#x27F6;<em>&#x2009;&#x2018;<span class="inlinecode">(</span>&#x2019; expr &#x2018;&#x2009;<span class="inlinecode">)</span>&#x2019;</em>. In both regular and extended BNF, many authors use<span style="display:inline-block; width: 0.45em;"/>::=<span style="display:inline-block; width: 0.45em;"/>instead of &#x27F6;&#x2009;.&#x201D;</p>
<p class="ftnote1" epub:type="footnote" role="doc-footnote" id="fn009"><sup><a epub:type="noteref" role="doc-noteref" href="#cf0180">8 </a></sup>&#xA0;<a id="np0050"/>&#x201C;Given a specific grammar, there are many ways to create other equivalent grammars. We could, for example, replace <em>A</em> with some new symbol <em>B</em> everywhere it appears in the right-hand side of a production, and then create a new production <em>B</em> &#x27F6;<em>&#x2009;A</em>.&#x201D;</p>
<p class="ftnote1" epub:type="footnote" role="doc-footnote" id="fn010"><sup><a epub:type="noteref" role="doc-noteref" href="#cf0325">9 </a></sup>&#xA0;<a id="np0055"/>&#x201C;Many languages include <em>predefined</em> identifiers (e.g., for standard library functions), but these are not keywords. The programmer can redefine them, so the scanner must treat them the same as other identifiers. Contextual keywords, similarly, must be treated by the scanner as identifiers.&#x201D;</p>
<p class="ftnote1" epub:type="footnote" role="doc-footnote" id="fn011"><sup><a epub:type="noteref" role="doc-noteref" href="#cf0340">10 </a></sup>&#xA0;<a id="np0060"/>&#x201C;In actuality, the faulty software for Mariner 1 appears to have stemmed from a missing &#x201C;bar&#x201D; punctuation mark (indicating an average) in handwritten notes from which the software was derived <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br0375">[Cer89, pp. 202&#x2013;203]</span>. The Fortran <span class="inlinecode">DO</span> loop error does appear to have occurred in at least one piece of NASA software, but no serious harm resulted <span class="anychar dropdown def-img anychar-tooltip pos-up" aria-label="m dash" tabindex="0" role="button" data-dfn="B9780323999663000118_br2130">[Web89]</span>.&#x201D;</p>
<p class="ftnote1" epub:type="footnote" role="doc-footnote" id="fn012"><sup><a epub:type="noteref" role="doc-noteref" href="#cf0385">11 </a></sup>&#xA0;<a id="np0065"/>&#x201C;In general, an algorithm is said to run in time <span class="hiddenClass"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math><!--<mml:math><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:math>--></span><span><img alt="Equation" class="icon1" src="images/B9780323999663000118/si9.png"/></span>, where <em>n</em> is the length of the input, if its running time <span class="hiddenClass"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>t</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></math><!--<mml:math><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math>--></span><span><img alt="Equation" class="icon1" src="images/B9780323999663000118/si10.png"/></span> is proportional to <span class="hiddenClass"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></math><!--<mml:math><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math>--></span><span><img alt="Equation" class="icon1" src="images/B9780323999663000118/si11.png"/></span> in the worst case. More precisely, we say <span class="hiddenClass"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>t</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo linebreak="goodbreak" linebreakstyle="after">=</mo><mi>O</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">&#x27FA;</mo><mo>&#x2203;</mo><mspace width="0.2em"/><mi>c</mi><mo>,</mo><mi>m</mi></math><!--<mml:math><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">⟺</mml:mo><mml:mo>∃</mml:mo><mml:mspace width="0.2em"/><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:math>--></span><span><img alt="Equation" class="icon1" src="images/B9780323999663000118/si12.png"/></span> <span style="display:inline-block; width: 0.45em;"/><span class="hiddenClass"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">[</mo><mi>n</mi><mo linebreak="badbreak" linebreakstyle="after">&gt;</mo><mi>m</mi><mo stretchy="false">&#x27F6;</mo><mi>t</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo linebreak="badbreak" linebreakstyle="after">&lt;</mo><mi>c</mi><mspace width="0.2em"/><mi>f</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></math><!--<mml:math><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mo linebreak="badbreak" linebreakstyle="after">&gt;</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">⟶</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo linebreak="badbreak" linebreakstyle="after">&lt;</mml:mo><mml:mi>c</mml:mi><mml:mspace width="0.2em"/><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:math>--></span><span><img alt="Equation" class="icon1" src="images/B9780323999663000118/si13.png"/></span>.&#x201D;</p>
<p class="ftnote1" epub:type="footnote" role="doc-footnote" id="fn013"><sup><a epub:type="noteref" role="doc-noteref" href="#cf0505">12 </a></sup>&#xA0;<a id="np0070"/>&#x201C;Following conventional notation, we use uppercase Roman letters near the beginning of the alphabet to represent nonterminals, uppercase Roman letters near the end of the alphabet to represent arbitrary grammar symbols (terminals or nonterminals), lowercase Roman letters near the beginning of the alphabet to represent terminals (tokens), lowercase Roman letters near the end of the alphabet to represent token strings, and lowercase Greek letters to represent strings of arbitrary symbols.&#x201D;</p>
<p class="ftnote1" epub:type="footnote" role="doc-footnote" id="fn014"><sup><a epub:type="noteref" role="doc-noteref" href="#cf0680">13 </a></sup>&#xA0;<a id="np0075"/>&#x201C;Noam Chomsky (1928&#x2013;), a linguist and social philosopher at the Massachusetts Institute of Technology, developed much of the early theory of formal languages.&#x201D;</p>
<p class="ftnote1" epub:type="footnote" role="doc-footnote" id="fn015"><sup><a epub:type="noteref" role="doc-noteref" href="#cf0880">14 </a></sup>&#xA0;<a id="np0080"/>&#x201C;Dana Scott (1932&#x2013;), Professor Emeritus at Carnegie Mellon University, is known principally for inventing domain theory and launching the field of denotational semantics, which provides a mathematically rigorous way to formalize the meaning of programming languages. Michael Rabin (1931&#x2013;), of Harvard University, has made seminal contributions to the concepts of nondeterminism and randomization in computer science. Scott and Rabin shared the ACM Turing Award in 1976.&#x201D;</p>
</div>
</section></footer>
</section></body></html>
